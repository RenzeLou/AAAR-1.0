<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction-Following">
  <meta name="keywords" content="Large Language Models, Instruction Tuning, Automatic Data Generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Muffin (Multi-faceted Instructions)</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    
    gtag('js', new Date());
    
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/cupcake.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://www.wenpengyin.org/languagex-lab">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/RenzeLou/awesome-instruction-learning">
            Survey on Instruction Learning
          </a>
          
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h3 class="title is-1 publication-title"><span class="muffin-font">MUFFIN</span>: Curating Multi-Faceted Instructions for Improving Instruction-Following</h3>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              <sup>‚ô†Ô∏è</sup><a href="https://renzelou.github.io/" target="_blank">Renze Lou</a>,</span>
                          <span class="author-block">
                <sup>‚ô¢</sup><a href="https://drogozhang.github.io/" target="_blank">Kai Zhang</a>,
                          <span class="author-block">
                    <span class="author-block">
                      <sup>‚ô£</sup><a href="https://hsaest.github.io/" target="_blank">Jian Xie</a>,
                    </span>
                  <sup>‚ô•</sup><a href="https://scholar.google.com/citations?user=bKh1cAoAAAAJ&hl=en" target="_blank">Yuxuan Sun</a>,
                </span>
                          <span class="author-block">
                  <sup>‚ô†Ô∏è</sup><a href="https://www.linkedin.com/in/jihyun-ahn-4b6037225/" target="_blank">Janice Ahn</a>,
                </span>
                          <span class="author-block">
                  <sup>‚Ä†</sup><a href="https://www.linkedin.com/in/hanzixu/" target="_blank">Hanzi Xu</a>,
                </span>
                          <span class="author-block">
                  <sup>‚ô¢</sup><a href="https://ysu1989.github.io/" target="_blank">Yu Su</a>,
                          <span class="author-block">
                    <span class="author-block">
                      <sup>‚ô†Ô∏è</sup><a href="https://sites.google.com/site/yinwenpeng1987/home" target="_blank">Wenpeng Yin</a>
                          <span class="author-block">
                    <span class="author-block">
                </span>
                      </div>
            <!-- <span class="author-block">
              <a href="https://renzelou.github.io/">Renze Lou</a><sup>1</sup>&nbsp; &nbsp;</span>
            <span class="author-block">
              <a href="https://drogozhang.github.io/">Kai Zhang</a><sup>2</sup>&nbsp; &nbsp;</span>
            <span class="author-block">
              <a href="https://hsaest.github.io/">Jian Xie</a><sup>3</sup>&nbsp; &nbsp;
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=bKh1cAoAAAAJ&hl=en">Yuxuan Sun</a><sup>4</sup>&nbsp; &nbsp;
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/jihyun-ahn-4b6037225/">Janice Ahn</a><sup>1</sup>&nbsp; &nbsp;
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/hanzixu/">Hanzi Xu</a><sup>5</sup>&nbsp; &nbsp;
            </span>
            <span class="author-block">
              <a href="https://ysu1989.github.io/">Yu Su</a><sup>2</sup>&nbsp; &nbsp;
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/site/yinwenpeng1987/home">Wenpeng Yin</a><sup>1</sup>
          </div> -->


          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>‚ô†Ô∏è</sup>The Pennsylvania State University,
            <sup>‚ô¢</sup>The Ohio State University,
            <sup>‚ô£</sup>Fudan University,
            <sup>‚ô•</sup>Westlake University,
            <sup>‚Ä†</sup>Temple University</span>
          
            <!-- <span class="author-block"><a href="renze.lou@psu.edu">renze.lou@psu.edu</a>, <a href="wenpeng@psu.edu">wenpeng@psu.edu</a></span> -->
          </div>
          <p>
            <span>{renze.lou, wenpeng}@psu.edu</span>
    
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://github.com/RenzeLou/Muffin/blob/github-page/static/paper/MUFFIN.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/Reza8848/MUFFIN_68k"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    ü§ó
                  </span>
                  <span>Data</span>
                  </a>
              <!-- Model Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/Reza8848/MUFFIN-T5-3B"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    ü§ó
                  </span>
                  <span>Models</span>
                  </a>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/RenzeLou/Muffin"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              
              <!--  Link. -->
              <span class="link-block">
                <a href="TODO"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-twitter"></i>
                  </span>
                  <span>Twitter</span>
                  </a>
              </span>
              
            </div>
    
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- TODO: put the introduction vedio here -->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->




    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>

          <!-- Paradigm Illustration -->
            <section class="hero">
              <div class="hero-body">
                  <div class="container is-max-desktop">
                      <div class="columns is-centered">
                          <div class="column is-full">
                              <div class="item">
                                  <!-- Your image here -->
                                  <video id="paradigm" autoplay muted loop playsinline height="100%">
                                    <source src="./static/videos/paradigms.mov"
                                            type="video/mp4">
                                  </video>
                                  <!-- <img src="static/images/paradigms.png" alt="DataCollection" /> -->
                                  <h2 class="subtitle"  style="text-align:center;">
                                      Figure 1: Three different paradigms for designing instruction-following datasets.
                                  </h2>
                              </div>
                          </div>
                      </div>
                  </div>
              </div>
              </section>
              <!-- End Paradigm Illustration -->

        <div class="content has-text-justified">
          <p>
            In the realm of large language models (LLMs), enhancing instruction-following capability often involves <u>curating expansive training data</u>. This is achieved through two primary schemes:
          </p>
          <style>
            #indented {
               padding-left: 50px;
            }
         </style>
          <p id="indented">
            (a). <u><b>Scaling-Inputs</b></u>: Amplifying (input, output) pairs per task instruction, aiming for better instruction adherence. 
          </p>
          <p id="indented">
            (b). <u><b>Scaling Input-Free Tasks</b></u>: Enlarging tasks, each composed of an (instruction, output) pair, without requiring a separate input anymore.
          </p>
          <p> 
          However, LLMs under <u><b>Scaling-Inputs</b></u> tend to be overly sensitive to inputs, leading to misinterpretation or non-compliance with instructions. Conversely, <u><b>Scaling Input-Free Tasks</b></u> demands a substantial number of tasks but is less effective in dealing with instances that require additional input text (e.g., reading comprehension tasks). 
          </p>
          <p>
          This work introduces <span class="printerFont">MUFFIN</span>, a new scheme of instruction-following dataset curation: 
          </p>
          <p id="indented">
            (c). <u><b>Scale Tasks per Input (Ours)</b></u>: Scale task for each input by using various input facets. 
          </p>
            <p>
          Experimental results across four zero-shot benchmarks, spanning both (a) and (b) schemes, reveal that LLMs, at various scales, trained on <span class="printerFont">MUFFIN</span> generally demonstrate superior instruction-following capabilities compared to those trained on the two aforementioned schemes.  
          </p>
          <!-- <p>
            Our approach augments neural radiance fields
            (NeRF) by optimizing an
            additional continuous volumetric deformation field that warps each observed point into a
            canonical 5D NeRF.
            We observe that these NeRF-like deformation fields are prone to local minima, and
            propose a coarse-to-fine optimization method for coordinate-based models that allows for
            more robust optimization.
            By adapting principles from geometry processing and physical simulation to NeRF-like
            models, we propose an elastic regularization of the deformation field that further
            improves robustness.
          </p>
          <p>
            We show that <span class="dnerf">Nerfies</span> can turn casually captured selfie
            photos/videos into deformable NeRF
            models that allow for photorealistic renderings of the subject from arbitrary
            viewpoints, which we dub <i>"nerfies"</i>. We evaluate our method by collecting data
            using a
            rig with two mobile phones that take time-synchronized photos, yielding train/validation
            images of the same pose at different viewpoints. We show that our method faithfully
            reconstructs non-rigidly deforming scenes and reproduces unseen views with high
            fidelity. -->
          <!-- </p> -->
        </div>
      </div>
    </div>
    <!--/ Abstract. -->






<!-- Method. -->
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Data Curation</h2>

        <!-- Method Overview -->
        <section class="hero">
          <div class="hero-body">
              <div class="container is-max-desktop">
                  <div class="columns is-centered">
                      <div class="column is-full">
                          <div class="item">
                              <!-- Your image here -->
                              <video id="DataCollection" autoplay muted loop playsinline height="100%">
                                <source src="./static/videos/data_collection.mov"
                                        type="video/mp4">
                              </video>
                              <!-- <img src="static/images/data_collection.png" alt="DataCollection" /> -->
                              <h2 class="subtitle"  style="text-align:center;">
                                  Figure 2: The overall data curation pipeline of <span class="muffin-font">MUFFIN</span>.
                              </h2>
                          </div>
                      </div>
                  </div>
              </div>
          </div>
      </section>
      <!-- End Method Overview -->

    <div class="content has-text-justified">
      As shown in Figure 2, the data curation strategy of <span class="printerFont">MUFFIN</span> mainly consists of following steps: 
          <ol>
            <li><strong>Input Collection:</strong>
              <br>&nbsp &nbsp - We randomly sample input texts from two distinct sources to ensure textual diversity</li> 
            <li><strong>Instruction Collection (we use two different instruction collection methods):</strong>
              <br>&nbsp &nbsp - <i><u>Instruction Brainstorm Based on Inputs' Facets.</u></i> We first prompt LLMs (i.e., ChatGPT) to generate various textual facets (or attributes) for the given input text. Next, regarding the facets as "<i>hint</i>", we further prompt LLMs to brainstorm corresponding tasks instructions, ensuring the input-instruction relevance.</li>
              &nbsp &nbsp - <i><u>Instruction Rematching.</u></i> We gather suitable instructions from existing human-annotated sets. For each input-instruction pair, we ask LLMs (i.e., GPT-4) to determine if the instruction can align with the input to form a valid task.</li>
            <li><strong>Output Annotation:</strong>
              <br>&nbsp &nbsp - We use LLMs (i.e., ChatGPT) to annotate outputs, or mark them as ‚ÄúNone‚Äù for instances that ChatGPT deems unanswerable</li>
              <li><strong>Instruction Filtering:</strong>
                <br>&nbsp &nbsp - We filter those overlapped task instructions (the ROUGE-L score with any existing instructions is larger than 0.7) and non-answerable instructions (the outputs are marked as ‚ÄúNone‚Äù by LLMs).</li> 
                <li><strong>Classification Expansion:</strong>
                  <br>&nbsp &nbsp - For each brainstormed instrucion, we ask LLMs (i.e., ChatGPT) to generate some "<i>wrong outputs</i>" that should be suboptimal compared to the correct output. We then combine
                  the correct output with these wrong outputs to form the whole output space of this task (i.e., transforming it into a classification paradigm). </li>
          </ol>
    </div>
  </div>
</div>







<!-- Main Results. -->
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Automatic Evaluation Results</h2>

      <!-- Main Table Overview -->
      <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column is-full">
                        <div class="item">
                            <!-- Your image here -->
                            <img src="static/images/main_table.png" alt="MainTable" />
                            <h2 class="subtitle"  style="text-align:center;">
                                Table 1: Main results.
                            </h2>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

<div class="content has-text-justified">
  <p>
    We evaluate models (T5-3B & T5-11B) trained with <span class="printerFont">MUFFIN</span> on four widely adopted zero-shot benchmarks, including <i><a href="https://instructions.apps.allenai.org/">SuperNI-Test</a></i>, <i><a href="https://github.com/hendrycks/test">MMLU</a></i>, <i><a href="https://huggingface.co/datasets/bigscience/P3">T0-Eval</a></i> and <i><a href="https://github.com/suzgunmirac/BIG-Bench-Hard">BBH</a></i>. 
    Compared with the previous LLM-generated datasets, <u>the models tuned on our <span class="printerFont">MUFFIN</span> consistently achieve
    better performance across 3 out of 4 benchmarks, under various metrics</u>. We also find that the larger model benefits more from tuning on <span class="printerFont">MUFFIN</span> (4.42 ave. performance improvement of T5-3B ‚áí
    8.03 ave. performance improvement of T5-11B, compared with the strongest baseline). Notably, <span class="printerFont">MUFFIN</span> can gain the comparable or even better performances, compared with the models trained on huam-crafted datasets (e.g., SuperNI-Train, T0, T0++).
  </p>
</div>
</div>
</div>



<!-- Human Eval. -->
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Human Evaluation Results</h2>

      <!-- Human Eval Table 1 Overview -->
      <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column is-full">
                        <div class="item">
                            <!-- Your image here -->
                            <img src="static/images/human_eval_1.jpg" alt="Human_eval_1" />
                            <h3 class="subtitle" style="text-align:center; font-size: 16px;">
                                Table 2: Human evaluation acceptance ratio. We randomly sample 200 instances from each benchmark
                                and let workers evaluate different systems‚Äô outputs.
                            </h3>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <div class="content has-text-justified">
      <p>
        Table 2 shows the human evaluation results (acceptance ratios). Compared with the strongest results among all the 8 baselines, <span class="printerFont">MUFFIN</span> mostly
        results in better human acceptance ratios with large margins (except for the tiny lower result on
        T0-Eval).
      </p>
    </div>

    <!-- Human Eval Table 2 Overview -->
    <section class="hero">
      <div class="hero-body">
          <div class="container is-max-desktop">
              <div class="columns is-centered">
                  <div class="column is-full">
                      <div class="item">
                          <!-- Your image here -->
                          <img src="static/images/human_eval_2.jpg" alt="Human_eval_2" />
                          <h3 class="subtitle" style="text-align:center; font-size: 16px;">
                              Table 3: Pair-wise comparison between <span class="printerFont">MUFFIN</span> (Ours) and three strong baselines, namely Self-Instruction (Self-Inst.), Unnatural Instruction (Unnatural), and SuperNI, across four benchmarks.
                          </h3>
                      </div>
                  </div>
              </div>
          </div>
      </div>
  </section>


  <div class="content has-text-justified">
    <p>
      Table 3 shows the pair-wise comparison results between <span class="printerFont">MUFFIN</span> and other three baselines. Compared with the LLM-generated datasets (<a href="https://github.com/yizhongw/self-instruct/tree/main">Self-Instruction</a> and <a href="https://github.com/orhonovich/unnatural-instructions">Unnatural Instruction</a>), our <span class="printerFont">MUFFIN</span>
      consistently gains higher human preferences across four benchmarks, aligning with the previous conclusion. Notebly, owing to the diverse tasks and our novel paradigm, <span class="printerFont">MUFFIN</span> can even surpass the high-quality human-crafted SuperNI on three out of four benchmarks.
    </p>
  </div>

</div>
</div>



<section class="section">
  <div class="container is-max-desktop">


    


<!-- BibTex citation -->
<section class="section" id="BibTeX">
<div class="container is-max-desktop content">
<h2 class="title">Reference</h2>
Please kindly cite our paper if you use our code, data, models or results:
<br><br>
<!-- TOOD: update after arxiv is out -->
<pre><code>
@misc{Lou2023MUFFIN,
  title={MUFFIN: Curating Multi-Faceted Instructions for Improving Instruction-Following}, 
  author={Renze Lou and Kai Zhang and Jian Xie and Yuxuan Sun and Janice Ahn and Hanzi Xu and Yu su and Wenpeng Yin},
  year={2023},
  primaryClass={cs.CL}
}
</code></pre>
      </div>
  </section>


  <footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">

                    <p>
                        This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a>                            project page. You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
                            target="_blank">Creative
          Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>

                </div>
            </div>
        </div>
    </div>
</footer>

</body>
<style>
.buttonGroup {
    text-align: center;
}

.buttonGroup>button {
    padding: 15px;
    color: white;
    background-color: #363636;
    border-radius: 5px;
}

.buttonGroup>button:hover {
    box-shadow: 5px;
}
</style>

</html>