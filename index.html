<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="AAAR-1.0: Assessing  AI's Potential to Assist Research">
  <meta name="keywords" content="Large Language Models, AI4Science">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Assessing  AI's Potential to Assist Research</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    
    gtag('js', new Date());
    
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/robot_expert.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://www.wenpengyin.org/ai4research-lab">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/RenzeLou/awesome-instruction-learning">
            Survey on Instruction Learning
          </a>
          <a class="navbar-item" href="https://renzelou.github.io/Muffin/">
            Instruction Tunning Data
          </a>

        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h3 class="title is-1 publication-title"> <img src="static/images/robot_expert.png" alt="robot_expert" width="68" height="68"> <span class="muffin-font">AAAR-1.0</span>: Assessing  AI's Potential to Assist Research</h3>
          <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block">
              <sup>1</sup><a href="https://renzelou.github.io/" target="_blank">Renze Lou</a>,</span>
                          <span class="author-block">
                <sup>2</sup><a href="https://www.linkedin.com/in/hanzixu/" target="_blank">Hanzi Xu</a>,
              </span>
      
                    <span class="author-block">
                      <sup>3</sup><a href="https://scholar.google.com/citations?user=raDkYB8AAAAJ&hl=en" target="_blank">Sijia Wang</a>,
                    </span>
                          

                      <span class="author-block">
                        <sup>4</sup><a href="https://scholar.google.com/citations?user=0MQ9s5UAAAAJ&hl=en" target="_blank"> Jiangshu Du</a>,
                      </span>

                        <span class="author-block">
                          <sup>1</sup><a href="https://ryokamoi.github.io/" target="_blank">Ryo Kamoi</a>,
                        </span>
                          <span class="author-block">
                            <sup>1</sup><a href="https://aclanthology.org/people/x/xiaoxin-lu/" target="_blank">Xiaoxin Lu</a>,
                          </span>
                      <span class="author-block">
                      <sup>5</sup><a href="https://hsaest.github.io/" target="_blank">Jian Xie</a>,
                    </span>
                  <sup>6</sup><a href="https://scholar.google.com/citations?user=bKh1cAoAAAAJ&hl=en" target="_blank">Yuxuan Sun</a>,
                </span>


                <span class="author-block">
                  <sup>1</sup><a href="https://yuszh.com/" target="_blank">Yusen Zhang</a>,
                </span>

                          <span class="author-block">
                  <sup>1</sup><a href="https://www.linkedin.com/in/jihyun-ahn-4b6037225/" target="_blank">Janice Ahn</a>,
                </span>

                <span class="author-block">
                  <sup>1</sup><a href="https://www.linkedin.com/in/hongchao-fang-0b5438181/" target="_blank">Hongchao Fang</a>,
                </span>

                <span class="author-block">
                  <sup>1</sup><a href="https://www.linkedin.com/in/zhuoyang-zou-44b2b3238/" target="_blank">Zhuoyang Zou</a>,
                </span>

                <span class="author-block">
                  <sup>1</sup><a href="https://wenchao-m.github.io/" target="_blank">Wenchao Ma</a>,
                </span>

                <span class="author-block">
                  <sup>7</sup><a href="https://lixi1994.github.io/" target="_blank">Xi Li</a>,
                </span>

                          <span class="author-block">
                  <sup>8</sup><a href="https://drogozhang.github.io/" target="_blank">Kai Zhang</a>,
                </span>
                          <span class="author-block">
                  

                  <span class="author-block">
                    <sup>9</sup><a href="https://scholar.google.com/citations?user=gJDablUAAAAJ&hl=en" target="_blank">Congying Xia</a>,
                  </span>

                  <span class="author-block">
                    <sup>3</sup><a href="https://wilburone.github.io/" target="_blank">Lifu Huang</a>,
                  </span>
                  
                    <span class="author-block">
                      <sup>1</sup><a href="https://www.wenpengyin.org/" target="_blank">Wenpeng Yin</a>
                          <span class="author-block">
                    <span class="author-block">
                </span>
                      </div>
            <!-- <span class="author-block">
              <a href="https://renzelou.github.io/">Renze Lou</a><sup>1</sup>&nbsp; &nbsp;</span>
            <span class="author-block">
              <a href="https://drogozhang.github.io/">Kai Zhang</a><sup>2</sup>&nbsp; &nbsp;</span>
            <span class="author-block">
              <a href="https://hsaest.github.io/">Jian Xie</a><sup>3</sup>&nbsp; &nbsp;
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=bKh1cAoAAAAJ&hl=en">Yuxuan Sun</a><sup>4</sup>&nbsp; &nbsp;
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/jihyun-ahn-4b6037225/">Janice Ahn</a><sup>1</sup>&nbsp; &nbsp;
            </span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/hanzixu/">Hanzi Xu</a><sup>5</sup>&nbsp; &nbsp;
            </span>
            <span class="author-block">
              <a href="https://ysu1989.github.io/">Yu Su</a><sup>2</sup>&nbsp; &nbsp;
            </span>
            <span class="author-block">
              <a href="https://sites.google.com/site/yinwenpeng1987/home">Wenpeng Yin</a><sup>1</sup>
          </div> -->


          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>1</sup> <img src="static/images/penn-state-shield.jpg" alt="PennState Logo" width="23" height="23"> Pennsylvania State University;
              <sup>2</sup><img src="static/images/netflix.png" alt="PennState Logo" width="30" height="30">Netflix;
              <sup>3</sup> <img src="static/images/ucd.png" alt="PennState Logo" width="20" height="20"> University of California, Davis;
              <!-- line break -->
               <br>
              <sup>4</sup> <img src="static/images/uic.jpg" alt="PennState Logo" width="20" height="20"> University of Illinois Chicago;
              <sup>5</sup> <img src="static/images/fudan.png" alt="PennState Logo" width="25" height="25"> Fudan University;
              <sup>6</sup> <img src="static/images/Zhejiang_University_Logo.svg.png" alt="PennState Logo" width="25" height="25"> Zhejiang University;
              <br>
              <sup>7</sup> <img src="static/images/uab.png" alt="PennState Logo" width="30" height="30"> University of Alabama at Birmingham;
              <sup>8</sup> <img src="static/images/osu.jpg" alt="PennState Logo" width="20" height="20"> Ohio State University;
              <sup>9</sup> <img src="static/images/salesforce.png" alt="PennState Logo" width="25" height="25"> Salesforce Research
          </span>
          
            <!-- <span class="author-block"><a href="renze.lou@psu.edu">renze.lou@psu.edu</a>, <a href="wenpeng@psu.edu">wenpeng@psu.edu</a></span> -->
          </div>
          <p>
            <span>{renze.lou, wenpeng}@psu.edu</span>
    
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2410.22394"  
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/Reza8848/AAAR-1.0"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    ü§ó
                  </span>
                  <span>Benchmark Dataset</span>
                  </a>
              <!-- Model Link. -->
              <!-- <span class="link-block">
                <a href="https://huggingface.co/Reza8848/MUFFIN-T5-3B"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    ü§ó
                  </span>
                  <span>Models</span>
                  </a> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/RenzeLou/AAAR-1.0/tree/main"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              
              <!--  Link. -->
              <span class="link-block">
                <a href="https://x.com/Reza0843"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-twitter"></i>
                  </span>
                  <span>Twitter</span>
                  </a>
              </span>
              
            </div>
    
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- TODO: put the introduction vedio here -->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->




    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction to AAAR-1.0</h2>

          <!-- Paradigm Illustration -->
            <section class="hero">
              <div class="hero-body">
                  <div class="container is-max-desktop">
                      <div class="columns is-centered">
                          <div class="column is-full">
                              <div class="item">
                                  <!-- Your image here -->
                                  <!-- <video id="paradigm" autoplay muted loop playsinline height="100%">
                                    <source src="./static/videos/paradigms.mov"
                                            type="video/mp4">
                                  </video> -->
                                  <img src="static/images/task_illustration.png" alt="data_statistics" width="650" height="800" />
                                  <h2 class="subtitle"  style="text-align:center; font-size: 16px;">
                                  <u>Figure 1</u>: The input-output illustration of four tasks in the proposed <span class="muffin-font">AAAR-1.0</span> benchmark.
                                  </h2>
                              </div>
                          </div>
                      </div>
                  </div>
              </div>
              </section>
              <!-- End Paradigm Illustration -->

        <div class="content has-text-justified">
          <p>
            We introduce <span class="printerFont">AAAR-1.0</span> ("1.0" denotes <b>this work is a beginning of a series</b>), a benchmark dataset designed to evaluate LLM performance in four fundamental, expertise-intensive research tasks, which most AI and Machine Learning researchers encounter daily:
          </p>
          <style>
            #indented {
               padding-left: 50px;
            }
         </style>
         <p id="indented">
          (i) <span style="background-color: #cff0da;"><b>EquationInference</b></span>, assessing the correctness of equations based
          on the contextual information in paper submissions;
         </p>
          <p id="indented">
            (ii) <span style="background-color: #fadfdc;"><b>ExperiementDesign</b></span>,
designing experiments to validate research ideas and solutions;
          </p>
          <p id="indented">
            (iii) <span style="background-color: #ddf2ff;"><b>PaperWeakness</b></span>, identifying weaknesses in paper submissions;
          </p>
          <p id="indented">
            (iv) <span style="background-color: #fff1b9;"><b>ReviewCritique</b></span>,
identifying each segment in human reviews is deficient or not.
          </p>
          <!-- <p id="indented">
            (a). <u><b>Scaling-Inputs</b></u>: Amplifying (input, output) pairs per task instruction, aiming for better instruction adherence. 
          </p>
          <p id="indented">
            (b). <u><b>Scaling Input-Free Tasks</b></u>: Enlarging tasks, each composed of an (instruction, output) pair, without requiring a separate input anymore.
          </p>
          <p> 
          However, LLMs under <u><b>Scaling-Inputs</b></u> tend to be overly sensitive to inputs, leading to misinterpretation or non-compliance with instructions. Conversely, <u><b>Scaling Input-Free Tasks</b></u> demands a substantial number of tasks but is less effective in dealing with instances that require additional input text (e.g., reading comprehension tasks). 
          </p> -->
          <!-- <p>
          This work introduces <span class="printerFont">MUFFIN</span>, a new scheme of instruction-following dataset curation: 
          </p>
          <p id="indented">
            (c). <u><b>Scale Tasks per Input (Ours)</b></u>: Scale task for each input by using various input facets. 
          </p> -->
            <p>
              To ensure data quality, senior AI
              researchers with extensive domain expertise perform data annotation for AAAR-1.0, followed by
              rigorous multi-round data examination and filtering. 
          </p>

          The proposd AAAR-1.0 becnhmark is:
          <p id="indented">
            (a) <b>Challenging</b>: All four tasks require models to possess strong
            domain knowledge covering various cutting-edge research findings, as well as expert-level research
            experience, to the extent that even humans need substantial research accumulation to tackle the tasks
            we designed.
          </p>
          <p id="indented">
            (b) <b>Transparent & Quantitative</b>: Tasks here are singular, stand-alone challenges (with clear input and output
            expectations) rather than a complicated task chain. Benefiting from the proposed task-specific metrics, it provides a more transparent and quantitative assessment of the model's research outputs.
          <!-- <p>
            Our approach augments neural radiance fields
            (NeRF) by optimizing an
            additional continuous volumetric deformation field that warps each observed point into a
            canonical 5D NeRF.
            We observe that these NeRF-like deformation fields are prone to local minima, and
            propose a coarse-to-fine optimization method for coordinate-based models that allows for
            more robust optimization.
            By adapting principles from geometry processing and physical simulation to NeRF-like
            models, we propose an elastic regularization of the deformation field that further
            improves robustness.
          </p>
          <p>
            We show that <span class="dnerf">Nerfies</span> can turn casually captured selfie
            photos/videos into deformable NeRF
            models that allow for photorealistic renderings of the subject from arbitrary
            viewpoints, which we dub <i>"nerfies"</i>. We evaluate our method by collecting data
            using a
            rig with two mobile phones that take time-synchronized photos, yielding train/validation
            images of the same pose at different viewpoints. We show that our method faithfully
            reconstructs non-rigidly deforming scenes and reproduces unseen views with high
            fidelity. -->
          <!-- </p> -->
        </div>
      </div>
    </div>
    <!--/ Abstract. -->






<!-- Method. -->
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Expert-engaged Data Collection</h2>

        <!-- Method Overview -->
        <section class="hero">
          <div class="hero-body">
              <div class="container is-max-desktop">
                  <div class="columns is-centered">
                      <div class="column is-full">
                          <div class="item">
                              <!-- Your image here -->
                              <!-- <video id="DataCollection" autoplay muted loop playsinline height="100%">
                                <source src="./static/videos/data_collection.mov"
                                        type="video/mp4">
                              </video> -->
                              <img src="static/images/data_construction.png" alt="data_statistics" width="900" height="750" />
                              <!-- <img src="static/images/data_collection.png" alt="DataCollection" /> -->
                              <h2 class="subtitle"  style="text-align:center; font-size: 16px;">
                              <u>Figure 2</u>: Data construction workflows of the three tasks in <span class="muffin-font">AAAR-1.0</span>.
                              </h2>
                          </div>
                      </div>
                  </div>
              </div>
          </div>
      </section>
      <!-- End Method Overview -->

    <div class="content has-text-justified">
      We employ senior AI researchers to participate in the data collection:

      <p id="indented">
        (i) <span style="background-color: #cff0da;"><b>EquationInference</b></span>: we first crawl the source LaTeX data from arXiv and extract
        all the human-written equations from each paper,
       which are used as positive options. Then, we employ LLMs to synthesize more equations based on
        the paper contexts, namely the negative options.
       Afterwards, LLM-based filtering ensures the negative
       options are contextually appropriate. At last, the
       manual examination of human experts further enhances the quality of the classification instances.
      </p>

      <p id="indented">
        (ii) <span style="background-color: #fadfdc;"><b>ExperiementDesign</b></span>: we crawl all the source
        data from arXiv, including LaTeX code, PDF, and
         image files. After that, we ask experts to annotate
        the experiment plan and the corresponding explanations for each paper, followed by a subsequent
        multi-round peer discussion steps to improve the annotation accuracy.
      </p>

      <p id="indented">
        (iii) <span style="background-color: #ddf2ff;"><b>PaperWeakness</b></span>: since under-review drafts are required for this task; we crawl paper PDFs from the OpenReview website
        instead of arXiv. With the help of LLMs, we extract all the weaknesses from the raw comments 
        while keeping the reviewers' original words. As not all the under-review papers have arXiv sources,
        we use parsing tools to get the final input text and images from PDFs.
      </p>

      <p id="indented">
        (iv) <span style="background-color: #fff1b9;"><b>ReviewCritique</b></span>: we reuse the data from our recent work (<a href="https://arxiv.org/pdf/2406.16253" target="_blank">Du et al., 2024</a>), where we crawled
        papers' initial submissions along with their reviews from OpenReview and employed more than
        40 AI research experts to label each review segment (i.e., deficient or not), with detailed human
        explanations. In total, there were 100 papers with 380 human reviews. Each review was divided into
        sentence-level segments, resulting in 11,376 review segments (viewpoints).
      </p>  


          <!-- <ol>
            <li><strong>Input Collection:</strong>
              <br>&nbsp &nbsp - We randomly sample input texts from two distinct sources to ensure textual diversity</li> 
            <li><strong>Instruction Collection (we use two different instruction collection methods):</strong>
              <br>&nbsp &nbsp - <i><u>Instruction Brainstorm Based on Inputs' Facets.</u></i> We first prompt LLMs (i.e., ChatGPT) to generate various textual facets (or attributes) for the given input text. Next, regarding the facets as "<i>hint</i>", we further prompt LLMs to brainstorm corresponding tasks instructions, ensuring the input-instruction relevance.</li>
              &nbsp &nbsp - <i><u>Instruction Rematching.</u></i> We gather suitable instructions from existing human-annotated sets. For each input-instruction pair, we ask LLMs (i.e., GPT-4) to determine if the instruction can align with the input to form a valid task.</li>
            <li><strong>Output Annotation:</strong>
              <br>&nbsp &nbsp - We use LLMs (i.e., ChatGPT) to annotate outputs, or mark them as ‚ÄúNone‚Äù for instances that ChatGPT deems unanswerable</li>
              <li><strong>Instruction Filtering:</strong>
                <br>&nbsp &nbsp - We filter those overlapped task instructions (the ROUGE-L score with any existing instructions is larger than 0.7) and non-answerable instructions (the outputs are marked as ‚ÄúNone‚Äù by LLMs).</li> 
                <li><strong>Classification Expansion:</strong>
                  <br>&nbsp &nbsp - For each brainstormed instrucion, we ask LLMs (i.e., ChatGPT) to generate some "<i>wrong outputs</i>" that should be suboptimal compared to the correct output. We then combine
                  the correct output with these wrong outputs to form the whole output space of this task (i.e., transforming it into a classification paradigm). </li>
          </ol> -->
    </div>
  </div>
</div>


<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
<!-- data analysis. -->
<!-- <div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths"> -->
    <h2 class="title is-3">Automatic Metrics</h2>

<!-- data statistics -->
<!-- <section class="hero">
  <div class="hero-body">
      <div class="container is-max-desktop">
          <div class="columns is-centered">
              <div class="column is-full">
                  <div class="item">
                       Your image here -->
                      <!-- <img src="static/images/data_statistics.png" alt="data_statistics" width="450" height="300" />
                      <h3 class="subtitle" style="text-align:center; font-size: 16px;">
                          Table 1: Statistics of <span class="printerFont">MUFFIN</span>.
                      </h3>
                  </div>
              </div>
          </div>
      </div>
  </div>
</section> --> 


<div class="content has-text-justified">
<p>
  Since <span class="muffin-font">AAAR-1.0</span> introduces high-level research tasks with language outputs, semantic-based metrics are necessary. We propose several task-specific metrics for quantitatively assessing LLM research outputs.
</p>
<!-- <p id="indented"> -->
  <ul>
    <li><b>S-F1</b>: similarity-based F1 for assessing the experiment design quality. It measures how well each model-generated experiment aligns with the human experiments.</li>
  <li><b>S-Match</b>: "soft" match score for evaluating the explanation. It calculates the similarity between human and model-generated explanations.</li>
<li><b>SN-F1</b>: updated version of S-F1 to deal with the ‚Äúnested‚Äù review weaknesses.</li>
<li><b>ITF-IDF</b>: inspired by the classic TF-IDF; measures the inter- and intra-paper diversity of model-generated weaknesses.</li>
</ul>

<p>
  Please refer to our paper for the formal mathematical definitions of these metrics.
</p>
<!-- </div> -->
</div>
</div>
</div>








<!-- Task 1. -->
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Main Experiments</h2>

      <!-- Main Table Overview -->
      <section class="hero">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column is-full">
                        <div class="item">
                            <!-- Your image here -->
                            <img src="static/images/performance_task1.png" alt="MainTable_task1" width="450" height="400" />
                            <h2 class="subtitle"  style="text-align:center; font-size: 16px;">
                            <u>Table 1</u>: Various LLMs' performances on the 1,049 instances of <span style="background-color: #cff0da;"><b>EquationInference</b></span>.
                            </h2>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>

<div class="content has-text-justified">
  <p>
    Table 1 shows the main results on <span style="background-color: #cff0da;">EquationInference</span>.
Firstly, the open-source LLMs, especially the
Falcon and Gemma, perform unexpectedly disappointing (even worse than random guesses).
These screwed scores are mainly due to the poor
long-context instruction following ability, where
we find some open-source LLMs are confused
with the massive input and often copy the LaTeX code from the input. In contrast, closed-source
LLMs generally achieve superior accuracy, probably owing to the richer scientific knowledge from
the larger model parameters.
</p>
<p>
However, considering the conventional multi-choice QA formulation
of <span style="background-color: #cff0da;">EquationInference</span>, the recently-released GPT-4o solely gets 43.18, implying the unique challenge of
<span style="background-color: #cff0da;">EquationInference</span> compared with other scientific QA benchmarks. Notably, with the help of internal CoT, o1 gains stronger performances than GPT-4/GPT-4o, indicating the potential benefits of adopting reasoning for this task.
  </p>
</div>

<!-- Task 2-->
<section class="hero">
  <div class="hero-body">
      <div class="container is-max-desktop">
          <div class="columns is-centered">
              <div class="column is-full">
                  <div class="item">
                      <!-- Your image here -->
                      <img src="static/images/performance_task2.png" alt="MainTable_task2" width="770" height="650" />
                      <h2 class="subtitle"  style="text-align:center; font-size: 16px;">
                      <u>Table 2</u>: Various LLMs' performances on the 100 instances of <span style="background-color: #fadfdc;"><b>ExperiementDesign</b></span>. The explanation
                          generation is based on the oracle experiments to prevent error propagation. 
                          <!-- "Copy Input" is a random baseline: for experiment design, randomly select 5 sentences from the input paper; for experiment explanation, directly copy each experiment idea. -->
                      </h2>
                  </div>
              </div>
          </div>
      </div>
  </div>
</section>

<div class="content has-text-justified">
<p>
  Table 2 shows the main results on <span style="background-color: #fadfdc;">ExperiementDesign</span>. For the experiment design, the closed-source LLMs
  generally outperform open-source LLMs, and both closed-/open-source LLMs are superior to the
  ‚ÄúCopy Input‚Äù baseline (except the Falcon). Despite the higher S-Precision, the open-source LLMs
  are seriously deficient in S-Recall compared with closed-source LLMs (~10%‚Üì). We find that
  <b>closed-source LLMs are more creative in experiment design and tend to generate more experiment
  ideas than open-source LLMs (though most of the experiment ideas are trivial), leading to excellent
  S-Recall</b>. 
</p>

<p>
  As for the experiment explanation, the S-Match scores of closed-source LLMs still surpass
  the open-source LLMs, while the score difference is not significant. Furthermore, we find the negative
  correlation between S-Match and the ROUGE, where the ROUGE scores of closed-source LLMs are
  broadly inferior. We find that the open-source LLMs often try to copy the terms or phrases from the
  given experiment, or even simply paraphrase the experiment instead of explaining, which results in a
  high superficial overlap with the ground-truth explanation. This observation highlights the importance
  of adopting the proposed S-Match to avoid evaluation bias of traditional generation metrics.
</p>
</div>





<!-- Task 3 -->
<section class="hero">
  <div class="hero-body">
      <div class="container is-max-desktop">
          <div class="columns is-centered">
              <div class="column is-full">
                  <div class="item">
                      <!-- Your image here -->
                      <img src="static/images/performance_task3.png" alt="MainTable_task3" width="770" height="650" />
                      <h2 class="subtitle" style="text-align:center; font-size: 16px;">
                      <u>Table 3</u>: Various LLMs' performances on the 993 instances of <span style="background-color: #ddf2ff;"><b>PaperWeakness</b></span>.
                      </h2>
                  </div>
              </div>
          </div>
      </div>
  </div>
</section>


    <div class="content has-text-justified">
      <p>
        Table 3 shows the main results on <span style="background-color: #ddf2ff;">PaperWeakness</span>, where the closed-source LLMs' overall performances
are generally superior to the results of open-source LLMs. Similarly, closed-source LLMs are
particularly excellent in SN-Recall because of more generated weaknesses. However, there is still
a considerable gap in the weakness diversity between the LLMs and human experts. <b>Compared
with human review, most LLM-generated weaknesses are vague and lack the necessary knowledge
about some frontier research works</b>. Surprisingly, AI-scientist performs worse than backbone GPT-4o,
especially on ITF-IDF, which suggests the challenge of <span style="background-color: #ddf2ff;">PaperWeakness</span>, i.e., simply adopting popular
prompting techniques cannot well address this task.
      </p>
    </div>

    <!-- Task 4 -->
    <section class="hero">
      <div class="hero-body">
          <div class="container is-max-desktop">
              <div class="columns is-centered">
                  <div class="column is-full">
                      <div class="item">
                          <!-- Your image here -->
                          <img src="static/images/performance_task4.png" alt="MainTable_task4" width="770" height="650" />
                          <h2 class="subtitle" style="text-align:center; font-size: 16px;">
                              <u>Table 4</u>: From (<a href="https://arxiv.org/pdf/2406.16253" target="_blank">Du et al., 2024</a>), various LLMs' performances on the 11,376 instances of
                              <span style="background-color: #fff1b9;"><b>ReviewCritique</b></span>. The best F1 score among different prompt methods for a single model is
                              underlined. The best F1 score across all models is also bold.
                          </h2>
                      </div>
                  </div>
              </div>
          </div>
      </div>
  </section>


  <div class="content has-text-justified">
    <p>
      We put the results of <span style="background-color: #fff1b9;">ReviewCritique</span> in Table 4. Closed-source models (GPT-4,
Claude Opus, and Gemini 1.5) generally outperform open-source models (Llama3-8B and 70B,
Qwen2-72B) in F1 score. Claude Opus achieves the highest F1 scores, with GPT-4 and Gemini 1.5
performing slightly worse. Notably, recall scores are consistently higher than precision scores across
all LLMs and prompting strategies, suggesting that LLMs tend to incorrectly identify segments as
deficient. Despite the superior performance of the closed-source models, their F1 scores remain
relatively low even with different prompt strategies, highlighting the challenges LLMs face in such
expertise-intensive tasks and emphasizing the importance of human expertise in the meta-reviewing
process.
    </p>
  </div>

</div>
</div>





<!-- <section class="section">
  <div class="container is-max-desktop"> -->


    


<!-- BibTex citation -->
<section class="section" id="BibTeX">
<div class="container is-max-desktop content">
<h2 class="title">Reference</h2>
Please kindly cite our paper if you use our code, data, or results:
<br><br>
<!-- TOOD: update after arxiv is out -->
<pre><code>
@article{Lou2024AAAR,
  title={{AAAR-1.0}: Assessing AI's Potential to Assist Research},
  author={Renze Lou and Hanzi Xu and Sijia Wang and Jiangshu Du and Ryo Kamoi and Xiaoxin Lu and Jian Xie and Yuxuan Sun and Yusen Zhang and Jihyun Janice Ahn and Hongchao Fang and Zhuoyang Zou and Wenchao Ma and Xi Li and Kai Zhang and Congying Xia and Lifu Huang and Wenpeng Yin},
  journal={arXiv preprint arXiv:2410.22394},
  year={2024}
}
</code></pre>
      </div>
  </section>


  <footer class="footer">
    <div class="container">
        <div class="columns is-centered">
            <div class="column is-8">
                <div class="content">

                    <p>
                        This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a>                            project page. You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
                            target="_blank">Creative
          Commons Attribution-ShareAlike 4.0 International License</a>.
                    </p>

                </div>
            </div>
        </div>
    </div>
</footer>

</body>
<style>
.buttonGroup {
    text-align: center;
}

.buttonGroup>button {
    padding: 15px;
    color: white;
    background-color: #363636;
    border-radius: 5px;
}

.buttonGroup>button:hover {
    box-shadow: 5px;
}
</style>

</html>