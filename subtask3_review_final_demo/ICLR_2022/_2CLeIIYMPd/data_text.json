{
  "ID": "_2CLeIIYMPd",
  "Title": "Discovering Latent Network Topology in Contextualized Representations with Randomized Dynamic Programming",
  "URL": "https://openreview.net/forum?id=_2CLeIIYMPd",
  "paper_draft_url": "/references/pdf?id=nzsmfcKPSQ",
  "Conferece": "ICLR_2022",
  "input": {
    "source": "CRF",
    "title": "Discovering Latent Network Topology in Contextualized Representations with Randomized Dynamic Programming",
    "authors": [],
    "emails": [],
    "sections": [
      {
        "heading": "1 INTRODUCTION",
        "text": "The discovery of large-scale discrete latent structures is crucial for understanding the fundamental generative processes of language, and has been shown useful to various NLP tasks ranging from data-to-text generation (Li & Rush, 2020), summarization (Angelidis et al., 2021), syntactic parsing (Kim et al., 2019), and knowledge graph reasoning (Qu et al., 2020). In this work, we use latent structures to analyze geometric properties of representation space of pretrained language models (PLMs). Despite the large volume of recent work analyzing PLMs and proposing various improvements (Rogers et al., 2020), little is known about the topological structure of their representation manifold. Since such structure cannot be easily observed, it is only natural to resort to latent variables. Yet scaling discrete combinatorial structures is extremely difficult with multiple modeling and computational challenges (Wainwright & Jordan, 2008).\nIn this work, we address the computational challenges arising from working with combinatorial structures. We consider linear-chain CRFs, a popular structured model family (Ma & Hovy, 2016; Sutton & McCallum, 2006) that uses dynamic programming for exact inference. Specifically, we focus on the forward algorithm (Rabiner, 1989), which is widely used to compute the partition function. Space complexity for this algorithm is O(TN2) where N is the number of latent states and T the length of the sequence. It is precisely the N2 term that becomes problematic when we construct the adjacent gradient graph with automatic differentiation. DP-based inference algorithms are not optimized for modern computational devices like GPUs and typically work under small-data regimes, with N in the range [10, 100] (Ma & Hovy, 2016; Wiseman et al., 2018). With larger N , inference becomes intractable since gradients do not easily fit into GPU memory (Sun et al., 2019).\nOur algorithmic contribution is a randomization technique for dynamic programming which allows us to scale N to thousands (possibly more) latent states. Specifically, to approximate the partition function, instead of summing over all possible combinations of latent states, we only sum over paths with most probable states, and sample a subset of less likely paths to correct the bias according\nto a reasonable proposal. Since we only calculate the sampled path, memory consumption can be reduced to a small controllable budget which is scale invariant. With a larger memory budget, our method becomes more accurate, and our estimation error smaller. We thus recast the memory complexity challenge into a tradeoff between memory budget, proposal accuracy, and estimation error. When applied to linear-chain CRFs, we show that RDP scales the model by two orders of magnitude with memory complexity as small as one percent of the full DP. Beyond linear-chains, RDP is applicable to any structured model with DP-style exact inference such as trees (Kim et al., 2019) and semi-Markov models (Li & Rush, 2020), and could also be extended to more general message passing algorithms (Wainwright & Jordan, 2008).\nOur analytical contribution is a geometric study of the representation manifold of PLMs, using the proposed RDP algorithm. We hypothesize that there exist latent anchor embeddings (or landmarks) that describe the manifold topology. We also expect these anchor states to be informative enough to generate sentences, and their connections to be linguistically meaningful. We induce latent structures using a VAE with an inference model parameterized by a scaled CRF where state-word relations are modeled by the emission potential and state-state transitions are modeled by the transition matrix. The connections of words and states together form a latent network. We use the vector product between contextualized embeddings and state embeddings to parameterize the CRF potentials, bringing together the geometry of the representation space with graphical model inference. We further show that it is possible to generate paraphrases by traversing the induced network.\nOur approach is fully unsupervised and the discovered latent network is intrinsic to the representation manifold, rather than imposed by external supervision, eschewing the criticism of much previous work on supervised probes (Hewitt & Liang, 2019; Chen et al., 2021). In experiments, we first verify the basic properties of RDP (bias-variance) and show its effectiveness for training latent variable models. We then visualize the discovered network based on BERT Devlin et al. (2019), demonstrating how states encode information pertaining to syntax, morphology, and semantics. Finally, we perform unsupervised paraphrase generation by latent network traversal."
      },
      {
        "heading": "2 RANDOMIZED DYNAMIC PROGRAMMING",
        "text": "Preliminaries: Speeding Summation by Randomization To motivate our randomized DP, we start with a simple setting, namely estimating the sum of a sorted list. Given a sorted list of positive numbers a, naive summation S = a1 + ...,+aN requires N \u2212 1 addition operations, which is expensive whenN is large. Suppose we wish to reduce the number of addition operations toK1 << N , and we already know that the list is long-tailed (similar to how words in language follow a Zipfian distribution such that there are few very high-frequency words that account for most of the tokens in text and many low-frequency words). Then, we only need to sum over the top K1 values to get an efficient estimate:\nS\u03021 = a1 + ...+ aK1 where {ai}Ni=1 sorted, large to small (1)\nClearly, S\u03021 underestimates S. When the summands are \u201cdense\u201d, i.e., not very different from each other, the bias is large because the top K1 terms do not contribute much to the sum (Fig. 1A). To correct this bias, we add samples a\u03b41 , ..., a\u03b4K2 from the remaining summands whose indices \u03b4i are sampled from proposal \u03b4i \u223c q = [qK1+1, ..., qN ]:\nS\u03022 = a1 + ...+ aK1 + 1\nK2 (\n1\nq\u03b41 a\u03b41 + ...+\n1\nq\u03b4K2 a\u03b4K2 ) \u03b4i \u2208 {K1 + 1, ..., N} (2)\nwhere K1 + K2 = K. Note that this is an unbiased estimator as E[S\u03022] = S, irrespective of how we choose q. Without any knowledge about a, the simplest proposal would be uniform, no matter what variance it induces. The more qi correlates with ai, the less variance S\u03022 has. The oracle qi is proportional to ai, under which S\u03022 becomes exact S\u03022 \u2261 S as q\u03b4i = a\u03b4i/(aK+1 + ...+ aN ) for all i. So, the strategy is to exploit our knowledge about a to construct a correlated proposal q. Given this estimator, we can also adjust the computation budget in order to reduce variance. When the distribution is long-tailed, we may increase K1 as an instance of Rao-Blackwellization (Liu et al., 2019). When the distribution is not long-tailed (enough), and top K1 summation underestimates significantly, we may increase K2 to reduce variance, provided we have a fairly accurate q, as an instance of importance sampling. This procedure is also discussed in Kool et al. (2020) for\nA. Sampled Sum of a sorted list B. Sampled Forward recursion C. Model Architecture\nLong-tail case a\u0303t\u22121 a\u0303t\nBERT\nx0 x1 x2 x3\nr0 r1 r2 r3\nz0 z1 z2 z3\nDecoder\nx0 x1 x2 x3\nTopK summand Sampled summand Gap to oracle Dropped summand TopK state Sampled state\nRepresentation Space\nDense case\nEncoder\nLatent structure in representation space\nRDP Decoder Backprop. 1 2 3 Log Z\nParaphrasing4\nD. Experiment Protocol\nFigure 1: (A): Sampled summation of an array; in the dense case the proposal is important for variance reduction, while in the long-tailed case, topK summands are important; (B): core recursion step of the Randomized Forward algorithm. We get topK and sample from the proposal (black and grey bars); Errors stem from the difference (green bars) between the oracle proposal a\u0303 and constructed proposal q\u0303; (C): Inferring latent states within the BERT representation space. We parametrize the CRF factors with vector products; the relations between states and contextualized embeddings together form a latent network (Fig. 3 and 4); (D): Experimental protocol; we first study the basic properties of RDP (steps 1, 2) and then integrate RDP into a LVM for inferring the structure of the representation space (steps 3, 4). Best viewed in color.\ngradient estimation. In fact, it is the underlying basis of many Monte Carlo estimators in various settings (Mohamed et al., 2020).\nThe Sampled Forward Algorithm Now we will show how estimator S\u03022 can be used to scale summation in DP. Consider a linear chain CRF which defines a discrete state sequence z = [z1, ..., zT ], zt \u2208 {1, ..., N} over an input sentence x = [x1, ..., xT ]. Later we will use this CRF to construct an inference model to discover latent network structures within contextualized representations. We are interested in the partition function Z which is commonly computed with the Forward algorithm, a dynamic programming algorithm that sums over the potentials of all possible state sequences. The core recursion steps are:\n\u03b1t+1(i) = N\u2211 j=1 a\u0303t+1(i, j) = N\u2211 j=1 \u03b1t(j)\u03a6(j, i)\u03c6(xt+1, i) Z = N\u2211 j=1 \u03b1T (j) (3)\nwhere \u03b1t(i) is the sum of all possible sequences up to step t and at state i, \u03a6(\u00b7, \u00b7) is an N \u00d7 N transition matrix, and \u03c6(xt, i) is the emission potential that models how word xt generates state i. We assume all potentials are positive for simplicity. When implemented on GPUs, space complexity is O(TN2) (see number of edges in the DP graph in Figure 1B) and it is the squared term N2 that causes memory overflows under automatic differentiation (see Appendix B for engineering details).\nOur key insight is to recursively use the memory-efficient randomization of Eq. 2 to estimate Eq. 3 at every step. Given a proposal q\u0303t for each step t that correlates with summands a\u0303t (we discuss how to construct q\u0303t in the next section), we obtain its top K1 index and sample K2 from the rest:\n[\u03c3t,1, ..., \u03c3t,K1 , ..., \u03c3t,N ] = arg sorti{q\u0303t(i)}Ni=1 (4) [\u03b4t,1, ..., \u03b4t,K2 ] \u223c Categorical{q\u0303t(\u03c3t,K1 + 1), ..., q\u0303t(\u03c3t,N )} (5)\nwhere q\u0303t(\u00b7) are normalized to construct the categorical. Compared to Eq. 3, the key recursion of our Sampled Forward uses the top K1 index \u03c3t and sampled K2 index \u03b4t to substitute the full index:\n\u03b1\u0302t+1(i) = K1\u2211 j=1 \u03b1t(\u03c3t,j)\u03a6(\u03c3t,j , i)\u03c6(xt+1, i) + 1 K2 K2\u2211 j=1 Z\u0303t q\u0303t(\u03b4t,j) \u03b1t(\u03b4t,j)\u03a6(\u03b4t,j , i)\u03c6(xt+1, i) (6)\nZ\u0303t = N\u2211 j=K1+1 q\u0303t(\u03c3t,j) Z\u0302 = K1\u2211 j=1 \u03b1\u0302T (\u03c3T,j) + 1 K2 K2\u2211 j=1 Z\u0303T q\u0303T (\u03b4T,j) \u03b1\u0302T (\u03b4T,j) (7)\nwhere the oracle proposal q\u2217t is proportional to the actual summand a\u0303t (Eq. 3), which is only accessible with the full Forward. So, we use the proposal weight q\u0303t (Eq. 4) to move the computation outside the DP. In Fig. 1B, the top K1 summed terms correspond to black nodes. The proposal q\u0303t corresponds to black and grey bars, and its distance from the oracle proposal a\u0303t (which is the major\nsource of variance) is highlighted in green. Sampled indices are shown as blue nodes. Essentially, our Sampled Forward algorithm restricts the DP computation from the full graph to subgraphs with top and sampled edges, reducing complexity toO(TK2) whereK = K1+K2. By varyingK, memory complexity becomes a tradeoff between memory budget and estimation error. By induction, we can show that Z\u0302 (Eq. 7) is an unbiased estimator of Z since \u2200t,E[\u03b1\u0302t] = \u03b1t. When implemented in log space, the expected log Z\u0302 is a lower bound of the exact logZ due to Jensen\u2019s inequality, and the variance is (trivially) reduced by log(\u00b7). See Appendix for details on implementation (Section C), theoretical analysis (Section A), and extensions to general sum-product structures (Section D)."
      },
      {
        "heading": "3 LATENT NETWORK TOPOLOGY IN PRETRAINED LANGUAGE MODELS",
        "text": "Latent States within Representation Space We now use the above technique to uncover hidden geometric structures in contextualized representations. In experiments we work with BERT (Devlin et al., 2019) and GPT2 (Radford et al., 2019), however, our method can be easily applied to other pretrained language models. Given sentence x = [x1, ..., xT ], we denote its contextualized representations as [r1, ..., rT ] = PLM(x). Representations r for all sentences lie in one manifold M, namely, the representation space of the language model. We hypothesize there exists a set of latent states s1, ..., sM that function as anchors and outline the space topology. We emphasize that all parameters of the PLM are fixed (i.e., no fine-tuning takes place), so all learned states are intrinsic toM. We focus on two topological relations: (a) state-word relations, which represent how word embeddings may be summarized by their states and how states can be explained by their corresponding words; and (b) state-state relations, which capture how states interact with each other and how their transitions denote meaningful word combinations. Taken together, these two relations form a latent network withinM (visualized in Fig. 3 and 4). We adopt a minimal parametrization of the inference network so as to respect the intrinsic structure of the representation manifold without imposing strong assumptions (e.g., via regularization). Specifically, for state-word relations, we associate each word embedding rt with a latent state indexed by zt \u2208 {1, ..., N} (the corresponding embedding of zt is szt ). For state-state relations, we assume a transition weight \u03a6(i, j). Together we have a linear-chain CRF:\nlog \u03c6(xt, zt) = r \u1d40 t szt log \u03a6(zt\u22121, zt) = s \u1d40 zt\u22121szt (8)\nwhere the dot product follows the common practice of fine-tuning contextualized representations. We use log space for numerical stability. The probability of a state sequence given a sentence is:\nq\u03c8(z|x) = T\u220f t=1 \u03a6(zt\u22121, zt)\u03c6(xt, zt)/Z (9)\nHere, the only learnable parameters are state embeddings: \u03c8 = [s1, ..., sN ] as we try to be faithful to the representation manifold. Note how this parametrization reconciles space geometry with graphical models. As N is large, we estimate Z with the proposed Sampled Forward (Eq. 7).\nConstructing the Proposal We now return to proposal q\u0303t (Eq. 4) which we construct based on a common observation that linguistic phenomena are long-tailed:\nq\u0303t(i) \u221d \u03a6(i)\u03c6(xt, i) \u03a6(i) = ||si||1 (10) where \u03c6(xt, i) states that only a few states are likely to generate observation xt, which is often the case in NLP (e.g., there are only a few possible POS tags for each word); and \u03a6(i) models the prior probability of state i. This choice stems from the empirical observation that larger L1 norm correlates with larger dot product, and is thus more likely to be inferred. Essentially, our proposal combines local emissions \u03c6 and global prior \u03a6 to approximate the a\u0303t variables (Eq. 3) and bypass their expensive computation.\nInference and Learning We use amortized variational inference to learn s. We simply reuse the architecture from previous work Fu et al. (2020); Li & Rush (2020) and build a generative model:\np\u03b8(x, z) = \u220f t p(xt|z1:t, x1:t\u22121) \u00b7 p(zt|z1:t\u22121, x1:t\u22121) ht = Dec([szt\u22121 ;xt\u22121],ht\u22121) (11)\np(xt|z1:t, x1:t\u22121) = softmax(FF(ht)) p(zt|z1:t\u22121, x1:t\u22121) = softmax(FF([szt ;ht])) (12)\nwhere \u03b8 denotes the decoder parameters, Dec(\u00b7) denotes the decoder (we use an LSTM), ht denotes decoder states, and FF(\u00b7) denotes a feed-forward network. This autoregressive formulation essentially encourages states to be \u201cgenerative\u201d, i.e., to generate sentences and themselves. We will show in experiments how this formulation lends itself to paraphrasing. We use q\u03c8 directly from Eq. 9 as our variational posterior, and optimize the following \u03b2-ELBO objective:\nLELBO = Eq\u03c8(z|x)[log p\u03b8(x, z)]\u2212 \u03b2H(q\u03c8(z|x)) (13)\nwhere the \u03b2 parameter modulates the topology of the latent structure and prevents posterior collapse. We follow Fu et al. (2020) and use their Gumbel reparameterization to optimize q\u03c8 , which is more stable than the REINFORCE gradient estimator (Li & Rush, 2020).\nWhen integrating RDP with the Gumbel reparameterization, we noticed that the gradient will only pass through the top K1 and sampled K2 states, in other words, not all states receive gradients. In this case, trading K1 against K2 amounts to exploration versus exploitation. A large K1 means we give gradients to high-confidence states, i.e., we exploit large local emission and global transition potentials. While increasing K2 means we explore low-confidence states. So, by splitting the computation budget between top K1 and sampled K2 states, we not only reduce variance for estimating the partition, but also effectively introduce different strategies for searching over the latent space."
      },
      {
        "heading": "4 RELATED WORK",
        "text": "Efficient Inference for Structured Latent Variables There has been substantial interest recently in the application of deep latent variable models (LVMs) to various language related tasks (Wiseman et al., 2018; Li & Rush, 2020), which has also exposed scalability limitations. Earlier attempts to render CRF models efficient (Sokolovska et al., 2010; Lavergne et al., 2010) either make many stringent assumptions (e.g., sparsity), rely on handcrafted heuristics for bias correction (Jeong et al., 2009), or cannot be easily adapted to modern GPUs with tensorization and parallelization. Sun et al. (2019) are closest to our work, however they only consider topK summation and consistently underestimate the partition. Chiu & Rush (2020) scale HMMs but assume words are clustered beforehand. Our approach systematically trades computation with proposal accuracy and estimation error (rather than over-compromising for efficiency). Moreover, we do not impose any hard restrictions like sparsity (Correia et al., 2020), and can accommodate dense and long-tailed distributions. Our method is inspired by randomized automatic differentiation (RAD, Oktay et al., 2020), and can be viewed as RAD applied to the DP computation graph. Advantageously, our proposal is compatible with existing efficient implementations (like Rush, 2020) since it does not change the computation graph.\nInterpretability of Contextualized Representations There has been a good deal of interest recently in analyzing contextualized representations and the information they encode. This line of research, collectively known as \u201cBertology\u201d (Rogers et al., 2020; Hewitt & Manning, 2019), focuses mainly on supervised probing of linguistic properties (Tenney et al., 2019), while the geometric properties of the representations have been less studied (Cai et al., 2021). A major dilemma facing this work is whether supervised linguistic probes reveal properties intrinsic to the embeddings or imposed by the supervision signal itself (Hewitt & Liang, 2019; Hall Maudslay et al., 2020; Chen et al., 2021). In this work, we do not use any supervision to ensure that the discovered network is intrinsic to the representation space."
      },
      {
        "heading": "5 EXPERIMENTS",
        "text": "In this section, we present our experimental results aimed at analyzing RDP and showcasing its practical utility (see Fig. 1). Specifically, we (1) verify the basic properties of RDP by estimating the partition function and (2) using it to train the structured latent variable model introduced in Section 3; (3) we then turn our attention to pretrained language models and examine the network induced with our approach and whether it is meaningful; and (4) we generate sentence paraphrases by traversing this network. For experiments (1, 2, 4), we use (a). pretrained GPT2 as the encoder since they are more about autoregressive language modeling and generation; (b). the MSCOCO dataset, a common benchmark for paraphrasing (Fu et al., 2019). For experiment (2), we use (a). BERT since it has been the main focus of most previous analytical work (Rogers et al., 2020); (b). the 20News dataset, a popular benchmark for training latent variable models (Grisel et al.). Across all experiments, we\nuse an LSTM decoder with states identical to the encoder (762 for BERT base and GPT2 as in Wolf et al., 2020). More details on experiments and model settings can be found in Appendix E."
      },
      {
        "heading": "5.1 BASIC PROPERTIES",
        "text": "We examine the estimation of the partition function for three unit cases, namely dense, intermediate, and long-tailed distributions. Instead of simulating these unit cases, to make our experiments more realistic, we extract CRFs on-the-fly from different LVM training stages. We also study the effects of memory budget by setting K to 20, 200, and 400 (corresponding to 1, 10, and 20 percent of the full memory). We use TopK summation (Sun et al., 2019) as our main baseline. This method can be viewed as setting K1 = K and K2 = 0 in our framework, i.e., it does not use the random sample. For training LVMs, We consider 100 and 2,000 latent states. With 100 states we are able to perform the summation exhaustively which is the same as Fu et al. (2020). Full summation with 2,000 states is intractable, so we only compare with TopK summation and use K = 100.\nEstimating the Partition Function As shown in Figure 2, TopK summation always underestimates the partition. The gap is quite large in the dense case (large entropy), which happens at the initial stages of training when the model is not confident enough. The long-tailed case represents later training epochs when the model has converged and is more concentrated. Our method effectively corrects the bias, and works well in all unit cases with significantly less memory.\nTraining Latent Variable Models We compare different LVMs in Table 1. Following common practice, we report negative log likelihood (NLL) and perplexity (PPL). We perform an extensive search over multiple hyperparameters (e.g., \u03b2, learning rate, word dropout) across multiple random seeds (3\u20136) and report the average performance of the best configuration for each method. Our model performs best in both 100 and 2,000 state settings. The advantage is modest (as there are no architecture changes, only different training methods) but consistent. RDP trades off exploitation (i.e., increasing K1) and exploration (i.e., increasing K2) while TopK summation always focuses on the local solutions by passing gradients through top states. Intuitively, we have the chance of discovering better latent states (i.e., larger likelihood) by randomly searching the unexplored space."
      },
      {
        "heading": "5.2 DISCOVERING LATENT NETWORKS FROM PRETRAINED EMBEDDINGS",
        "text": "We now discuss how latent structures induced with RDP reveal linguistic properties of contextualized representations. We focus on BERT Devlin et al. (2019) and set the number of latent states to"
      },
      {
        "heading": "1291 Literature lines 1848 | read 701 | writes 502 | line 376 | book 319 | books 244 | write 203 | written 177 | text 171",
        "text": "2,000. As BERT\u2019s vocabulary size is 32K, one state would approximately handle 15 words in the uniform case, functioning as a type of \u201cmeta\u201d word. After convergence, we use q\u03c8 to sample z for each x in the training set (recall we use the 20News dataset). These z can be viewed as samples from the aggregated posterior \u2211 x q\u03c8(z|x)p?(x) where p?(x) denotes the empirical data distribution. To get a descriptive summary of BERT\u2019s latent topology, we compute the following statistics on z samples: state frequency (Fig. 3, A3); number words corresponding to each state (Fig. 3, A2); number of states corresponding to each word (Fig. 3, A1); and state bigrams (Fig. 4). We further differentiate stopwords (e.g., in, of, am, is) from content words.\nState-word Relations Figure 3 gives a first impression of how latent states spread over the representation space. Overall, we observe that the joint space is Zipfian, and this property characterizes\nthe distribution of words (A1), states (A3), and word occurrence within each state (C). We also see that the top 500 states account for most word occurrence (A2) while the remaining states model tail phenomena (A3). We conjecture this number is related to the intrinsic dimension of the data manifold (see Aghajanyan et al. 2021). The induced states encode multiple linguistic properties (Fig. 3, C). Some states are similar to a lexicon entry encoding specific words and their morphological variants; other states exhibit clustering based on morphological features (-s, -er, -ly suffix). We believe this is closely related to the fact that BERT learns embeddings over subwords. Note that the past tense cluster contains words exhibiting both regular (-ed suffix) and irregular morphology (e.g., lost and built). Finally, we also see that some states are largely semantic, similar to a conventional topic model (e.g., Computer and Medicine clusters). See Appendix E.6 for more state-word examples.\nState-State Relations As shown in Fig. 4, we observe a clear geometric difference between top and tail states. Most linguistic constructions seem to be captured by the top 500 states (A1). The connections of top states are visualized in (A2\u2013A4). From a statistical perspective, the similarity of (A3) and (A4) clearly shows how the empirical prior (encoded by the transition matrix \u03a6) matches\nModel iB4\u2191 B4\u2191 sB4\u2193 CGMH (Miao et al., 2018) 7.84 11.45 - UPSA (Liu et al., 2020) 9.26 14.16 - GUMBEL-CRF (Fu et al., 2020) 10.20 15.75 -\nthe aggregated posterior (encoded in the bigram sample from q\u03c8), which is an important desideratum of generative modeling (Mathieu et al., 2019). Note that the number of edges linked to each node, again, follows a Zipfian distribution as top nodes have most of the connections. From a linguistic perspective, we see how the combination of states leads to meaningful syntactic and semantic constructions. Again, BERT encodes various syntactic configurations such as to infinitives, passive voice, and even manages to distinguish adverbials (e.g., in fact) from prepositional phrases (e.g., in Bosnia). In general, the latent network seems to have some grasp of syntax, semantic roles, and collocations. In the following section, we examine whether this inherent knowledge can be harvested for generation. See Appendix E.7 for more state transition examples."
      },
      {
        "heading": "5.3 PARAPHRASING THROUGH NETWORK TRAVERSAL",
        "text": "We now study how the latent network can be usefully employed to generate paraphrases without access to parallel training instances. Given a sentence, we generate its paraphrase by conditioning on the input which we represent as a bag-of-words Fu et al. (2020) and by sampling from latent states. This amounts to traversing the latent network then fill in the traversal path to assemble a sentence, as visualized in Fig. 4 C. We instantiate our approach with a latent network learned from GPT2 representations (Radford et al., 2019) and refer to our model collectively as GPTNET.\nWe compare against three previous unsupervised models (first block in Table 2), including CGMH (Miao et al., 2019), a general-purpose MCMC method for controllable generation; UPSA (Liu et al., 2020), a strong paraphrasing model with simulated annealing, and GUMBEL-CRF (Fu et al., 2020), a template induction model based on a continuous relaxation of the CRF sampling algorithm. We present GPTNET variants with 50 and 2,000 states, and show results with RDP and topK, and the full summation for 50 states. Following previous work, we use iBLEU (Sun & Zhou, 2012) as our main metric, which trades off fidelity to the references (BLEU) and variation from the input (self-BLEU). Table 2 shows that RDP is superior to TopK and full summation in terms of iBLUE. GPTNet models do not outperform GUMBEL-CRF or UPSA. This is expected as these methods are highly tailored to the task and more flexible (e.g., they do not fix the encoder), while we restrict the modeling within the GPT2 representation space (to infer its structure). So, our results should be viewed as a sanity check demonstrating the latent network is indeed meaningful for generation (see Appendix E.8 for more generation examples)."
      },
      {
        "heading": "6 CONCLUSION",
        "text": "In this paper, we have developed a general method for scaling the inference of structured latent variable models with randomized dynamic programming. It is a useful tool for the visualization and inspection of the intrinsic structure of contextualized representations. Experiments with BERT reveal the topological structure of its latent space: state-word connections encapsulate syntactic and semantic roles while state-state connections correspond to phrase constructions. Moreover, traversal over a sequence of states represents underlying sentence structure.\nEthics Statement As this paper inspects the internal structure of pretrained language models, it is likely that it will reveal frequent linguistic patterns encoded in the language model. Specifically, the frequent words, phrases, and sentences associated with different gender, ethnic groups, nationality, interest groups, social status, and all other factors, are likely to be revealed by our model. When calling the generative part of our model for paraphrasing, these differences are likely to exist in the generated sentences (depending on the dataset). These facts should be considered when using this model.\nReproducibility Statement A step-by-step implementation guide for our randomized forward algorithm is provided in Appendix section C. The comparison of RDP versus other possible solutions for scaling the structured models is provided in Appendix section B. A detailed description of the model architecture is provided in the Appendix section E.1. A detailed description of data processing is provided in the Appendix section E.2. A detailed description of training strategy, hyperparameter search strategy, and model selection, is provided in Appendix section E.3. A detailed description of visualization procedure is provided in Appendix section E.5. We will release code after the anonymity period."
      },
      {
        "heading": "A THEORETICAL ANALYSIS OF SAMPLED FORWARD ALGORITHM",
        "text": "In this section, we discuss the unbiasedness and variance of the Randomized Forward algorithm. We first show that Randomized Forward gives an unbiased estimator of the partition function.\nTheorem A.1 (Unbiasedness). For all t \u2208 [1, 2, ..., T ], the sampled sum \u03b1\u0302t (Eq. 6) is an unbiased estimator of the forward variable \u03b1t (Eq. 3). The final sampled sum Z\u0302 (Eq. 7) is an unbiased estimator of the partition function Z (Eq. 3).\nProof. By the Second Principle of Mathematical Induction. Assume initialization \u03b11(i) = \u03c6(x1, i). Firstly at t = 2, for all i, we have:\nEq1 [\u03b1\u03022(i)] = K1\u2211 j=1 \u03b11(\u03c31,j)\u03a6(\u03c31,j , i)\u03c6(x2, i) + 1 K2 K2\u2211 j=1 Eq1 [ Z\u03031 q\u03031(\u03b41,j)\n\u03b11(\u03b41,j)\u03a6(\u03b41,j)\u03c6(x2, i)\ufe38 \ufe37\ufe37 \ufe38 =A\n]\n(14)\nwhere the second term can be expanded as a masked summation with the index rearranged from \u03c32,K1+1 to \u03c32,N :\nA = K2\u2211 j=1 Eq1 [ Z\u03031 q\u03031(\u03b41,j) \u03b11(\u03b41,j)\u03a6(\u03b41,j)\u03c6(x2, i) ] (15)\n= K2\u2211 k=1 N\u2211 j=K1+1 Eq1 [ 1 q1(\u03b41,k) 1(\u03b41,k = j)\u03b11(\u03c31,j)\u03a6(\u03c31,j)\u03c6(x2, i) ] (16)\n= K2\u2211 k=1 N\u2211 j=K1+1 Eq1 [ 1 q1(\u03b41,k) 1(\u03b41,k = j) ] \ufe38 \ufe37\ufe37 \ufe38\n=1\n\u03b11(\u03c31,j)\u03a6(\u03c31,j)\u03c6(x2, i) (17)\n= K2 N\u2211 j=K1+1 \u03b11(\u03c31,j)\u03a6(\u03c31,j)\u03c6(x2, i) (18)\nNotice how we re-index the sum. Now put it back to Eq. 14 to get:\nEq1 [\u03b1\u03022(i)] = K1\u2211 j=1 \u03b11(\u03c31,j)\u03a6(\u03c31,j , i)\u03c6(x2, i) + 1 K2 \u00b7K2 N\u2211 j=K1+1 \u03b11(\u03c31,j)\u03a6(\u03c31,j)\u03c6(x2, i) (19)\n= N\u2211 j=1 \u03b11(\u03c31,j)\u03a6(\u03c3,j , i)\u03c6(x2, i) (20)\n= N\u2211 j=1 \u03b11(j)\u03a6(j, i)\u03c6(x2, i) (21)\n= \u03b12(i) (22)\nThis verifies the induction foundation that Eq1 [\u03b1\u03022(i)] = \u03b12(i) for all i.\nNow assume for time index t we have \u2200i,Eq1:t\u22121 [\u03b1\u0302t(i)] = \u03b12(i). Consider t+ 1, we have:\nE1:qt [\u03b1\u0302t+1(i)] = K1\u2211 j=1 Eq1:t\u22121 [ \u03b1\u0302t(\u03c3t,j) ] \u03a6(\u03c3t,j , i)\u03c6(xt+1, i) (23)\n+ 1\nK2 K2\u2211 j=1 Eq1:t\u22121 [ \u03b1\u0302t(\u03b4t,j) ] \u00b7 Eqt [ Z\u0303t q\u0303t(\u03b4t,j) \u03a6(\u03b4t,j)\u03c6(xt+1, i)] ]\n(24)\n= K1\u2211 j=1 \u03b1t(\u03c3t,j)\u03a6(\u03c3t,j , i)\u03c6(xt+1, i) (25)\n+ 1\nK2 K2\u2211 j=1 \u03b1t(\u03c3t,j) \u00b7 Eqt [ Z\u0303t q\u0303t(\u03b4t,j) \u03a6(\u03b4t,j)\u03c6(xt+1, i)] ]\n\ufe38 \ufe37\ufe37 \ufe38 =A\u2032\n(26)\nNote how we decompose the expectation by using the independence: q1:t = q1:t\u22121 \u00b7qt With a similar masked summation trick as A, we have:\nA\u2032 = K2 N\u2211 j=K1+1 \u03b1t(\u03c3t,j)\u03a6(\u03c3t,j , i)\u03c6(xt+1, i) (27)\nThis gives us:\nE1:qt [\u03b1\u0302t+1(i)] = K1\u2211 j=1 \u03b1t(\u03c3t,j)\u03a6(\u03c3t,j , i)\u03c6(xt+1, i) + 1 K2 \u00b7K2 N\u2211 j=K1+1 \u03b1t(\u03c3t,j)\u03a6(\u03c3t,j , i)\u03c6(xt+1, i)\n(28)\n= N\u2211 j=1 \u03b1t(\u03c3t,j)\u03a6(\u03c3t,j , i)\u03c6(xt+1, i) (29)\n= N\u2211 j=1 \u03b1t(j)\u03a6(j, i)\u03c6(xt+1, i) (30)\n= \u03b1t+1(i) (31)\nThus showing \u03b1\u0302t is an unbiased estimator for \u03b1t at each step t. Setting t = T , the last step, gives us E[Z\u0302] = Z (details similar to the above).\nCorollary A.1.1. When we change the (sum, product) semiring to the (log-sum-exp, sum) semiring, the expectation of the estimator will become a lower bound of log\u03b1t and logZ.\nProof. Denote lt(i) = log\u03b1t(i) and \u2126 the set of sampled indices where |\u2126| = K2. For simplicity, we omit the top K1 summation and only show the summation of the sample. Cases where t > 2 can\nbe derived similarly as following: l\u03022(i) = log \u2211 j\u2208\u2126 exp(l1(j) + log \u03a6(j, i) + log \u03c6(xt, i))\u2212 logK2 (32)\nE[l\u03022(i)] = E [ log \u2211 j\u2208\u2126 exp(l1(j) + log \u03a6(j, i) + log \u03c6(xt, i)) ] \u2212 logK2 (33)\n\u2264 logE [\u2211 j\u2208\u2126 exp(l1(j) + log \u03a6(j, i) + log \u03c6(xt, i)) ] \u2212 logK2 (34)\n= logK2 \u2212 logK2 + log \u2211 j\u2208\u2126 exp(l1(j) + log \u03a6(j, i) + log \u03c6(xt, i)) (35)\n= l2(i) (36)\nwhere Eq. 34 comes from Jensen\u2019s inequality. Then by induction one can show at everystep, we have E[l\u0302t(i)] \u2264 lt(i).\nAlthough implementation in the log space makes the estimate biased, it reduces the variance exponentially in a rather trivial way. It also provides numerical stability. So, in practice we use it for training."
      },
      {
        "heading": "B CHALLENGES OF FULL DP UNDER AUTOMATIC DIFFERENTIATION",
        "text": "Now we analyze in detail why direct full summation causes memory overflow. Firstly, we note that in a bachified stochastic gradient descent setting, memory complexity for the forward algorithm is\nO(BTN2) (37)\nWhere B is the batch size, T is the maximum sequence length after padding, and N is the number of states. Consider a typical setting where B = 100, T = 100, N = 1000, then the complexity is:\nC = c\u00d7 100\u00d7 100\u00d7 1000\u00d7 1000 = c\u00d7 1010 (38)\nwhere c is a constant depending on specific hardware and libraries. At first sight this may seem reasonable with a large GPU memory (e.g., 16G). Also, one can come up with improvements, such as not storing the intermediate \u03b1t variables. If we only need one single forward pass, such engineering tricks are indeed effective. In our experiments, when setting B = 50, T = 15, and N = 2000, the actual memory consumption for the forward computation, together with other model components, is about 4G when implemented with PyTorch 1.8.0.\nHowever, this is not the case when working under an automatic differentiation (AD) setting. If we want to compute the gradients of the partition function (e.g., to optimize the likelihood), we inevitably need to store all the intermediate steps to keep the full computation graph. Then, the adjacent graph, in principle, has the same complexity as the forward graph. But in practice, it will be more complicated due to the actual implementation of AD engines. Following the above example where B = 50, T = 15, N = 2000, we get a memory overflow on a 16G memory GPU when calling the PyTorch backward function. This situation also immediately invalidates many engineering tricks (e.g., we cannot drop the intermediate \u03b1t variables anymore), and substantially increases the difficulty of coming up with new ones, since we would also be working with the internal mechanism of automatic differentiation engines. Note that the internal mechanism of these engines is complicated and opaque to general practitioners (e.g., they may change the underlying computation graph for better speed), and many of these engines (like PyTorch) are not optimized for dynamic programming.\nIn fact, it would be unwise to overwrite the backward computation, even if the AD engine allows us to do so. Doing so would significantly increase engineering difficulty, as we not only need to overwrite the first order gradients (like the gradients for the likelihood), but also the second order gradients (e.g., gradients for marginals or reparameterized samples). In fact, a brute force implementation would not only forego all the advantages of AD engines (like operator-level optimization), but would also require separate implementations for every graph (e.g., chains, trees, semi-Markovs, and\nIsing-Potts), every inference algorithm (e.g., partition, marginal, sampling, reparameterization, entropy), and higher order gradients. In summary, this is an extremely difficult path that requires clever tricks to efficiently improve a large table of already complicated DP algorithms.\nThis is why we do not interfere with the AD framework and work on more general and efficient algorithms, rather than tailored implementation tricks. With the randomized DP, we do not re-implement anything. We only choose the index by the pre-computed proposal, and introduce the re-indexed potentials to existing efficient libraries (like Torch-struct in Rush, 2020). This is least difficult from an implementation perspective and best exploits the power of efficient structured prediction libraries and AD engines.\nC IMPLEMENTATION OF SAMPLED FORWARD ALGORITHM\nNow we discuss how to implement the sample forward in detail. Our implementation aims to set all computation outside the actual DP, and reuse existing optimized DP libraries. Specifically, we:\n1. normalize the log potentials; 2. construct the proposal according to the local potential and a global prior; 3. obtain the top K1 index and sample K2 from the rest, retrieve the potentials according to\nthese indices; 4. correct the bias before the DP, then insert the corrected potentials to an existing DP imple-\nmentation.\nNow we explain each step in detail.\nPotential Normalization is a technique useful for numerical stability (see Fu et al., 2021). Specifically, in practice, the log-sum-exp function will saturate (it returns the max input value) when the maximum difference between inputs is larger than 10. So, we normalize all log potentials to be within range [1,m],m < 10. Given the emission potential \u03c6 and transition potential \u03a6 from Eq. 8, we normalize as:\n\u03c6\u0303(xt, i) = m \u2217 log \u03c6(xt, i)\u2212mini log \u03c6(xt, i)\nmaxi log \u03c6(xt, i)\u2212mini log \u03c6(xt, i) (39)\n\u03a6\u0303(i, j) = m \u2217 log \u03a6(i, j)\u2212mini,j log \u03a6(i, j) maxi,j log \u03a6(i, j)\u2212mini log \u03a6(i, j)\n(40)\nThen the actual CRF is constructed based on the normalized potentials. Proposal Construction Our proposal is constructed based on the normalized local emission and a\nprior distribution. Specifically:\nqt(i) = 1 2 \u00b7 ( exp \u03c6\u0303(xt, i)\u2211N\ni=1 exp \u03c6\u0303(xt, i) + exp ||si||1\u2211N i=1 exp ||si||1 ) (41)\nIndex Retrieval For each step t, we retrieve the top K1 index and get a K2 sized sample from the rest, as is discussed in the main paper:\n[\u03c3t,1, ..., \u03c3t,K1 , ..., \u03c3t,N ] = arg sorti{qt(i)}Ni=1 (42) [\u03b4t,1, ..., \u03b4t,K2 ] \u223c Categorical{qt(\u03c3t,K1 + 1), ..., qt(\u03c3t,N )} (43)\nNote that for simplicity, we use sampling with replacement. For sampling without replacement, see the procedure in Kool et al. 2020\nConventional Forward Before calling an existing implementation of Forward, we need to correct the bias of sampled terms. We do this by multiplying the probability of the sample to its emission potential (which becomes addition in log space). Note that this will be equivalent to multiplying them inside the DP:\n\u03c6\u0303\u2032(\u03c3t,i) = \u03c6\u0303(\u03c3t,i) (44)\n\u03c6\u0303\u2032(\u03b4t,i) = \u03c6\u0303(\u03b4t, i) + log qt(\u03b4t,i) (45)\nWe treat any duplicate indices as if they are different, and view the resulting potentials as if they are a new CRF. We also retrieve the transition potentials at each step according to the chosen index. So the new CRF has different transitions across steps. Finally, we run an existing efficient implementation of the Forward algorithm on the new CRF to get the estimated \u03b1\u0302 and Z\u0302."
      },
      {
        "heading": "D EXTENSION TO GENERAL SUM-PRODUCT ALGORITHM",
        "text": "This section discusses the extension of sampled DP to general graph structures and message-passing algorithms, following the Bethe variational principle (Wainwright & Jordan, 2008). Recall the general message-passing algorithm computes pseudo marginals by recursively updating the message at each edge:\nMts(xs)\u2190 \u03ba \u2211 x\u2032t { \u03c6st(xs, x \u2032 t)\u03c6t(x \u2032 t) \u220f u\u2208N(t)/s Mut(x \u2032 t) } (46)\nwhere Mts(xs) denotes the message from node t to node s evaluated at Xs = xs, \u03c6st is the edge potential, \u03c6t is the node potential, N(t)/s denotes the set of neighbor nodes of t except s, and \u03ba the normalization constant. At convergence, we have the pseudo marginals\n\u00b5s(xs) = \u03ba\u03c6s(xs) \u220f\nt\u2208N(s)\nM\u2217ts(xs) (47)\nand the Bethe approximation of the log partition function is given by the evaluation of the Bethe variational problem:\nlogZBethe = \u03c6 \u1d40\u00b5+ \u2211 s Hs(\u00b5s)\u2212 \u2211 s,t Ist(\u00b5st) (48)\nwhere H denotes marginal entropy and I denotes marginal mutual information.\nWe consider the application of randomization to the computation of pseudo marginals and the Bethe log partition. Note that the Bethe approximation will be exact if the underlying graph is a tree. We consider a local proposal q\u0303 combined with a global prior \u03c4 :\nq(xs) = 1\n2 (q\u0303(xs) + \u03c4(xs)) (49)\nwhere the prior \u03c4 depends on our knowledge of specific problem structures. As one can always retreat to uniform proposals, we can explore more advanced choices. For q\u0303, one may consider: (a) local normalization as (\u0303q)(xs) = \u03c6(xs)/ \u2211 s \u03c6(xs) and (b) pre-computed mean-field approximations from algorithms like SVI (Hoffman et al., 2013). For \u03c4 , one can further use a frequency-based empirical prior estimated from the data. To determine which nodes to perform DP on, one may consider the following principles:\n\u2022 Special nodes of interest depending on their actual meaning. For example, one may be interested in some xs = 0 (e.g., if a user is under a bank fraud threat).\n\u2022 Nodes with large local weight \u03c6(xs). \u2022 Nodes with large global weight \u03c4(xs). \u2022 Loop elimination. If two nodes have similarly small local and global weight, we could\ndrop those which eliminate loops in the resulting graph. Also note that we would prefer removing small-weighted nodes for loop reduction.\nWith the above principles, one may construct three subsets to perform the sum-product:\n\u2022 \u21261 including nodes of special interest, where we perform exact computation. \u2022 \u21262 from the top items of the proposal, where we also perform exact computation. \u2022 \u21263 by sampling from the remaining items of the proposal (optionally with loop-reduction).\nFor this set of nodes, one needs to correct the estimation by dividing the proposal probabilty: \u03c6(xs)\u2190 \u03c6(xs)/q(xs).\nAfter these steps, we treat nodes in \u21261 \u22c3 \u21262 \u22c3\n\u21263 as if they are a new model, then feed them to an existing sum-product implementation."
      },
      {
        "heading": "E EXPERIMENT DETAILS",
        "text": ""
      },
      {
        "heading": "E.1 MODEL ARCHITECTURE DETAILS",
        "text": "For training LVMs, partition function estimation, and paraphrasing, we use the Huggingface checkpoint of GPT2 model1 since these experimental variables are more about autoregressive language modeling and generation. For analyzing latent network topologies, we use Huggingface checkpoint of BERT base model2 since it has been the main focus of most previous analytical work. The decoder LSTM is a one-layer LSTM with hidden state size 762, the same as the size of the contextualized embeddings. It shares the embeddings of the inferred states with the encoder, and uses its own input word embeddings, whose size is also 762. This architecture suffices for training LVMs and inferring latent networks. For paraphrase generation, we change the decoder to be conditional by: (a) using the average word embeddings of content words of the input sentence as its initial hidden state (rather than zero hidden states); (b) letting it attend to the embeddings of content words of the input sentence, and copy from them. This effectively makes this decoder conditional on the BOW of the content words of the input sentence. Then decoding a paraphrase becomes a neural version of slot filling: a sequence of latent states by traversing the network becomes a template of the sentence which we fill with words."
      },
      {
        "heading": "E.2 DATASET PREPROCESSING",
        "text": "For the MSCOCO dataset, we use the GPT2 tokenizer to process sentences. As the official website 3 does not release a test split (only a training and a development split), we use the official development split as our test split, and re-split the official training split to be a new training and a development split. For training LVMs, we only use 1/10 of the training set for quicker experiments (the full dataset takes over more than 20 hours to converge). For paraphrasing, we use the full dataset.\nFor the 20News dataset, we use the data from the sklearn website4. We follow its official split and process the sentences with the BERT tokenizer."
      },
      {
        "heading": "E.3 HYPERPARAMETERS AND TRAINING STRATEGIES",
        "text": "To get meaningful convergence of the latent space without posterior collapse, the following techniques are important: (a). set \u03b2 in the correct range. A large \u03b2 force the posterior to collapse to a uniform prior, while a small \u03b2 encourages the posterior to collapse to a Dirac distribution. (b). use word dropout in initial training epochs, otherwise the decoder may ignore the latent code. (c). use potential normalization, otherwise the logsumexp function used in the forward algorithm may saturate and only return the max of the input, this would consequently lead the posterior to collapse to few \u201cactivated\u201d states and other states will never be inferred.\nWe further note that a full DP based entropy calculation will cause memory overflow with automatic differentiation. So we approximate it with local emission entropy, i.e., the sum of the entropy of the normalized local emission factors. Compared with the full entropy, this approximation does not directly influence the transition matrix, but encourages more activated states and mitigates the posterior collapse problem. To get a full entropy regularization, one can further regularize the transition matrix towards an all-one matrix, or extend our randomized DP to entropy computation. As the current local entropy regularization is already effective for inducing a meaningful posterior, we leave the full entropy computation to future work.\nOur reported numbers are averaged over \u201cgood enough\u201d runs. Specifically, for all hyperparameter configurations, we first run three random seeds and get the mean and standard deviation of the performance metrics. If the standard deviation is small enough, we report mean performance metrics (NLL, PPL, and iBLEU) and the standard deviation. If the standard deviation is large, we run extra five seeds. Then we drop runs with bad performance (usually 2-3), and compute the mean and\n1https://huggingface.co/transformers/model_doc/gpt2.html 2https://huggingface.co/transformers/model_doc/bert.html 3https://cocodataset.org/#home 4https://scikit-learn.org/stable/modules/generated/sklearn.datasets.\nfetch_20newsgroups.html\nstandard deviation of the performance metrics again. In total, we experiment more than 100 runs over more than 7 hyperparameters: (a). learning rate 10\u22123, 10\u22124, 5 \u00d7 10\u22124; (b). optimizer: SGD, Adam, AdamW; (c). dropout: 0.2, 0.3, 0.4; (d). \u03b2 variable: 10\u22124, 5 \u00d7 10\u22124, 10\u22123, 10\u22122; (e). Gumbel CRF v.s. Gumbel CRF straight-through (f). K1 v.s. K2; (g). word dropout schedule; and their combinations. Note that different models may achieve best performance with different hyperparameter combinations. We make sure that all models are searched over the same set of hyperparameter combinations, and report average performance metrics over multiple seeds of the best hyperparameter configuration for each model."
      },
      {
        "heading": "E.4 TIME COMPLEXITY ANALYSIS",
        "text": "Table 3 shows the actual efficiency comparison of our method. Time is measured by seconds per batch. For experiments with 500 and 2,000 states, we set K = 100. *When the number of states is small, our method underperforms due to the overhead of constructing the proposal. However, its advantage becomes clear as the number of states becomes large.\nE.5 VISUALIZATION PROCEDURE\nThis section describes how we produce Fig. 3(B1-2) and Fig. 4(A1-4). We use the sklearn implementation 5 of tsne (Van der Maaten & Hinton, 2008). For Fig. 3(B1-2), the word embeddings are obtained by sampling 8,000 contextualized embeddings from the full word occurrences in the 20News training set. Then we put the sampled word embeddings and the 2,000 states into the tsne function. The perplexity is set to be 30. An important operation we use, for better seperating the states from each other, it to manually set the distances between states to be large, otherwise the states would be concentrated in a sub-region, rather than spread over words. Fig. 4 is produced similarly, except we do not use word embeddings as background, and change the perplexity to be 5. For Fig. 4(A3), we connect the states if their transition potential is larger than a threshold. For Fig. 4(A4), we connect the states if their bigram frequency is larger than a threshold. All our decisions of hyperparameters are for the purpose of clear visualization which includes reducing overlapping, overcrowding, and other issues. We further note that no single visualization method can reveal the full structure of high-dimensional data, and any projection to 2-D plain inevitably induces information loss. We leave the investigation of better visualization methods to future work."
      },
      {
        "heading": "E.6 STATE-WORD PAIR EXAMPLES",
        "text": "See Tables 4 and 5 for more sample."
      },
      {
        "heading": "E.7 STATE-STATE TRANSITION EXAMPLES",
        "text": "See Table 6 for transitions involving stopwords and Table 7 for transitions without stopwords. Also note their differences as transitions involving stopwords are more about syntactic constructions and transitions without stopwords are more about specific meaning."
      },
      {
        "heading": "E.8 STATE TRAVERSAL EXAMPLES",
        "text": "See Table 8 for more sample.\n5https://scikit-learn.org/stable/modules/generated/sklearn.manifold. TSNE.html"
      }
    ],
    "year": 2021,
    "abstractText": "The discovery of large-scale discrete latent structures is crucial for understanding the fundamental generative processes of language. In this work, we use structured latent variables to study the representation space of contextualized embeddings and gain insight into the hidden topology of pretrained language models. However, existing methods are severely limited by issues of scalability and efficiency as working with large combinatorial spaces requires expensive memory consumption. We address this challenge by proposing a Randomized Dynamic Programming (RDP) algorithm for the approximate inference of structured models with DP-style exact computation (e.g., Forward-Backward). Our technique samples a subset of DP paths reducing memory complexity to as small as one percent. We use RDP to analyze the representation space of pretrained language models, discovering a large-scale latent network in a fully unsupervised way. The induced latent states not only serve as anchors marking the topology of the space (neighbors and connectivity), but also reveal linguistic properties related to syntax, morphology, and semantics. We also show that traversing this latent network yields unsupervised paraphrase generation.",
    "creator": "LaTeX with hyperref"
  },
  "output": [
    [
      "1. \"There isn\u2019t any novel contribution in this idea, nor a value of such analysis, at least from the perspective to advancing science.\"",
      "2. \"Even empirically, I don\u2019t see any interesting insights from the analysis.\"",
      "3. \"Further, since CRF models are trained using dynamic programming which is efficient in itself but it has quadratic cost in the number of states.\"",
      "4. \"It is proposed to sub-sample upon some of the sequences with low weights, while retraining the ones with high weights. This approach is just a heuristic, with many possible flaws.\"",
      "5. \"For instance, it is proposed to compute the sequences with high weights absolutely without recursive formulation, which should be expensive.\"",
      "6. \"Sampling strategy is not explained well, and I believe challenging to obtain for NLP like high dimensional problems.\"",
      "7. \"Besides, the approximation is applied at coarser level, not accounting for tree like structure emerging from recursion, big sequences as child nodes and subsequences as parent nodes.\"",
      "8. \"Intuitively, something like brand and bound could have helped if there was sampling accounting for tree structure.\"",
      "9. \"It is also worth noting that the proposed sampling approach is not novel but explored in related problems.\"",
      "10. \"Theoretical analysis on zero bias is not enough, and there isn\u2019t sufficient analysis for ensuring low variance.\""
    ],
    [
      "1. \"The main reason for approximating partition calculation is to scale LVMs to more states, but from experiment results, it doesn't seem that scaling LVMs to more states helps performance. For example, in table 1, the PPL of RDP-2k is only 0.19 better than RDP-100 which doesn't seem very significant, and in table 2, RDP-2k gets worse BLEU and iBLEU than RDP-50. Based on existing results in the paper, I cannot tell if it's because the approximation is bad, or because LVMs with more states do not get better performance, but either case is evidence against this work (in the former case, the proposed approximation is bad even though it's better than top-k; in the latter case, why would we want to scale LVMs then?).\"",
      "2. \"Another concerning result is that RDP-100 outperforms FULL-100 in table 1, and RDP-50 outperforms FULL-50 in table 2 other than self-BLEU. How could an approximation be better than exact inference?\"",
      "3. \"The paper only focuses on a single application of the proposed randomized dynamic programming technique. When introducing such a broad technique, it seems natural to explore a range of applications to showcase the generality. It is unknown how it will perform when applied to other dynamic programming problems found in natural language processing or in machine learning.\"",
      "4. \"I don't think the application of analyzing the representation space of pretrained language models makes sense: the CRF is only used to parameterize the inference network, but the generative model is trained on raw sentences (x) and does not use representations from the pretrained model. This setting contradicts the claim that 'the discovered latent network is intrinsic to the representation manifold' (section 1, last paragraph), since the generative model is trained on x instead of representations r (Eq. 12).\"",
      "5. \"Even with a BERT with random weights, I think it is possible to learn meaningful clusters since the generative model is trained on x (so the posterior can still reveal meaningful structures). To me, a more natural setting would be to use an HMM to parameterize the generative model and perform exact marginalization over latent states using the proposed approach, which does not directly use raw words x but only uses representations r and would be truly 'intrinsic to the representation manifold'.\"",
      "6. \"The paraphrasing experiment is not very clear. Where is bag-of-words used? What is the generative process?\"",
      "7. \"Another issue with the paper is the lack of exploration of different sampling strategies. This is another place where I would be interested in results from a different task or domain. A stronger analysis of how the selection of K1 and K2 changes the result might also give insight into improving the method.\""
    ],
    [
      "1. To the best of my knowledge, the RDP mechanism detailed in this paper combines two well studied approaches (i) importance sampling (i)TopK budget [Sun et al. 2019]. As a result, the methodological novelty of the sampling scheme is somewhat limited.",
      "2. Additionally, the latent network modeling approach is quite similar to standard well-studied approaches (linear-chain CRF, long-tailed prior on proposals, amortized variational inference) and provides limited novelty.",
      "3. As discussed in the paper, most linguistic information seems to be captured by 500 states. It would be helpful to compare experimental results for state-word and state-state relations among the Full, TopK, and RDP approaches which should all be feasible for 500 states. In particular, as seen in Figure 2, the benefits of RDP seem especially beneficial in Dense distributions. How does this translate to differences in the learned state-word and state-state connections?",
      "4. Marginal performance benefits in terms of Test PPL on MSCOCO [Table 1] and on Paraphrase generation on MSCOCO [Table 2] when compared to TopK and Full (for small # of states)."
    ],
    [
      "1. \"The first is the difference between the true proposal  a that is proved by the authors to lead to an unbiased estimator but cannot achieve improvements in computational efficiency, and the practically used proposal as in Eq. 10.  Although Fig 2. shows some distributions by using the practically used prospal that show no bias, it remains unclear how much it differs from the true proposal. Maybe a study of distances vs. memory presentation (e.g., K1) could show a better picture of the trade-offs here.\"",
      "2. \"The second issue is that, the conceptual benefit from the proposed method is being able to deal with larger N (the number of latent states) where full Dynamic Programming could not run efficiently. However there is no such experiment showing what would be happen with larger N.\"",
      "3. \"Finally, the experiments visualizing the latent space are for the proposed method only. It would be interesting to see how it compares with methods that only takes top K1 and the vanilla Forward Algorithm (which takes the full N2 transition matrix).\"",
      "4. \"I thanks the authors for the responses which improves the manuscript a lot. Still, I would like to keep my recommendation \"6: marginally above the acceptance threshold\", since (1) the contribution on RDP and that in linguistic application are entangled and do not provide a clear message, and (2) as issues in my reviews and other reviews remain.\""
    ]
  ],
  "review_num": 4,
  "item_num": [
    10,
    7,
    4,
    4
  ]
}