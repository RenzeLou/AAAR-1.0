{
  "ID": "2bO2x8NAIMB",
  "Title": "Should We Be Pre-training? An Argument for End-task Aware Training as an Alternative",
  "URL": "https://openreview.net/forum?id=2bO2x8NAIMB",
  "paper_draft_url": "/references/pdf?id=gYBELHpXNu",
  "Conferece": "ICLR_2022",
  "input": {
    "source": "CRF",
    "title": "Should We Be Pre-training? An Argument for End-task Aware Training as an Alternative",
    "authors": [],
    "emails": [],
    "sections": [
      {
        "heading": null,
        "text": "1 INTRODUCTION\nThe increasingly popular pre-training paradigm (Dai & Le, 2015; Devlin et al., 2018; Gururangan et al., 2020) involves first training a generalist model on copious amounts of easy-to-obtain data, e.g. raw text data in NLP, and then using this model to initialize training on a wide swath of downstream tasks. Generalist models like BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019), and GPT-3 (Brown et al., 2020) have a strong appeal; a few institutions with significant resources incur the cost of training these large models whilst the rest of the research community enjoys a significant performance improvement at minimal computational overhead. However, the advantages of initializing a downstream task from a generalist model are not guaranteed. Previous work has shown that the benefits of pre-training depend heavily on the degree of domain overlap between the end task data and the massive, heterogenous data on which\nthe generalist model was trained (Beltagy et al., 2019; Gururangan et al., 2020; Lee et al., 2020).\nNotably, Gururangan et al. (2020) have demonstrated the benefits of continued pre-training of generalist models using data that is similar to that of the end task. Their approach is formalized into two classes: Domain Adaptive Pre-training (DAPT) and Task Adaptive Pretraining (TAPT) where further stages of pre-training of generalist models are conducted on domain- and task-specific data, respectively. DAPT and TAPT exploit the fact that we often know the end task beforehand, and so we can make specific choices about our pre-training regimen to improve end-task performance.\nHowever, in both pre-training for generalist models and continued pre-training, the training procedure itself does not explicitly incorporate the end task objective function. Because of this, practitioners have to be careful with their choice of auxiliary tasks, the order in which they are trained on, and the early-stopping criteria for each pre-training stage so as to actually achieve good downstream\nend-task performance (Gururangan et al., 2020; Dery et al., 2021). In the absence of principled criteria to make these difficult design choices, it is common to instead resort to the computationally demanding heuristic of pre-training on as much data as possible for as long as possible.\nIn this paper, we ask the following question: \u201cIn settings where we have a particular end task in mind, should we be pre-training at all?\u201d. In response, we advocate for an alternative approach in which we directly introduce the end task objective of interest into the learning process. This results in a suite of end-task aware methods called TARTAN (end-Task AwaRe TrAiniNg). Our formulations incorporate both unsupervised auxiliary objectives traditionally used in NLP pre-training (such as masked language modeling as in Devlin et al. (2018)) and the end-task objective, followed by an optional fine-tuning step on the end task. We motivate TARTAN experimentally by focusing on the more computationally friendly and ubiquitous continued pre-training setting. Based on this, we make the following contributions to the literature on leveraging auxiliary tasks and data:\n\u2022 In lieu of standard end-task agnostic continued pre-training, we suggest introducing the end task objective into the training process via multi-task learning (Caruana, 1997; Ruder, 2017). We call this procedure Multi-Tasking end-Task AwaRe TrAiniNg (MT-TARTAN) (Section 3.1). MTTARTAN is a simple yet surprisingly effective alternative to task-agnostic pre-training. In Section 5, we demonstrate that MT-TARTAN significantly improves performance, convergence time, and data efficiency over Gururangan et al. (2020)\u2019s results. It also obviates the need for fickle hyperparameter tuning through direct optimization of validation performance. \u2022 To allow more fine-grained control of the end task over the auxiliary tasks, in Section 3.2, we present an online meta-learning algorithm that learns adaptive multi-task weights with the aim of improving final end task performance. Our META-learning end-Task AwaRe TrAiniNg (METATARTAN) allows us to robustly modulate between multiple objectives and further improves performance over MT-TARTAN . \u2022 A naive implementation of META-TARTAN based on first-order meta-learning analysis results in a sub-optimal algorithm that ignores all tasks except the end-task. We trace this problem to the use of a single model training head for computing both the end-task training loss and meta-objective (end-task validation loss). To guard against this pathological solution, we introduce a separate model head for computing the meta-objective. In Section 3.3, we justify this simple-to-implement fix and validate its practical efficacy in Section 5.\nOur results suggest that TARTAN may be an attractive alternative to the widely used pre-training paradigm, and further research in this direction is warranted."
      },
      {
        "heading": "2 FORMALIZING PRE-TRAINING AND CONTINUED PRE-TRAINING",
        "text": "Consider a dataset D = {(xi, yi)i\u2208[m]} consisting of m labelled examples. We define a task as an objective function and dataset pair: T = {L(\u00b7), D}. Consider M\u03b8, a model parameterized by \u03b8. The objective function L(yi,M\u03b8(xi)) evaluates how well a model prediction M\u03b8(xi) fits the true label yi, such as cross-entropy loss in the case of classification. Note that the task dataset, D, is typically decomposed into the sets (Dtrain, Dval, Dtest). Dtrain is the set of examples used for model training whilst Dtest is used for final task evaluation. The validation set, Dval, is typically used for model selection but it is also frequently used in meta-learning to define the meta-objective \u2013 LvalT\u2217 . Given a specific end-task T \u2217, our aim is to improve performance on T \u2217 (as measured by the model loss on DtestT\u2217 ) by leveraging auxiliary tasks Taux = {T1, . . . , Tn}. Note that we do not particularly care about the performance of any of the tasks in Taux. We are willing to sacrifice performance on Taux if it improves performance on T \u2217.\nFrom the perspective of model architecture, there are several ways to leverage Taux. We focus on the simple but ubiquitous parameter sharing setting. Here, all tasks share a model body \u03b8body but each task Ti has its own head \u03c6i for prediction. We denote the head belonging to T \u2217 as \u03c6\u2032. Thus \u03b8 = [ \u03b8body; ( \u03c61, . . . , \u03c6n, \u03c6\u2032 )] and \u03b8body is reusable across new tasks."
      },
      {
        "heading": "2.1 PRE-TRAINING",
        "text": "Pre-training is when a model is first trained on Taux before performing a final fine-tuning phase on T \u2217. The motivation behind pre-training is that learning Taux first hopefully captures relevant\ninformation that can be utilized during training of T \u2217. This desire has led to the proliferation of generalist pre-trained models like BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019) and GPT3 (Brown et al., 2020) that have been trained on copious amounts of data. Generalist models have been widely successful at improving downstream task performance when used as initilization.\nWe can formalize the pre-training procedure as follows:\n\u03b80 = argmin\u03b8 ( \u2211 Ti\u2208Taux LTi(\u03b8) )\n(1)\nIn Equation 1, we seek a point \u03b80 that achieves minimal loss on the tasks in Taux. We hope that \u03b80 will be a good starting point for gradient descent on T \u2217. Let g(\u03b80) represent the set of end-points of stochastic gradient descent on an initialization, \u03b80. Stochastic gradient descent from the same initialization can produce different end-points due to differences in hyper-parameters like learning rate, batch size and order, as well as regularization strength. We can write the fine-tuning phase as:\n\u03b8\u2217 = argmin{\u03b8 \u2208 g(\u03b80)} LT\u2217(\u03b8) (2)\nNote that pre-training is end-task agnostic: the pre-training Equation 1 occurs entirely before training on the end-task Equation 2, and does not explicitly incorporate the end-task objective, T \u2217. Since there is no awareness of the end-task during pre-training it is important to carefully choose Taux so that pre-training actually results in improved performance on T \u2217 (Wang et al., 2018a). For text data, past work has found left-to-right language modeling (Peters et al., 2017) and masked language modeling (MLM) (Devlin et al., 2018) to be good choices to include in Taux."
      },
      {
        "heading": "2.2 CONTINUED PRE-TRAINING",
        "text": "Recent work (Beltagy et al., 2019; Gururangan et al., 2020; Lee et al., 2020) showed that downstream performance on T \u2217 can be improved by further adapting generalist models via continued pre-training on a more relevant set of auxiliary tasks. This is equivalent to sequentially performing multiple steps of Equation 1, with different Taux, before finally performing Equation 2 on T \u2217. Domain and Task Adaptive Pre-training Gururangan et al. (2020) present Domain Adaptive PreTraining (DAPT) and Task Adaptive Pre-Training (TAPT) as methods for continued pre-training. During DAPT, a generalist model is further pre-trained on an unsupervised objective with large amounts of data from the same domain as the end-task. TAPT also pre-trains with the same unsupervised objective as DAPT, but on the actual dataset of the end task. Gururangan et al. (2020) find that performance can be further improved by chaining objectives, DAPT first, followed by TAPT.\nThough TAPT and DAPT do not directly incorporate the end-task objective during training, it still indirectly informs both the choice of pre-training data and the order in which the pre-training tasks are trained on. Below, we explore stronger versions of this influence."
      },
      {
        "heading": "3 END-TASK AWARE TRAINING (TARTAN)",
        "text": "In this section, we argue for the end-task to be added directly into the training process to create explicit interactions between T \u2217 and Taux."
      },
      {
        "heading": "3.1 END-TASK AWARE TRAINING VIA MULTI-TASKING (MT-TARTAN)",
        "text": "We propose to directly incorporate knowledge of the end-task by multi-tasking T \u2217 together with Taux, before optionally fine-tuning on T \u2217 exclusively. To this end, we introduce a set of task weights w = (w\u2217, w1, \u00b7 \u00b7 \u00b7 , w|Taux|) satisfying w\u2217 + \u2211 i wi = 1, to modulate between the different losses. Our new formulation is:\n\u03b80 = argmin\u03b8 Ltotal(\u03b8,w) = argmin\u03b8 ( w\u2217LT\u2217(\u03b8) + \u2211 i wiLTi(\u03b8) )\n(3)\nHere, Equation 3 replaces Equation 1 and can be followed by the optional fine-tuning stage of Equation 2. Note that this formulation fixes the tasks weights w throughout the training process. We call this formulation End-task Aware Training via Multi-tasking (MT-TARTAN) because we introduce the end-task directly into the training procedure, and do so by multi-tasking it with Taux.\nMT-TARTAN allows us to prioritize performance on T \u2217 in several ways. First, we can weight the end-task higher than all the other auxiliary tasks. Also, during training, we can monitor LT\u2217 on the end-task validation set and early stop when it plateaus; even if the auxiliary tasks have not yet converged. This is not possible during standard pre-training because we do not train T \u2217 and so it performs at random before we actually start fine-tuning. Early stopping on T \u2217 represents significant computational savings over end-task agnostic pre-training."
      },
      {
        "heading": "3.2 END-TASK AWARE TRAINING VIA META-LEARNING (META-TARTAN)",
        "text": "MT-TARTAN, DAPT and TAPT, all share the same drawback: they implicitly assume that the auxiliary tasks have static importance to the end-task over the lifetime of its training, either by being end-task agnostic (DAPT and TAPT) or by having static task weights (MT-TARTAN). With MTTARTAN, an additional drawback noted by Wang et al. (2019); Yu et al. (2020) is that multi-tasking can negatively impact task performance compared to isolated training. These shortcomings motivate the formulation of an adaptive algorithm that can mitigate the negative influence of some tasks whilst responding to the changing relevance of auxiliary tasks over the lifetime of end-task training.\nAs they stand, the pre-training equation pair (Equations 1, 2) and the MT-TARTAN pair (Equations 2, 3) are decoupled. The inner-level variables of the pre-training phase do not depend on the outerlevel variables of the fine-tuning phase. Thus the equation pairs are typically solved sequentially. We propose to tightly couple Equations 2 and 3 by formulating jointly learning w and \u03b80 as a bi-level optimization problem. A bi-level formulation allows us to leverage meta-learning (Schmidhuber, 1995) techniques to learn adaptive task weights which capture variable auxiliary task importance whilst mitigating the contribution of harmful tasks. We propose a meta-learning algorithm in the mold of Model Agnostic Meta-Learning (MAML) (Finn et al., 2017) to learn task weights. As a bi-level problem, this can be formulated as :\n\u03b8\u2217,w\u2217 = argmin{\u03b8 \u2208 g(\u03b80), w} LT\u2217(\u03b8) (4)\nwhere\n\u03b80 = argmin\u03b8 Ltotal(\u03b8,w) = argmin\u03b8 ( w\u2217LT\u2217(\u03b8) + \u2211 Ti\u2208Taux wiLTi(\u03b8) )\n(5)\nWe want to jointly learn w, with \u03b80, such that taking a gradient descent step modulated by w leads to improvement in end-task generalization. We use performance on the end-task validation set (DvalT\u2217 ) as a meta-objective to train w. Performance on DvalT\u2217 serves as a stand-in for end-task generalization performance whilst also naturally capturing the asymmetrical importance of T \u2217.\nOur joint descent algorithm proceeds as follows. At each timestep t, we hold the task weights fixed and update \u03b8t based on \u2207\u03b8Ltotal(\u03b8t,w). We then proceed to update w via gradient descent on the end-task validation loss at \u03b8t+1. For this, we derive an approximation for\u2207wLvalT\u2217 (\u03b8t+1,w) below:\nLvalT\u2217 (\u03b8t+1(w)) = LvalT\u2217 ( \u03b8t \u2212 \u03b2 ( w\u2217\u2207LT\u2217 + \u2211 i wi\u2207LTi ))\n\u2248 LvalT\u2217 (\u03b8t)\u2212 \u03b2 ( w\u2217\u2207LT\u2217 + \u2211 i wi\u2207LTi )T \u2207LvalT\u2217 (\u03b8t)\nWe can take the gradient of the above first-order approximation w.r.t an individual weight wi. This tells us how to update wi to improve the meta-objective.\n\u2202LvalT\u2217 (\u03b8t+1(w)) \u2202wi\n\u2248 \u2212\u03b2 ( \u2207LTi )T (\u2207LvalT\u2217 (\u03b8t)) = \u2212\u03b2(\u2207LTi)T (\u2207LvalT\u2217 ([\u03b8body, \u03c6\u2032]t)) (6) In Equation 6, we explicitly specify [ \u03b8body, \u03c6 \u2032] t\nbecause computing losses on T \u2217 depend on only these parameters. LTi depends solely on [ \u03b8body, \u03c6 i ] t but we leave this out to avoid notation clutter.\nOur analysis above is similar to that of Lin et al. (2019) with one key difference: we learn a weighting for the main task w\u2217 too. This ability to directly modulate T \u2217 allows us to capture the fact that at certain stages in training, auxiliary tasks may have greater impact on end-task generalization than the end-task\u2019s own training data. This choice also allows us to control for over-fitting and the influence of bad (mislabelled or noisy) training data."
      },
      {
        "heading": "3.3 INTRODUCING A SEPARATE CLASSIFICATION HEAD FOR META-LEARNING",
        "text": "Observe that from Equation 6, updates forw 6= w\u2217 involve gradients computed from different model heads \u03c6i and \u03c6\u2032 whilst for w\u2217, we are taking the dot product of gradients from the same end-task head \u03c6\u2032. As we will show empirically in Section 5.4, computing weight updates this way creates a strong bias towards the primary task, causing w\u2217 to rail towards 1 whilst the other weights dampen to 0, which may be sub-optimal in the long run.\nIntuitively, this short-horizon (greedy) behavior makes sense: the quickest way to make short-term progress (improve LvalT\u2217 (\u03b8t+1)) is to descend solely on T \u2217. More formally, the greedy approach arises because we derive \u2207wiLvalT\u2217 (\u03b8t+1) in Equation 6 as a proxy for the gradient at \u03b8\u2217, the outerloop end-point in Equation 4. Variations of this substitution are common in the meta-learning literature (Finn et al., 2017; Liu et al., 2018; Nichol et al., 2018) because it is computationally infeasible to train a model to convergence every time we wish to compute\u2207wiLvalT\u2217 (\u03b8\u2217).\nTo remedy the greedy solution, instead of estimating \u2207\u03b8LT\u2217 and \u2207\u03b8LvalT\u2217 from the same classification head (Equation 6), we introduce a special head \u03c6\u2217 for computing the meta-objective. Specifically, instead of trying to compute \u03b8\u2217, we approximate it by fixing the body of the network \u03b8body and training the randomly initialized head \u03c6\u2217 to convergence on a subset of the end-task training data. We do this every time we wish to estimate\u2207wiLvalT\u2217 (\u03b8\u2217). Introducing \u03c6\u2217 eliminates the strong positive bias on w\u2217 and enables us to compute a better proxy for the meta-gradient at \u03b8\u2217:\n\u2202LvalT\u2217 (\u03b8\u2217(w)) \u2202wi\n\u2248 ( \u2207\u03b8LTi )T (\u2207\u03b8LvalT\u2217 ([\u03b8body;\u03c6\u2217]t)) (7) Equation 7 represents a simple-to-implement alternative to Equation 6. We provide a more detailed justification for Equation 7 in Appendix A.1. In Section 5.4, we empirically validate that the transition from Equation 6 to 7 improves performance whilst mitigating pathological solutions. Our approach of creating \u03c6\u2217 for approximating the meta-objective (down-stream validation performance) is inspired by Metz et al. (2018), who use a similar technique to construct a meta-objective for evaluating the quality of unsupervised representations.\nSee Algorithm 1 in Appendix A.3 for details about META-TARTAN."
      },
      {
        "heading": "4 EXPERIMENTAL SETUP",
        "text": "In keeping with previous work (Devlin et al., 2018; Gururangan et al., 2020; Lee et al., 2020), we focus on Taux as a set of MLM tasks on varied datasets. In the case of DAPT and our end-task aware variants of it, Taux is an MLM task with data from the domain of the end-task. For TAPT, Taux is an MLM task with data from the end-task itself. DAPT, TAPT and DAPT+TAPT (chained pre-training with DAPT followed by TAPT) will serve as our baseline pre-training approaches. We will compare these baselines to their end-task aware variants that use MT-TARTAN and META-TARTAN.\nOur choice of baselines reflect our focus on the continued pre-training setting (Section 2.2) instead of pre-training from scratch as with generalist models (Section 2.1). The continued pre-training setting is more ubiquitous amongst everyday practitioners as it is less computationally demanding. It thus lends itself more easily to extensive exploration. We leave investigating end-task-aware training from scratch to future work.\nDatasets Our experiments focus on two domains: computer science (CS) papers and biomedical (BIOMED) papers. We follow Gururangan et al. (2020) and build our CS and BIOMED domain data from the S2ORC dataset (Lo et al., 2019). We extract 1.49M full text articles to construct our CS corpus and 2.71M for our BIOMED corpus. Under both domains, our end-tasks are low-resource classification tasks. Using low-resource tasks allows us to explore a setting where pre-training can have a significant impact.\nUnder the CS domain, we consider two tasks: ACL-ARC (Jurgens et al., 2018) and SCIERC (Luan et al., 2018). ACL-ARC is a 6-way citation intent classification task with 1688 labelled training examples. For SCIERC, the task is to classify the relations between entities in scientific articles. This task has 3219 labelled examples as training data. We choose CHEMPROT (Kringelum et al., 2016) as the classification task from the BIOMED domain. This task has 4169 labelled training\nexamples and the goal is to classify chemical-protein interactions. More details of these datasets can be found in Table 2 of Gururangan et al. (2020). Gururangan et al. (2020) evaluate against all 3 tasks and their available code served as a basis on which we built MT-TARTAN and META-TARTAN.\nModel Details We use a pre-trained RoBERTabase (Liu et al., 2019) as the shared model base and implement each task as a separate multi-layer perceptron (MLP) head on top of this pre-trained base. As in Devlin et al. (2018), we pass the [CLS] token embedding from RoBERTabase to the MLP for classification.\nTraining Details For DAPT and TAPT, we download the available pre-trained model bases provided by Gururangan et al. (2020). To train thier corresponding classification heads, we follow the experimental setup described in Appendix B of Gururangan et al. (2020).\nPerforming end-task aware training introduces a few extra hyper-parameters. We fix the other hyperparameters to those used in Gururangan et al. (2020). MT-TARTAN and META-TARTAN introduce joint training of a classification head for the end-task T \u2217. We experiment with batch sizes of 128, 256 and 512 for training this head. We try out learning rates in the set {10\u22123, 10\u22124, 10\u22125} and drop out rates of {0.1, 0.3}. For META-TARTAN since we are now learning the task weights, w, we test out task weight learning rates in {10\u22121, 5 \u00d7 10\u22122, 3 \u00d7 10\u22122, 10\u22122}. Note that for all MTTARTAN experiments we use equalized task weights 1|Taux|+1 . A small grid-search over a handful of weight configurations did not yield significant improvement over the uniform task weighting. We use the Adam optimizer (Kingma & Ba, 2014) for all experiments.\nAs mentioned in section 3.3, we train as separate meta-classification head, \u03c6\u2217, to estimate the validation meta-gradients. To estimate \u03c6\u2217, we use batch sizes of {16, 32} samples from T \u2217\u2019s train set. We regularize the meta-head with l2 weight decay and set the decay constant to 0.1. We use a learning rate 10\u22123 to learn the meta-head. We stop training \u03c6\u2217 after 10 gradient descent steps."
      },
      {
        "heading": "5 RESULTS AND DISCUSSION",
        "text": "In this section, we will discuss the results of comparing our models against DAPT and TAPT baselines.1 Broadly, we demonstrate the effectiveness of end-task awareness as improving both performance and data-efficiency."
      },
      {
        "heading": "5.1 END-TASK AWARENESS IMPROVES OVER TASK-AGNOSTIC PRE-TRAINING",
        "text": "Table 1 compares TAPT to its end-task aware variants. Just as in Gururangan et al. (2020), we observe that performing task adaptive pre-training improves upon just fine-tuning RoBERTa. However, note that introducing the end-task by way of multi-tasking with the TAPT MLM objective leads to a significant improvement in performance. This improvement is consistent across the 3 tasks we evaluate against. We find that both MT-TARTAN and META-TARTAN achieve similar results in this setting. In Appendix A.4, we show that the gains from end-task awareness are not restricted to only text data; we observe similar trends for image classification.\n1Our results are slightly different from those presented in Table 5 of Gururangan et al. (2020) in terms of absolute values but the trends observed there still hold here. We attribute these differences to (1) minor implementation differences, and (2) averaging performance over ten seeds instead of five as used in the original paper in order to more strongly establish statistical significance. We observe slightly lower performance on ACL-ARC and SCIERC tasks due to these changes and higher performance on CHEMPROT."
      },
      {
        "heading": "5.2 END-TASK AWARENESS IMPROVES DATA-EFFICIENCY",
        "text": "Gururangan et al. (2020) train DAPT on large amounts of in-domain data in order to achieve results that are competitive with TAPT. They use 7.55 billion tokens in the case of the BIOMED domain and 8.10 billion tokens for the CS domain. This is on average over 104\u00d7 the size of the training data of our end-tasks of interest. The large amount of data required to train a competitive DAPT model represents a significant computational burden to the every-day practitioner. This begets the question: are such large amounts of auxiliary data necessary for achieving good downstream performance? To answer this, we train DAPT and its TARTAN version on variable amounts of data for both SCIERC and ACL-ARC tasks.\nTARTAN is more data-efficient than DAPT In Figure 2, we focus on training on a small fraction of available domain data n = {100, 101} \u00d7 |Train| for the DAPT auxiliary task. Full domain data is n\u2032 \u2248 104 \u00d7 |Train|. This relatively low auxiliary data regime represents a realistic setting that is akin to those encountered by everyday practitioners who are likely to be computationally constrained. As can be seen in Figure 2, on the ACL-ARC task, META-TARTAN matches the performance of DAPT when the sizes of the domain data and end-task data are of the same order (100). At this data size, META-TARTAN supersedes DAPT on the SCIERC task. When trained on 10\u00d7 more auxiliary data, META-TARTAN supersedes DAPT in performance on both tasks. META-TARTAN establishes a new state-of-the-art result (82.081.19) on the SCIERC task. On the ACL-ARC task, META-TARTAN achieves 71.194.88, which is close to DAPT\u2019s performance of 72.493.28 using more than 103\u00d7 auxiliary data. These results indicate that end-task awareness can improve data-efficiency and in this case, improvements are on the order of 1000\u00d7.\nTARTAN is more data-efficient than DAPT+TAPT Table 2 compares DAPT and DAPT+TAPT (DAPT followed by TAPT) to *-TARTAN which multi-task DAPT, TAPT and the end-task. MTTARTAN and META-TARTAN significantly outperform DAPT and DAPT+TAPT in 2 of the tasks whilst giving higher average performance in the ACL-ARC task. We thus conclude that end-task awareness allows us to get a greater performance boost out of the same amount of data.\nWe explore the data efficiency of TARTAN methods even further by comparing the relatively data-poor versions of MT-TARTAN and META-TARTAN above (n = 10 \u00d7 |Train|) to the DAPT and DAPT+TAPT variants trained on all the available domain data (n\u2032 \u2248 104 \u00d7 |Train|). We can see from Table 3 that for the CS domain, our end-task aware variants come close to (ACL-ARC) and even supersede (SCIERC) the end-task agnostic variants though trained with \u2248 1000\u00d7 less data. For BIOMED domain (CHEMPROT task), increasing the amount of data drastically improves the performance of end-task agnostic variants compared to MT-TARTAN and META-TARTAN trained on much less data.\nZhang et al. (2020) show that different tasks exhibit sigmoid-like curves in terms of how much pretraining data is required to achieve good results before performance levels off. We contextualize Tables 2 and 3 within said work and posit that the CHEMPROT task intrinsically requires much more data (compared to our other tasks) before performance begins to improve appreciably."
      },
      {
        "heading": "5.3 META-TARTAN MORE EFFECTIVELY UTILIZES OUT-OF-DISTRIBUTION AUXILIARY",
        "text": "DATA OVER MT-TARTAN\nTARTAN ACL-ARC SCIERC CHEMPROT MT 69.270.96 81.530.99 80.263.79 META 71.194.88 82.081.19 82.310.75\ndata for masked language modelling, DAPT uses heterogeneous domain data whose impact on the end-task performance is less clear. Notice from Table 4 that when required to rely solely on domain data for auxiliary tasking, META-TARTAN improves performance over MT-TARTAN. We attribute META-TARTAN\u2019s improvement over MT-TARTAN on its ability to more flexibly adapt to incoming data of variable utility to the end-task."
      },
      {
        "heading": "5.4 TASK WEIGHTING STRATEGIES DISCOVERED BY META-LEARNING",
        "text": "To illustrate the importance of the separate classification head \u03c6\u2217 for computing the meta-signal for the task weights (described in Section 3.3), we run META-TARTAN experiments with ACL-ARC as the end-task and DAPT as the auxiliary task. We compare using either a separate (\u03c6\u2217) or the same (\u03c6\u2032) classification head for calculating the meta-gradient. Figure 3 plots the task weightings learned in each setting during training. We can clearly see that using a separate head counteracts the pathological solution of down-weighting all tasks that are not T \u2217 and as a result, improves performance: a delta of 1.7 F1 points in this case. The strategy discovered by META-TARTAN presents an interesting contrast to classical pre-training: whilst the initial phase of classical pre-training involves solely the auxiliary task, early in training, META-TARTAN up-weights the auxiliary task but does not fully zero out the end-task. Later in training, we see leveling off of weights instead of railing the end-task to 1 as in classical pre-training.\nNext, we plot a similar graph for using both DAPT and TAPT across our three tasks in Figure 4. From the figure, it is apparent that META-TARTAN discovers similar task-weighting strategies across different end-tasks. This suggests that the MLM objective and META-TARTAN\u2019s strategy for learning task weights are generic enough to induce similar behaviours across tasks. In general, DAPT is significantly up-weighted compared to the end-task and TAPT. Note that the TAPT + ACL-ARC task weights (Figure 4) has the same approximate trajectory as ACL-ARC task weight in\nFigure 3. It seems important to assign high weight to the task data but not necessarily all of it needs to go to the actual task loss. We hypothesize that the diversity in the domain data counteracts overfitting to the end-task data (all our tasks are low-resource) and results in DAPT being up-weighted."
      },
      {
        "heading": "6 RELATED WORK",
        "text": "Multi-task learning can be traced back to seminal work by Caruana (1995), Caruana (1997), and has since been the subject of a flourishing literature, recent surveys of which can be found in Ruder (2017) or Zhang & Yang (2021). In NLP, while initial work from Collobert & Weston (2008) already showed the benefits of multi-task learning, it has only recently become a central topic in the field, with the advent of multi-task benchmarks (Wang et al., 2018b; McCann et al., 2018).\nPre-training is a variation on the multi-task learning paradigm where a machine learning model is first trained on a generic (generally more data-rich) task, before being fine-tuned on an end task. In NLP this practice dates back to the use of pre-trained word embeddings (Turian et al., 2010; Mikolov et al., 2013) and later pre-trained encoders (Kiros et al., 2015; Dai & Le, 2015). Peters et al. (2018) and Howard & Ruder (2018) heralded a renaissance of pre-training before BERT (Devlin et al., 2018) and its many offshoots (Liu et al., 2019; Yang et al., 2019; Lewis et al., 2019) cemented it as the de facto standard for modern NLP.\nAs an idea, meta-learning, or \u201clearning to learn\u201d, dates back to early work from Schmidhuber (1995); Thrun (1998). More relevant to our work however is a modern strand of the literature concerned with gradient-based meta-learning for solving bi-level optimization problems, first popularized by Finn et al. (2017) and followup work (Nichol et al., 2018; Rajeswaran et al., 2019) for few-shot learning. This method has transferred to a variety of applications such as architecture search (Liu et al., 2018), multilingual data weighting (Wang et al., 2020) and model poisoning (Kurita et al., 2020)."
      },
      {
        "heading": "7 CONCLUSION",
        "text": "In this work, we advocate for a paradigm shift in the way we approach pre-training. We have motivated making pre-training more end-task aware when the end task is known in advance. Our work introduced two novel end-task aware training algorithms: End-task Aware Training via Multitasking (MT-TARTAN) and End-task Aware Training via Meta-learning (META-TARTAN). In Section 5, through experiments on 3 different datasets, we demonstrated the ability of our proposed algorithms to improve performance and data-efficiency over their end-task agnostic counterparts.\nThis work suggests several promising directions for future work. First, it would be interesting to see if end-task aware training of generalist models like BERT and RoBERTa from scratch result in improved accuracy and efficiency; both in terms of data and computation. Also, instead of learning coarse task level weights, can further performance improvements be achieved via finer-grained example level weighting as in Wang et al. (2020)? Can meta-learning algorithms like METATARTAN enable more effective utilization of previously discarded (Aroca-Ouellette & Rudzicz, 2020) pre-training auxiliary tasks like Next Sentence Prediction (NSP) (Devlin et al., 2018)? We hope this work spurs conversation around these questions and many more."
      },
      {
        "heading": "8 ETHICS STATEMENT",
        "text": "Our work introduces new algorithms but leverages pre-existing datasets and models. Overall, this work inherits some of the risk of original work upon which it is implemented. Algorithms for continued training such as TAPT and DAPT necessitate per-task training of unsupervised objectives which result in corresponding green-house emissions due to energy consumption (Strubell et al., 2019). However, as shown in Sections 3 and 5, our new compute-efficient algorithms greatly increase the data efficiency of these algorithms, reducing these harms as well as the various harms associated with labor for data-collection (Jo & Gebru, 2020). Also, since our work is set in the context of pre-existing datasets and models (Section 4), we recognize that any ethical issues that have been revealed in these (such as bias (Bender et al., 2021) or privacy leakage (Carlini et al., 2021)) may also propagate to models trained using our work, and mitigation strategies such as Schick et al. (2021); Liang et al. (2021) may be necessary. Finally, there is a potential risk in META-TARTAN that leveraging a validation set for defining the meta-objective could amplifying bias that exists in this data split, although this is done indirectly through task weighting and hence we believe that this risk is small."
      },
      {
        "heading": "9 REPRODUCIBILITY STATEMENT",
        "text": "We pledge to release the source-code for this project to improve the ease of reproducibility of our results by the NLP and machine learning communities. In Section 4, we have specified details about datasets, training regimes and models to allow anyone who wishes to reproduce our results without our original source code to do so. Our discussion of the algorithmic and evaluation details can be found in Appendices A.1, A.3 and A.2. As we noted in 4, we build off of Gururangan et al. (2020)\u2019s implementations which can be found at https://github.com/allenai/dont-stop-pretraining."
      },
      {
        "heading": "A APPENDIX",
        "text": "A.1 JUSTIFYING THE INTRODUCTION OF A META-HEAD\nProof. To arrive at Equation 7 we start with the closed form solution for \u2207wiLvalT\u2217 (\u03b8\u2217) and then introduce approximations in order to produce Equation 7. First, note that :\n\u2202LvalT\u2217 (\u03b8\u2217(w)) \u2202wi =\n( \u2207\u03b8LvalT\u2217 (\u03b8\u2217(w)) )T( \u2207wi\u03b8\u2217(w) ) [Chain rule] (8)\nTo get \u2207wi\u03b8\u2217(w) we invoke the Cauchy Implicit Function Theorem (IFT) as with Lorraine et al. (2020); Navon et al. (2020); Liao et al. (2018):\n\u2207wi\u03b8\u2217(w) = [ \u22072\u03b8Ltotal(\u03b8\u2217(w)) ]\u22121[ \u2207wi\u2207\u03b8Ltotal(\u03b8\u2217(w)) ] [IFT]\n= [ \u22072\u03b8Ltotal(\u03b8\u2217(w)) ]\u22121[ \u2207wi\u2207\u03b8 ( w\u2217LT\u2217(\u03b8\u2217(w)) + \u2211 Ti\u2208Taux wiLTi(\u03b8\u2217(w)) )]\n= [ \u22072\u03b8Ltotal(\u03b8\u2217(w)) ]\u22121[ \u2207\u03b8LTi(\u03b8\u2217(w)) ] [Only terms with wi survive]\nBringing it all together, we get :\n\u2202LvalT\u2217 (\u03b8\u2217(w)) \u2202wi =\n( \u2207\u03b8LvalT\u2217 (\u03b8\u2217(w)) )T([ \u22072\u03b8Ltotal(\u03b8\u2217(w)) ]\u22121[ \u2207\u03b8LTi(\u03b8\u2217(w)) ]) (9)\nComputing \u2207wiLvalT\u2217 (\u03b8\u2217) from Equation 9 is computationally unwieldy since we would not only have to optimize \u03b8 to convergence for every step of wi but we would also have to invert the Hessian of a typically large model. Our middle ground between Equations 9 and 6 (Equation 7) makes use of the following approximations:\n\u2022 We approximate the inverse Hessian with the identity. This approximation is not new; we follow previous work like Lorraine et al. (2020)(Table 3) who explore the use of this approximation because of computational efficiency.[\n\u22072\u03b8Ltotal(\u03b8\u2217(w)) ]\u22121\n= lim i\u2192\u221e i\u2211 j=0 ( I\u2212\u22072\u03b8Ltotal(\u03b8\u2217(w)) )j \u2248 I\nWe are assuming the contribution of terms with i > 0 are negligible.\n\u2022 Instead of training the whole network to convergence, at each time-step, we fix the body of the network and train a special head \u03c6\u2217 to convergence on a small batch of end-task training data. We then use [\u03b8body;\u03c6\u2217] as a proxy for \u03b8\u2217. This is a computationally feasible workaround to training all of \u03b8 to convergence to get a single step gradient estimate. Especially in the continued pre-training setting where a pre-trained generalist model like BERT is used as \u03b8body, this approximation is reasonable. To our knowledge, we are the first to suggest this approximation.\n\u2207\u03b8LvalT\u2217 (\u03b8\u2217)\u2192 \u2207\u03b8LvalT\u2217 ([\u03b8body;\u03c6\u2217])\n\u2022 Above, we have approximated \u03b8\u2217 = [\u03b8body;\u03c6\u2217]. Since \u03c6\u2217 is only used to evaluate end-task (T \u2217) validation data, it means \u03b8 remains unchanged with respect to the training data for task Ti. Thus \u2207\u03b8LTi([\u03b8body; ( \u03c6\u2217, . . . , \u03c6i ) ]) = \u2207\u03b8LTi([\u03b8body;\u03c6i]) = \u2207\u03b8LTi(\u03b8)\nBringing it all together, we get Equation 7, repeated here:\n\u2202LvalT\u2217 (\u03b8\u2217(w)) \u2202wi\n\u2248 ( \u2207\u03b8LTi )T (\u2207\u03b8LvalT\u2217 ([\u03b8body;\u03c6\u2217]t))\nA.2 CALCULATING P-VALUES FROM PERMUTATION TEST\nWe used the permutation test (Good, 2005; Dror et al., 2018) to test for statistical significance. For each test, we generate 10000 permutations to calculate significance level. This is sufficient to converge to a stable p-value without being a computational burden. We chose this over the common student t-test because :\n1. We have only 10 runs per algorithm and permutation tests are more robust at low sample size\n2. Permutation test is assumption free. Student t-tests assume that the samples are normally distributed\n3. Permutation test is robust to variance in the samples, so even though error-bars can overlap, we still establish significant differences in the samples. Variance in our results is expected due to small dataset sizes of end-tasks.\nA.3 ALGORITHM FOR META-TARTAN\nAlgorithm 1: End-task Aware Training via Meta-learning (META-TARTAN) Require: T \u2217,Taux: End-task, Set of auxiliary pre-training tasks Require: \u03b7, \u03b21, \u03b22: Step size hyper-parameters Initialize :\nPre-trained RoBERTa as shared network body, \u03b8body Task weightings: w\u2217, wi = 1|Taux|+1\nRandomly initialize : end-task head as \u03c6\u2032 meta head for end-task as \u03c6\u2217 task head, \u03c6i, for each Ti \u2208 Taux while not done do B\u2217tr \u223c T \u2217train // Sample a batch from end-task\ng\u2217\u03b8 , g \u2217 \u03c6 \u2190 [ \u2207\u03b8,\u2207\u03c6\u2032 ]( LT\u2217(\u03b8, \u03c6\u2032, B\u2217tr) ) // Get end-task grads\ngi\u03b8, g i \u03c6 \u2190 [ \u2207\u03b8,\u2207\u03c6i ]( LTi(\u03b8, \u03c6i, Bi) ) // Get task grads. \u2200i \u2208 [n], Bi \u223c Ti\n// Learn a new meta head \u03c6\u2217 \u2190 estimate meta head(B\u2217tr, \u03b22, \u03b8, \u03c6\u2217) // B\u2217tr \u223c T \u2217train g\u2217meta \u2190 \u2207\u03b8LT\u2217(\u03b8, \u03c6\u2217, B\u2217val) // B\u2217val \u223c T \u2217val // Update task weightings w\u2217 \u2190 w\u2217 + \u03b7 cos(g\u2217meta, g\u2217\u03b8) wi \u2190 wi + \u03b7 cos(g\u2217meta, gi\u03b8) // Update task parameters \u03b1\u2217, \u03b11, . . . , \u03b1|Taux| = softmax(w \u2217, w1, . . . , w|Taux|)\nUpdate \u03b8body \u2190 \u03b8body \u2212 \u03b21 ( \u03b1\u2217g\u2217\u03b8 + \u2211 i \u03b1ig i \u03b8 ) Update ( \u03c6i \u2190 \u03c6i \u2212 \u03b22gi\u03c6 ) , ( \u03c6\u2032 \u2190 \u03c6\u2032 \u2212 \u03b22g\u2217\u03c6\n) end Result : \u03b8, \u03c6\u2032\nA.4 VISION EXPERIMENTS\nWe validate that the gains from end-task Aware Training are not siloed to only learning from text. We conduct an experiment comparing end-task aware training on images to its end-task agnostic variant. We use the Cifar100 dataset (Krizhevsky et al., 2009). We use the Medium-Sized Mammals superclass (one of the 20 coarse labels) as our main task whilst the other 19 super classes are used as auxiliary data. Our primary task is thus a 5-way classification task of images different types of\nmedium-sized mammals whilst whilst the remaining 95 classes are grouped into a single auxiliary task.\nAs can be seen from Table 5, being end-task aware improves over task agnostic pre-training. We find that, again, when our auxiliary task consist of solely domain data and no task data, METATARTAN performs better than MT-TARTAN (as measured by averaged performance).\nA.5 FULL TAPT TABLE WITH SIGNIFICANCE LEVELS\nWe repeat Table 1 and provide details about levels of statistical signifance.\nA.6 FULL DAPT/DAPT+TAPT TABLE\nWe repeat Table 3 and provide details about levels of statistical signifance."
      }
    ],
    "year": 2021,
    "abstractText": "Pre-training, where models are trained on an auxiliary objective with abundant data before being fine-tuned on data from the downstream task, is now the dominant paradigm in NLP. In general, the pre-training step relies on little to no direct knowledge of the task on which the model will be fine-tuned, even when the end-task is known in advance. Our work challenges this status-quo of end-task agnostic pre-training. First, on three different low-resource NLP tasks from two domains, we demonstrate that multi-tasking the end-task and auxiliary objectives results in significantly better downstream task performance than the widely-used task-agnostic continued pre-training paradigm of Gururangan et al. (2020). We next introduce an online meta-learning algorithm that learns a set of multi-task weights to better balance among our multiple auxiliary objectives, achieving further improvements on end task performance and data efficiency.",
    "creator": "LaTeX with hyperref"
  },
  "output": [
    [
      "1. Intuitively, it is not surprising that the proposed method performs better than other end-task agnostic pretraining.",
      "2. While the paper argues that the end-task aware setting, when done correctly, will save computational power, it does not consider the comparisons when there are multiple end-tasks involved.",
      "3. When there are a large number of end-tasks involved, it become daunting to train/pre-train large models for each end-task."
    ],
    [
      "1. \"Lacking comparisons with other multi-task learning (MTL) work about weight selection\"",
      "2. \"The setting studied here is kind of limited. Only focusing on low-resource classification tasks.\"",
      "3. \"The experiments cover three datasets and show mixed results.\"",
      "4. \"The META-TARTAN doesn\u2019t show much improvement without OOD auxiliary data.\"",
      "5. \"According to Table 1, the META learning version seems only slightly on two tasks while worse on the other for TAPT. Is there any explanation here?\"",
      "6. \"It\u2019s more convincing to add more fine-tuning datasets and analyze the effect of dataset size and final performance for the proposed method.\"",
      "7. \"On the other hand, the proposed method always needs to mix in pre-training data to fine-tuning tasks. Given this, TARTAN also introduces overhead and it\u2019s not 100% true that DAPT is more data efficient given that you might only need to go through it once. Not to mention many domains actually have plenty of unlabeled corpus.\"",
      "8. \"What are the number of auxiliary tasks? Seems like for DAPT, TAPT and DAPT+TA, there\u2019s only one auxiliary task? The MLM on either the domain corpus or the target corpus? If that\u2019s the case, then it seems to be too few tasks to be worth selecting from.\"",
      "9. \"How hard is it to extend the proposed method to generation tasks instead of discriminative tasks?\"",
      "10. \"It\u2019s until the experimental section that I realize the paper focuses on continued pre-training rather than pre-training from scratch. I guess it requires some revision to make that clearer at the beginning of the paper.\""
    ],
    [
      "1. The description of the META-TARTAN method is not clear enough.",
      "2. A figure illustrating the data, parameters and optimization steps for META-TARTAN would be very helpful. (e.g., illustrating the relation of \u03b8body,\u03d51,\u03d52,...,\u03d5\u2032, how meta-objective is computed using weights w and parameters \u03b8, where does the separate classification head come in, ...)",
      "3. I get to understand the method better (e.g., whether \u03d5\u2217 is re-initialized at every time stamp t, whether \u03d5\u2032 is still being optimized when \u03d5\u2217 appears) only after seeing Algorithm 1 in the appendix. If space allows, please enrich the Sec 3.2-3.3 or move the algorithm to the main text.",
      "4. Though the authors put lots of efforts in META-TARTAN, it appears that META-TARTAN is comparable with MT-TARTAN in most cases. META-TARTAN seems to be better in the DAPT-only setting (Sec 5.3); however since we're doing task-aware pre-training, TAPT can be easily achieved in this case. I wonder what is the practical utility of META-TARTAN.",
      "5. The paper needs more thorough discussion on computation costs. Computation savings is claimed to be one major advantage of TARTAN. However the comparison is made according to training iterations/steps, while one step in META-TARTAN is much more computationally-expensive than one step in DAPT or TAPT. Please take this into consideration when discussion computation savings."
    ],
    [
      "1. \"I think the general framing of their paper in abstract / introduction is misleading.\"",
      "2. \"At no point do they train a model from scratch (i.e. without pre-training) with their proposed methods.\"",
      "3. \"So although they raise the question whether pre-training is necessary, they then don\u2019t actually compare against a model that is not pre-trained.\"",
      "4. \"I think the paper would be much stronger if they did not defer their main question (\u201cShould we be pre-training?\u201d) to future work, but rather tested their method with the typical MLM auxiliary task on a newly initialized Transformer model.\""
    ]
  ],
  "review_num": 4,
  "item_num": [
    3,
    10,
    5,
    4
  ]
}