{
  "ID": "_4n3k3d1ob",
  "Title": "Continuous-time identification of dynamic state-space models by deep subspace encoding",
  "URL": "https://openreview.net/forum?id=_4n3k3d1ob",
  "paper_draft_url": "/references/pdf?id=1ykd8g6mpA",
  "Conferece": "ICLR_2023",
  "input": {
    "source": "CRF",
    "title": "Continuous-time identification of dynamic state-space models by deep subspace encoding",
    "authors": [],
    "emails": [],
    "sections": [
      {
        "heading": "1 INTRODUCTION",
        "text": "Dynamical systems described by nonlinear state-space models with a state vector x(t) \u2208 Rnx are powerful tool of many modern sciences and engineering disciplines to understand potentially complex dynamical systems. One can distinguish between Discrete-Time (DT) xn+1 = f(xn) and Continuous-Time (CT) dx(t)dt = f(x(t)) state-space models. In general, obtaining DT dynamical models from data is easier than CT models since data and computers are inherently discrete. However, the additional implementation complexity and computational costs associated with identifying CT models can be justified in many cases. First and foremost, from the natural sciences, we know that many systems are compactly described by CT dynamics which makes the continuity prior of CT models a well-motivated regularization/prior (De Brouwer et al., 2019). It has been observed that this regularization can be beneficial for sample efficiency (De Brouwer et al., 2019) which is a common observation when \u201cincluding physics\u201d in learning approaches (Karniadakis et al., 2021). Furthermore, the analysis of ODE equations is a well-regarded field of study with many powerful results and methods which could further improve model interpretability (Fan et al., 2021), such as applied in Bai et al. (2019). Another inherent advantage is that these models can be used with irregular sampled or missing data (Rudy et al., 2019). Lastly, in the control community, CT models are generally regarded desirable for control synthesis tasks as shaping the behavior of controller is much more intuitive in CT (Garcia et al., 1989). Hence, developing robust and general CT models and estimation methods would be greatly beneficial.\nIn the identification of physical CT systems it is common to encounter aspects with challenges such as: external inputs (u(t)), noisy measurements, latent states, unknown measurement function/distribution (e.g. y(t) = h(x(t))), the need for accurate long-term predictions and a need for a sufficiently low computational cost. For instance, all these aspects need to be considered for the cascade tank benchmark problem (Schoukens & Noe\u0308l, 2017). These aspects and the considered CT state-space model is summarized in Figure 1. Many of these aspects have been studied independently, for instance, Brajard et al. (2020); Rudy et al. (2019) explicitly addressed the presence\nof noise on the measurement data, Maulik et al. (2020); Chen et al. (2018) provided methods for modeling dynamics with latent states, Zhong et al. (2020) considers the presence of known external inputs, Zhou et al. (2021a) provides a computationally tractable method for accurate long-term sequence modeling. However, formulating models and estimation methods for the combination of multiple or all aspects is in comparison underdeveloped with only a few attempts (Forgione & Piga, 2021).\nIn contrast to previous work, we present the CT encoder method which is a general, robust and well-performing estimation method for CT state-space model identification. That is, the formulation addresses noise assumptions, external inputs, latent states, an unknown output function, and provides state-of-the-art results on multiple benchmarks of real systems. The presented subspace encoder method is summarized in Figure 2. The proposed method considers a cost function evaluating only short subsections of the available dataset which reduces the computational complexity. Furthermore, we show theoretically that considering subsections enhances cost function smoothness and thus optimization stability. The initial states of these subsections are estimated using the encoder function for which we present necessary requirements for its existence. Lastly, we introduce a normalization of the state and state-derivative and we show that it is required for proper CT estimation. Moreover, we attain additional novelty as these results are obtained without needing to impose a specific structure on the state-space (e.g. Greydanus et al. (2019); Cranmer et al. (2020)) obtaining a widely applicable method.\nOur main contributions are the following;\n\u2022 We formally derive the problem of CT state-space model estimation with latent states, external inputs, and measurement noise.\n\u2022 We reduce the computational loads by proposing the subspace encoder identification algorithm that employs short subsections, an encoder function that estimates the initial latent states of these subsections, and a state-derivative normalization term for robustness.\n\u2022 We make multiple theoretical derivations; (i) we prove that the use of short subsections increases cost function smoothness by Lipschitz continuity analysis, (ii) we derive necessary conditions for the encoder function to exist and (iii) we show that a state-derivative normalization term is required for proper CT models estimation.\n\u2022 We demonstrate that the proposed estimation method obtains state-of-the-art results on multiple benchmarks."
      },
      {
        "heading": "2 RELATED WORK",
        "text": "One of the most influential papers in CT model estimation is the neural ODE contribution (Chen et al., 2018) which showed that residual networks can be viewed as an Euler discretization of a continuous in-depth neural network. Moreover, they also show that one can employ numerical integrators to integrate through depth in a computationally efficient manner. This depth can be interpreted as the time direction to be able to model dynamical systems. However, the neural ODE does not\nscale well for long sequences, nor does it consider external inputs or noise, and the optimization process is often unstable. The ideas in the neural ODE contribution have been extended to/used in, for instance, normalizing flows to efficiently model arbitrary probability distributions (Papamakarios et al., 2021; Grathwohl et al., 2019), and enhance the understanding and interpretability of neural networks (Fan et al., 2021).\nAn adjacent research direction is the method/models which consider CT dynamics and directly use the state derivatives and often the noiseless states to formulate structured and interpretable models such as Hamiltonian Neural Networks (HNN) (Greydanus et al., 2019), Lagrangian Neural Networks (LNN) (Cranmer et al., 2020) and Sparse Identification of Nonlinear Dynamics (SINDy) (Brunton et al., 2016). In contrast, the proposed method is formulated for an unstructured state-space and does not require the system state or the state derivatives to be known.\nOur method is in part related to (Ayed et al., 2019) which concerns the estimation of CT models with latent variables. They also employ an encoder function to estimate initial states, however it is only dependent on the past outputs, contains a partially known state and do not provide theoretical insight into it. However, they only consider a fixed output function and solve their optimization problem as an optimal control problem whereas our formulation alters the simulation loss function to obtain the computationally desirable form. Furthermore, (Forgione & Piga, 2021), to which we compare in this work, considers a CT model with latent variables, subsections, and an additional loss term for the integration error. However, they include the initial states of these subsections as free optimization parameters. This increases the model complexity with the number of subsections. In contrast, our proposed method uses an encoder to estimate the initial states. This results in fixed model complexity. Furthermore, we only employ a single loss function and a novel state-derivative normalization term."
      },
      {
        "heading": "3 PROBLEM STATEMENT",
        "text": "Consider a system represented by a continuous-time nonlinear state-space (CT NL-SS) description sampled at a fixed interval \u2206t for simplicity:\nx\u0307s(t) = f(xs(t), u(t)),\nyk = h(xs,k, uk) + wk, (1)\nwhere the subscript notation denotes sampling as xs,k = xs(k\u2206t), xs(t) \u2208 Rnxs is the system state variable, u(t) \u2208 Rnu the input, yk \u2208 Rny the output, f represents the system dynamics and h the output function and wk \u2208 Rny is a zero-mean white noise with finite variance \u03a3w. For this system, the CT model estimation problem can be expressed for a given dataset of measurements: DN = {(u0, y0), (u1, y1), ..., (uN\u22121, yN\u22121)}, with unknown wk, xs(t), x\u0307s(t), y\u0307k and initial state xs(0), as the following optimization problem (a.k.a. simulation loss minimization):\nmin \u03b8,x(0)\n1\nN N\u22121\u2211 k=0 \u2225y(k\u2206t)\u2212 y\u0302(k\u2206t)\u222522,\ns.t. y\u0302(t) = h\u03b8(x(t)), x\u0307(t) = f\u03b8(x(t), u(t)),\n(2)\nwhere x(t) \u2208 Rnx is the model state, h\u03b8 and f\u03b8 are the output and state-derivative functions parameterized by \u03b8 and being Lipschitz continuous in their inputs and parameterization.\nTo obtain the simulation output y\u0302(k\u2206t) one can integrate x\u0307(t) = f\u03b8(x(t), u(t)) starting from the initial state x(0). This integration can be performed with any ODE solver that allows for backpropagation such as Euler (x(t+\u2206t) = x(t) +\u2206tf\u03b8(x(t), u(t))), RK4, or numerous adaptive step methods (Chen et al., 2018; Ribeiro et al., 2020).1 To make this a well-posed optimization problem, additional information or an assumption on the inter-sample behavior of u(t) is required, since,\n1The adjoint methods for gradient computation is not within the scope of this research.\nfor example, u(\u2206t/2) is not present in DN . This behavior is often chosen to be Zero-Order Hold (ZOH) (Forgione & Piga, 2021) as seen an example of in Figure 1.\nMultiple major issues are encountered when solving the optimization Problem (2) with a gradientdescent-based method. A first issue is that computing the value of the loss function requires a forward pass on the whole length of the dataset (Ayed et al., 2019). Hence, the computational complexity grows linearly with the length of the dataset. Furthermore, a common occurrence is that the values of x(t) or its gradient grows exponentially which results in non-smooth loss functions or gradients. This causes gradient-based optimization algorithms to become unreliable since the optimization process might be unstable or converge to a local minima (Ribeiro et al., 2020). All these issues are addressed in the proposed method."
      },
      {
        "heading": "4 PROPOSED METHOD",
        "text": "We propose to consider multiple overlapping short subsections of length T\u2206t to form a truncated simulation loss instead of simulating over the entire length of the dataset. We express this in the following optimization problem (note that we express the optimization problem with discrete-time notation (uk := u(k\u2206t)) for brevity):\nmin \u03b8\n1\nN \u2212 T \u2212max(na, nb) + 1 N\u2212T\u2211 n=max(na,nb) 1 T T\u22121\u2211 k=0 \u2225yn+k \u2212 y\u0302n+k|n\u222522,\ns.t. y\u0302n+k|n = h\u03b8(xn+k|n),\nxn+k+1|n = ODEsolve[ 1\n\u03c4 f\u03b8, xn+k|n, un+k,\u2206t],\nxn|n = \u03c8\u03b8(un\u22121, ..., un\u2212nb , yn\u22121, ..., yn\u2212na).\n(3)\nHere, the pipe (|) notation indicates the current index and the starting index as (current index | start index) to differentiate between different subsections. Furthermore, ODEsolve indicates a numerical scheme which integrates 1/\u03c4 f\u03b8(x, u) from the initial state xn+k|n for a length of \u2206t given the input un+k. Lastly, we introduced an encoder function \u03c8\u03b8 with encoder lengths na and nb, for the past output and input samples respectively, which estimates the initial states of the considered subsection. A graphical summary of the proposed method called the CT subspace encoder (abbreviated as SUBNET) can be viewed in Figure 2.\nThe first observation is that optimization Problem (3) is a generalisation of (2) since if T = N and na = nb = 0, the original optimization Problem (2) is recovered. However, as one might observe, this optimization problem is less computationally challenging to solve if T < N since the first sum can be computed in parallel. Moreover, the smoothness of the encoder cost function is also enhanced since the associated Lipschitz constant LV (enc) can scale exponentially with the subsection length (T\u2206t) as shown in Theorem 1. The enhanced smoothness is reflected in the ease of optimization (Ribeiro et al., 2020).\nTheorem 1. The Lipschitz constant LV (enc) of the cost function (3)\n\u2225V (enc)(\u03b81)\u2212 V (enc)(\u03b82)\u22252 \u2264 LV (enc)\u2225\u03b81 \u2212 \u03b82\u22252 (4)\nscales as\nLV (enc) = O(exp(2T\u2206tLf/\u03c4)) (5)\nwhere Lf is the Lipschitz constant of f\u03b8.\nProof. See Appendix 8.1\nAn error in the initial state of xn|n can significantly bias the estimate due to the short nature of the subsections. To counter this, we formulated an encoder function \u03c8\u03b8 which estimates the initial state of each subsection. We do not add any additional loss term since an improved initial state error estimate also minimizes the transient error (Forgione & Piga, 2021) which is present in the encoder loss.\nA natural question to ask is under which conditions there exists an encoder function that can map from the past inputs and outputs to this initial state. In Appendix 8.2, we formally derive necessary conditions for the existence. These necessary conditions are that, state derivative f\u03b8 requires to be Lipschitz continuous in x, and if the number of considered past outputs na and inputs nb are equal then na \u2265 nx/ny needs to be satisfied, among other conditions. It is widely known that input and output normalization is essential for obtaining competitive models throughout deep learning. Input and output normalization is insufficient when considering CT statespace model due to the presents of the hidden state x(t) and the state-derivative f(x, u). However, as shown in Theorem 2, any CT system can be transformed to become normalized by the introduction of a state transform and a positive 1/\u03c4 normalization factor. Theorem 2. For any system of the form x\u0307(t) = f(x(t), u(t)) with finite x(t) and x\u0307(t) there exists a \u03c4 and a state transformation \u03b3x\u0303(t) = x(t) such that both the state x\u0303 and state-derivative f\u0303 of the transformed system \u02d9\u0303x = 1\u03c4 f\u0303(x\u0303(t), u(t)) are normalized as in\nRMS(x\u0303) =\n\u221a 1\nL \u222b L 0 1 nx \u2225x\u0303(t)\u222522dt = 1 & RMS(f\u0303) = 1. (6)\nProof. With\n\u03b3 = RMS(x) & 1\n\u03c4 = RMS(x\u0307) RMS(x)\n(7)\nthe normalization conditions are satisfied, as shown below\nRMS(x\u0303) = RMS(x)/\u03b3 = 1 (8a)\nRMS(f\u0303) = \u03c4RMS( \u02d9\u0303x) = \u03c4RMS(x\u0307)/\u03b3 = \u03c4RMS(x\u0307)/RMS(x) = 1 (8b)\nHence, considering only state normalization \u03c4 = 1, one cannot obtain models where both the statederivative f\u0303 function and state x\u0303 are normalized for most systems. Thus, including state-derivative normalization is essential for proper normalization in CT model estimation. Furthermore, this proof also guides the choice of \u03c4 since the amplitude of RMS(x)/RMS(x\u0307) might be known from physical insight or by an approximate model."
      },
      {
        "heading": "5 EXPERIMENTS",
        "text": ""
      },
      {
        "heading": "5.1 BENCHMARK DESCRIPTIONS",
        "text": "The Cascade Tank with overflow (CCT) benchmark (Schoukens & Noe\u0308l, 2017; Schoukens et al., 2017) consists of measurements taken from a two-tank fluid system with a pump. The input signal controls a water pump that delivers water from the reservoir to the upper tank. Through a small\nopening in the upper tank, the water enters the lower tank where the water level is recorded. Lastly, through a small opening in the lower tank, the water re-enters the reservoir. This benchmark is nonlinear as the flow rates are governed by square root relations and the water can overflow either tank which is a hard nonlinearity. The benchmark consists of two datasets with measurements of 1024 samples each at a sample rate of \u2206t = 4s. The first dataset is be used for training, the first 512 samples of the second set are used for validation (used only for early stopping) and the entire second set for testing. Most of the other methods to which we compare use the entire second set as validation and test set, as no explicit test set is provided in this benchmark description.\nThe Coupled Electric Drive (CED) benchmark (Wigren & Schoukens, 2017) consists of measurements from a belt and pulley with two motors where both clockwise and counter-clockwise movement is permitted. The motors are actuated by the given inputs and the measured output is a pulse transducer that only measures the absolute velocity (i.e. insensitive to the sign of the velocity) of the belt. The system approximately has three states; the velocity of the belt, the position of the pulley, and the velocity of the pulley. The benchmark consists of two datasets of 500 samples each at a sample rate of \u2206t = 20ms. The first 300 samples are used for training and the other 200 samples are for testing, of those samples the first 100 samples are also used for validation with both datasets. Similar to the last benchmark, even with this overlap, it is still a fair comparison as most of the other methods to which we compare use the entire second set as validation and test."
      },
      {
        "heading": "5.2 RESULTS",
        "text": "Using the SUBNET method Eq. (3) we estimate models where the three functions h\u03b8, f\u03b8 and \u03c8\u03b8 are implemented as 2 hidden layer neural networks with 64 hidden nodes per layer, tanh activation and a linear bypass from the input to the output for both benchmarks. As an ODE solver, we use a single RK4 step between samples and assume that the input signal is zero-order hold. As for the implementation of the CT subspace encoder method, the following hyper-parameters are considered; nx = 2, na = nb = 5 and T = 30 for CCT and nx = 3, na = nb = 4 and T = 60 for CED. These hyperparameters are chosen based on few-step prediction-error figures as was shown in (Beintema et al., 2021). The training is done by using the Adam optimizer with default settings (Kingma & Ba, 2015) with a batch size of 32 for CED and 64 for CCT and using a simulation on the validation dataset for early stopping to reduce overfitting.\nWe also directly compare with a reproduction of neural ODE on both benchmarks. We adapt the code and the example (\u201clatent ODE.py\u201d) available online (Chen et al., 2018) to include ZOH inputs, leaving the neural network unaltered and an RK4 integrator. We observed that the initial model was unstable for CCT and under performing for CED and, hence, the neural ODE method alone was unable to provide state-of-the-art results. To stabilize and improve the neural ODE method we also introduce a state-derivative normalization term 1/\u03c4 motivated by Theorem 2. The value of\n1/\u03c4 for CCT and CED for neural ODE was initially chosen to be the optimal value found in the SUBNET approach, however, in the CED case, it was lowered due to optimization instabilities. 2\nWe compared our obtained model to the literature in Table 1. We report both the mean and the minimum of an ensemble of models estimated only differing in parameter initialization. This ensemble consists of 17 SUBNET models for both CCT and CED and 24 and 8 neuralODE models for CCT and CED respectively. The table shows that the obtained models with the CT subspace encoder method are state-of-the-art. The obtained models are the best known models with a black-box approach on both benchmarks. Furthermore, we use an unrestricted state-space and fully connected neural networks as model elements. Remarkably, the resulting performance is close to the performance of a grey-box model. Furthermore, Figure 4 illustrates that the resulting models have been able to model the nonlinear behavior present in both benchmarks.\nTable 1 also contains neural ODE with normalization. One observation is that the best and mean performance difference is significantly larger than of SUBNET. We think that this is due to the availability of only a single sequence in the training set for the CCT benchmark (and two sequences for CED) which results in extensive overfitting. In comparison, the subspace encoder method is less prone to overfitting since it uses many subsections of the available sequence(s). Moreover, the subspace encoder method only requires about 20 minutes to train a model to the lowest validation loss, whereas, neural ODE requires about 2 hours for CED and 5 hours for CCT.\nWhen we introduced the state-derivative normalization factor 1/\u03c4 , we argued that it would normalize f\u03b8 and that it would increase optimization stability and, hence, the quality of the obtained models. We provide experimental insight for these two statements by providing a parameter sweep over \u2206t/\u03c4 . Moreover, to eliminate variations due to different initial parameters we trained an ensemble of models which creates box-plots with RMS(x) = \u221a 1/Nnx \u2211 k \u2225x(k\u2206t)\u222522, RMS(f), and the RMSE simulation. These box-plots as shown in Figure 5 indeed illustrate that there exists an 1/\u03c4 such that both RMS(f) \u2248 RMS(x) \u2248 1 and that the best performing models are close to that value of 1/\u03c4 . Moreover, to illustrate that improper normalization (i.e. \u03c4 = 1) can diminish the performance for both CCT and CED, observe that the RMSE simulation is 2.0 and 0.3 ticks/s for \u2206t/\u03c4 = \u2206t = 4 s and \u2206t/\u03c4 = \u2206t = 0.02 s respectively.\n2The code used for both SUBNET and neural ODE experiments is available at \u201cGithub link removed for double-blind reviews, see zip file from supplementary materials until de-anonymization\u201d"
      },
      {
        "heading": "6 CONCLUSION",
        "text": "In this paper, we introduced the CT subspace encoder approach to identify nonlinear dynamical systems in the presents of latent states, external inputs, and measurement noise. We showed that the proposed method can obtain state-of-the-art CT models only consisting of fully connected neural\nnetworks. The approach improved computational cost and stability by considering multiple subsections where the initial state is estimated with an encoder function to reduce bias and a state-derivative normalization term to improve optimization stability. On this structure we make multiple theoretical derivations; increased cost function smoothness, necessary conditions for the existence of the encoder function and that for proper normalization in CT modelling one requires the state-derivative normalization term. Furthermore, we obtain state-of-the-art results on two benchmark examples."
      },
      {
        "heading": "7 REPRODUCIBILITY STATEMENT",
        "text": "\u2022 Datasets: (i) The CCT dataset is described in Schoukens & Noe\u0308l (2017); Schoukens et al. (2017) and is available for download at https://data.4tu. nl/articles/dataset/Cascaded_Tanks_Benchmark_Combining_ Soft_and_Hard_Nonlinearities/12960104, (ii) The CED dataset is described in Wigren & Schoukens (2017) and is available for download at http://www.it.uu.se/research/publications/reports/2017-024/.\n\u2022 Code: Both the implementation and experiments of CT SUBNET and neural ODE are available at \u201cGithub link removed for double-blind reviews, see zip file from supplementary materials until de-anonymization\u201d.\n\u2022 Hardware: It takes about 15 minutes to estimate a single CT SUBNET model and 2 hours for a single neural ODE model on a consumer laptop."
      },
      {
        "heading": "8 APPENDIX",
        "text": ""
      },
      {
        "heading": "8.1 PROOF OF THEOREM 1",
        "text": "Recall that the subspace encoder loss function as in Eq. 3 can be expressed in the following form\nV enc(\u03b8) = 1\nN \u2212 T \u2212max(na, nb) + 1 N\u2212T\u2211 n=max(na,nb) 1 T T\u22121\u2211 k=0 \u2225yn+k \u2212 y\u0302n+k|n\u222522,\nwith y\u0302n+k|n = h\u03b8(xn+k|n) = h\u03b8(x((n+ k)\u2206t|n\u2206t)),\nx\u0307(t|n\u2206t) = 1 \u03c4 f\u03b8(x(t|n\u2206t), u(t)) x(n\u2206t|n\u2206t) = \u03c8\u03b8(un\u22121, ..., un\u2212nb , yn\u22121, ..., yn\u2212na).\n(9)\nOur aim is to derive the scaling in T of the Lipschitz constant Lenc as defined in\n|V enc(\u03b81)\u2212 V enc(\u03b82)|2 \u2264 L2enc\u2225\u03b81 \u2212 \u03b82\u222522. (10) We aim to express Lenc in terms of the following Lipschitz constants\n\u2225h\u03b81(x1)\u2212 h\u03b82(x2)\u222522 \u2264 L2h(\u2225x1 \u2212 x2\u222522 + \u2225\u03b81 \u2212 \u03b82\u222522). (11a) \u2225f\u03b81(x1, u)\u2212 f\u03b82(x2, u)\u222522 \u2264 L2f (\u2225x1 \u2212 x2\u222522 + \u2225\u03b81 \u2212 \u03b82\u222522), (11b)\n\u2225\u03c8\u03b81(upast, ypast)\u2212 \u03c8\u03b82(upast, ypast)\u222522 \u2264 L2\u03c8\u2225\u03b81 \u2212 \u03b82\u222522. (11c)\nFurthermore, to derive the scaling of Lenc we use known properties of the Lipschitz constant that being;\n\u2022 The sum property: c(x) = a(x) + b(x) has a Lipschitz constant of Lc = La + Lb. \u2022 The multiplication property: c(x) = a(x)b(x) has a Lipschitz Lc =MaLb+MbLa where Ma is the maximal value of a on a closed set of inputs x \u2208 X and Mb similarly defined.\nSince the encoder loss function as in Eq. 3 can be written as a sum as\nV enc(\u03b8) = 1\nN \u2212 T \u2212max(na, nb) + 1 N\u2212T\u2211 n=max(na,nb) V sec(n, \u03b81) (12a)\nV sec(n, \u03b81) = 1\nT T\u22121\u2211 k=0 \u2225yn+k \u2212 y\u0302n+k|n\u222522 (12b)\n|V sec(n, \u03b81)\u2212 V sec(n, \u03b82)|2 \u2264 L2sec\u2225\u03b81 \u2212 \u03b82\u222522, (12c) this implies that Lenc = Lsec by the sum property since\nLenc = 1\nN \u2212 T \u2212max(na, nb) + 1 N\u2212T\u2211 n=max(na,nb) Lsec. (13)\nHence, it is sufficient to consider only a single subsection, take n = 0 and drop the bar notation for simplicity of notation.\nBy the sum and multiplication properties we derive that\n|V sec(\u03b81)\u2212 V sec(\u03b82)| \u2264 2/T T\u22121\u2211 k=0 (My +Mk)\u2225y\u03021,k \u2212 y\u03022,k\u22252, (14)\nwhere My is the bound on \u2225y(t)\u22252 and Mk the bound on \u2225y\u0302k\u22252. The Mk bound scales the same as \u2225y\u03021,k \u2212 y\u03022,k\u22252 as shown in Ribeiro et al. (2020). The \u2225y\u03021,k \u2212 y\u03022,k\u22252 expression can be expanded by using Eq. 11a as\n\u2225y\u03021,k \u2212 y\u03022,k\u222522 \u2264 L2h(\u2225x1(k\u2206t)\u2212 x2(k\u2206t)\u222522 + \u2225\u03b81 \u2212 \u03b82\u222522). (15)\nNext, we aim to derive an expression for the Lipschitz constant Lx(t) given in terms of\n\u2225x1(t)\u2212 x2(t)\u222522 \u2264 Lx(t)2\u2225\u03b81 \u2212 \u03b82\u222522. (16)\nwhere by Eq. (11c) Lx(0) = L\u03c8. (17)\nBy considering a small increment in time of length h we can express Lx(t + h) in terms of Lx(t). Using the fact that h is small we can use an Eurler step and discard higher order terms of h as;\n\u2225x1(t+ h)\u2212 x2(t+ h)\u222522 = \u2225(x1(t)\u2212 x2(t)) + h/\u03c4(f\u03b81(x1(t), u(t))\u2212 f\u03b82(x2(t), u(t)))\u222522 \u2264 \u2225x1(t)\u2212 x2(t)\u222522 + 2h/\u03c4\u2225x1(t)\u2212 x2(t)\u22252\u2225f\u03b81(x1(t), u(t))\u2212 f\u03b82(x2(t), u(t))\u22252\nby the triangle inequality. Next, we can replace all the f terms by Eq. (11b) and x terms by Eq. (16) to derive an expression for Lx(t+ h) as\n\u2264 \u2225x1(t)\u2212 x2(t)\u222522 + 2h/\u03c4\u2225x1(t)\u2212 x2(t)\u22252Lf \u221a \u2225x1(t)\u2212 x2(t)\u222522 + \u2225\u03b81 \u2212 \u03b82\u222522\n\u2264 ( Lx(t) 2 + 2h/\u03c4Lx(t)Lf \u221a Lx(t)2 + 1 ) \u2225\u03b81(t)\u2212 \u03b82(t)\u222522\nLx(t+ h) = \u221a Lx(t)2 + 2hLx(t) \u221a Lx(t)2 + 1Lf/\u03c4 .\nThis expression allows us to derive that the derivative of Lx(t) is given by\nL\u0307x(t) = lim h\u21920 Lx(t+ h)\u2212 Lx(t) h , (18)\n= \u221a 1 + Lx(t)2Lf/\u03c4. (19)\nwhich suggests that L\u0307x(t) is continuous in t and has the closed form solution as\nLx(t) = Lx(0) + \u222b t 0 L\u0307x(t \u2032)dt\u2032 (20)\n= sinh(tLf/\u03c4 + arcsinh(L\u03c8)) (21)\nNow by substituting Eq. (21) into, (16), (15), (14) and using (13) we arrive at the following expression for Lenc as\nLenc = 2/T \u2211 k (My +Mk)Lh(sinh(k\u2206tLf/\u03c4 + arcsinh(L\u03c8)) + 1) (22a)\nwhich scales in the limit of large T as\nLenc = O(exp(2T\u2206tLf/\u03c4)), (23a)\nsince Lf > 0 and \u2206t > 0. Note that the 2 in the exponent is from the multiplication with Mk which also scales as the y term as previously mentioned."
      },
      {
        "heading": "8.2 RECONSTRUBABILITY OF THE INITIAL STATE FROM PAST INPUT AND OUTPUTS",
        "text": "To derive the conditions on the existence of the encoder function suppose that we have a system given by\nx\u0307(t) = f(x(t), u(t)) (24a) yn = h(xn) + wn (24b)\nwhere xn = x(n\u2206t). For this system if a state is given x(t0) along the input trajectory u(t) one would in principle be able to compute x(t) for all t > t0. However, since we aim to construct the state given past outputs we need x(t) for t < t0 which requires backwards in time integration. This backwards integration on f is guaranteed to be unique if f is Lipschitz continuous in x for all u as by the Picard\u2013Lindelo\u0308f theorem (Murray & Miller, 2013). Hence, since f is assumed to be Lipschitz continuous we can construct an operator fd which can integrate backwards or forwards as\nxn+1 = fd(xn, un) \u2192 xn\u22121 = f\u22121d (xn, un\u22121) (25)\nwhere u is subject to ZOH.\nThis operator allows us to construct past outputs as\nyn\u22121 = (h \u25e6 f\u22121d )(xn, un\u22121) + wn\u22121 yn\u22122 = (h \u25e6 f\u22122d )(xn, un\u22121, un\u22122) + wn\u22122\n...\nyn\u2212z = (h \u25e6 f\u2212zd )(xn, un\u22121, un\u22122, ..., un\u2212z) + wn\u2212z Y \u2212zn = (H \u25e6 F\u2212zd )(xn, U \u2212z n ) +W \u2212z n\n(26)\nwhere f\u2212pd (xn, un\u22121, ..., un\u2212p) = f \u2212p+1 d (f \u22121 d (xn, un\u22121), un\u22122, ..., un\u2212p) is the application of f\u22121d , p times to obtain xn\u2212p. Furthermore, Y \u2212z n = [y \u22a4 n\u22121, y \u22a4 n\u22122, ..., y \u22a4 n\u2212z] \u22a4 and, (H \u25e6 F\u2212zd ) and W\u2212zn are similarly defined. To construct the initial state xn we need to invert Eq. (26). This inverse is also known as a reconstructability map (Katayama, 2005). However, for the inverse to exist, several necessary requirements can be derived. One such necessary requirement is that a small perturbation to a solution xn should change (H \u25e6F\u2212zd ), otherwise these solutions are indistinguishable from the output. This is formalized stating by that the matrix \u2202(H\u25e6F \u2212z d )(xn,U \u2212z n ) \u2202xn has a null-space of rank zero which is also known as the local observability condition. This is the same as the condition that the column rank \u2265 nx. A necessary requirement for this column rank condition is that the number of columns is equal to or greater than the number of rows i.e. zny \u2265 nx. Hence, under the right conditions, it might be possible to solve Eq. (26) for a singular xn since this equation is a nonlinear fixed point problem if W\u2212zn is known. For the case that W \u2212z n is unknown one can estimate the state x\u0302n \u2248 xn by solving the nonlinear regression problem;\nx\u0302n = argmin x\u0302n\n\u2225Y \u2212zn \u2212 (H \u25e6 F\u2212zd )(x\u0302n, U \u2212z n )\u22252, (27a)\n= argmin x\u0302n\n\u2225L(x\u0302n, Y \u2212zn , U\u2212zn )\u22252. (27b)\nHence, both f uniformly Lipschitz continuous and (\u2207xL)T\u2207xL being full rank in x\u0302n are necessary conditions for the existence of a unique reconstrubability map.\nComputing the reconstructability map for our model thus requires solving an optimization problem that becomes computationally infeasible during training. Hence, the encoder function aims to approximate the solution to Problem (27)."
      }
    ],
    "year": 2022,
    "abstractText": "Continuous-time (CT) modeling has proven to provide improved sample efficiency and interpretability in learning the dynamical behavior of physical systems compared to discrete-time (DT) models. However, even with numerous recent developments, the CT nonlinear state-space (NL-SS) model identification problem remains to be solved in full, considering common experimental aspects such as the presence of external inputs, measurement noise, latent states, and general robustness. This paper presents a novel estimation method that addresses all these aspects and that can obtain state-of-the-art results on multiple benchmarks with compact fully connected neural networks capturing the CT dynamics. The proposed estimation method called the subspace encoder approach (SUBNET) ascertains these results by efficiently approximating the complete simulation loss by evaluating short simulations on subsections of the data, by using an encoder function to estimate the initial state for each subsection and a novel state-derivative normalization to ensure stability and good numerical conditioning of the training process. We prove that the use of subsections increases cost function smoothness together with the necessary requirements for the existence of the encoder function and we show that the proposed state-derivative normalization is essential for reliable estimation of CT NL-SS models.",
    "creator": "LaTeX with hyperref"
  },
  "output": [
    [
      "1. \"The main claims of the paper are, in my opinion, not sufficiently explored in the results section.\"",
      "2. \"Specifically, a proper evaluation of the advantage in terms of computational costs compared to the other methods in Table 1 is missing.\"",
      "3. \"This is particularly important for other methods, like Forgione & Piga 2021 that also employ subsections.\"",
      "4. \"Furthermore, the authors cite Ayed et al 2019 as a similar work that also employ an encoder function, but do not compare it to their method.\"",
      "5. \"Along the same lines, the authors do not investigate the choice of hyperparameters such as na, nb or the length of a subsection Tdt.\"",
      "6. \"Exploring these hyperparameters and their impact of performance vs computing costs would greatly enhance the impact of the paper.\"",
      "7. \"Finally, the second benchmark only contains 500 samples in total, which seems modest if contrasted with the claims made by the authors about 'accurate long-term predictions'.\"",
      "8. \"Perhaps a different benchmark \u2013 e.g. the EMPS dataset employed in Forgione & Piga 2021 \u2013 could be used to highlight the potential of the method, specifically the use of subsections.\"",
      "9. \"There\u2019s one last aspect that I think is not particularly explored in the paper despite the initial claims: the detection of latent states.\"",
      "10. \"The benchmark used in the paper are not particularly well suited to evaluate the ability of the model to infer the presence of latent states.\"",
      "11. \"Perhaps a better benchmark could be used since this is one of the main claims of the paper.\""
    ],
    [
      "1. \"there are multiple areas the presentation of these results should be clarified and improved.\"",
      "2. \"First, it is not clear how surprising theorem 1 is: given that the derivative has a bounded Lipschitz constant is it surprising that when we integrate over shorter sequences, the Lipschitz constant of the integral, also decreases?  To this point, it would be helpful to know how tight the bound is.  If I understand the notation correctly to be big-O, this is an upper bound.  If the tightest upper bound possible is indeed exponential with T, then this result is indeed interesting, but we would need assurance (or at least a reason to conjecture) that this is indeed the case.\"",
      "3. \"Second, while formal theoretical results are not needed, it would be helpful for the authors to discuss what, if anything, might be lost by breaking up longer observation sequences into shorter chunks.  For example, does this hinder the ability to learn dynamical systems with longer time scales?  Perhaps not if the network inferring initial state is accurate enough, but having an explicit discussion about this would help.\"",
      "4. \"Third, the paragraph following theorem 2 appeals to it in a way that the theorem itself is not currently worded to support.  In particular, theorem 2 is currently worded as an existence proof: it is possible to find two constants such that both the state and derivative are normalized.  However, the paragraph immediately after this theorem (beginning \"Hence, considering only...\") appeals to this theorem as if it were a proof of necessity.  Following, the authors math shown in the proof of the theorem, I believe such a claim could be made, but the theorem would need to be re-worded (or the logic of the following paragraph made more clear).\"",
      "5. \"I am a bit confused about how theorem 2 applies to model fitting.  In particular, I agree given a ground truth x(t) and f(t), two constants can be found such that the transformed descriptions are regularized as the authors claim.  However, when fitting models, there is flexibility in the f we chose, so that it would seem for any f and any value of 1/\\tau in equation (3), can't we always find a f' = \\tau f.  In other words, don't we have to restrict the flexibility of the function class we consider for f in some way for \\tau to have any affect?\"",
      "6. \"Finally, the authors provide empirical results comparing their method to others in two benchmark tests.  They claim to achieve state of the art results in both.  However, as the authors commendably make clear, part of their test sets are also used for validation.  They state this is fair because many of the existing methods also do the same.  However, to fully evaluate the claim that a new state of the art has been achieved, it would be helpful for the authors to show their existing results as well as results obtained when validation and test sets are fully distinct.   When comparing to previous methods which did in fact have overlapping test and validation sets, they could use the current set of results, but when comparing to methods that had disjoint test and validation sets, this second set of results could then be used to ensure a fair comparison. If their method still outperforms all existing results when comparing each on this more equal footing,  I believe they could then rigorously claim a new state of the art has been achieved.\""
    ],
    [
      "1. \"Since the framework uses three neural networks as function approximators for encoder, iterative dynamics, and output projection, the full model and optimization scheme look very similar to training a standard recurrent neural network.\"",
      "2. \"From this comparison, it is less clear about the benefits of having such elaborate framework in terms of increasing performance and reducing computational complexity.\""
    ],
    [
      "1. The structure of the SUBNET is unclear from Figure 2 and the corresponding text.",
      "2. As SUBNET is the key contribution of this work, more details are welcomed.",
      "3. The reviewer is also curious about how the discrete-time subspace encoder, as listed in Table 1, is implemented."
    ],
    [
      "1. The results in table 1 seems to suggest that for both problems the neural ODE approach with normalization seems to be performing as well or even better than the proposed technique. Do the authors suggest that the main advantage of their approach lies in the lower estimation time (lower computation complexity).",
      "2. I fail to understand how the proposed approach works, especially the joint training of 3 fully-connected neural networks.",
      "3. Figure 5 caption could be expanded for better understanding."
    ]
  ],
  "review_num": 5,
  "item_num": [
    11,
    6,
    2,
    3,
    3
  ]
}