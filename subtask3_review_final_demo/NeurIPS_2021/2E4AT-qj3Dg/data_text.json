{
  "ID": "2E4AT-qj3Dg",
  "Title": "Discovering Dynamic Salient Regions for Spatio-Temporal Graph Neural Networks",
  "URL": "https://openreview.net/forum?id=2E4AT-qj3Dg",
  "paper_draft_url": "/references/pdf?id=1fYgXZzsLg",
  "Conferece": "NeurIPS_2021",
  "input": {
    "source": "CRF",
    "title": "Discovering Dynamic Salient Regions for Spatio-Temporal Graph Neural Networks",
    "authors": [],
    "emails": [],
    "sections": [
      {
        "heading": null,
        "text": "Graph Neural Networks are perfectly suited to capture latent interactions between1 various entities in the spatio-temporal domain (e.g. videos). However, when an2 explicit structure is not available, it is not obvious what atomic elements should3 be represented as nodes. Current works generally use pre-trained object detectors4 or fixed, predefined regions to extract graph nodes. Improving upon this, our5 proposed model learns nodes that dynamically attach to well-delimited salient6 regions, which are relevant for a higher-level task, without using any object-level7 supervision. Constructing these localized, adaptive nodes gives our model inductive8 bias towards object-centric representations and we show that it discovers regions9 that are well correlated with objects in the video. In extensive ablation studies10 and experiments on two challenging datasets, we show superior performance to11 previous graph neural networks models for video classification.12\n1 Introduction13\nSpatio-temporal data, and videos, in particular, are characterised by an abundance of events that14 require complex reasoning to be understood. In such data, entities or classes exist at multiple scales15 and in different contexts in space and time, starting from lower-level physical objects, which are well16 localized in space and moving towards higher-level concepts which define complex interactions. We17 need a representation that captures such spatio-temporal interactions at different level of granularity,18 depending on the current scene and the requirements of the task. Classical convolutional nets address19 spatio-temporal processing in a simple and rigid manner, determined only by fixed local receptive20 fields [1]. Alternatively, space-time graph neural nets [2, 3] offer a more powerful and flexible21 approach modeling complex short and long-range interactions between visual entities.22\nIn this paper, we propose a novel method to enhance vision Graph Neural Networks (GNNs) by23 an additional capability, missing from any other previous works. That is, to have nodes that are24 constructed for spatial reasoning and can adapt to the current input. Prior works are limited to25 having either nodes attached to semantic attention maps [4] or attached to fixed locations such as26 grids [5, 3, 6]. Moreover, unlike works that require external object detectors [7] our method relies on27 a learnable mechanism to adapt to the current input.28\nWe propose a method that learns to discover salient regions, well-delimited in space and time, that are29 useful for modeling interactions between various entities. Such entities could be single objects, parts30 or groups of objects that perform together a simple action. Each node learns to associate by itself to31 such salient regions, thus the message passing between nodes is able to model object interactions more32 effectively. For humans, representing objects is a core knowledge system [8] and to emphasize them33 in our model, we predict salient regions [9] that give a strong inductive bias towards modeling them.34\nSubmitted to 35th Conference on Neural Information Processing Systems (NeurIPS 2021). Do not distribute.\nOur method, Dynamic Salient Regions Graph Neural Network (DyReg-GNN) improves the relational35 processing of videos by learning to discover salient regions that are relevant for the current scene36 and task. Note that the model learns to predict regions only from the weak supervision given by37 the high-level video classification loss, without supervision at the region level. Our experiments38 convincingly show that the regions discovered are well correlated with the objects present in the39 video, confirming the intuition that action recognition should be strongly related to salient region40 discovery. The capacity to discover such regions makes DyReg-GNN an excellent candidate model41 for tackling tasks requiring spatio-temporal reasoning.42\nOur main contributions are summarised as follow:43\n1. We propose a novel method to augment spatio-temporal GNNs by an additional capability:44 that of learning to create localized nodes suited for spatial reasoning, that adapt to the input.45\n2. The salient regions discovery enhance the relational processing for high-level video clas-46 sification tasks: creating GNN nodes from predicted regions obtains superior performance47 compared to both using pre-trained object detectors or fixed regions48\n3. Our model leads to unsupervised salient regions discovery, a novelty in the realm of49 GNNs: it predicts such regions in videos, with only weak supervision at the video class level.50 We show that regions discovered are well correlated with actual physical object instances.51\n2 Related work52\nGraph Neural Networks in Vision. GNNs have been recently used in many domains where the53 data has a non-uniform structure [10, 11, 12, 13]. In vision tasks, it is important to model the54 relations between different entities appearing in the scene [14, 15] and GNNs have strong inductive55 biases towards relations [16, 17], thus they are perfectly suited for modeling interactions between56 visual instances. Since an explicit structure is not available in the video, it is of critical importance57 to establish what atomic elements should be represented as graph nodes. As our main contribution58 revolves around the creation of nodes, we analyse other recent GNN methods regarding the type of59 information that each node represents, and group them into two categories, semantic and spatial.60\nThe approaches of [4, 18, 19, 20, 21, 22] capture the purely semantic interactions by reasoning over61 global graph nodes, each one receiving information from all the points in the input, regardless of62 spatio-temporal position. In [4] the nodes assignments are predicted from the input, while in [18] the63 associations between input and nodes are made by a soft clusterization. The work of [22] discovers64 different representation groups by using an iterative clusterization based on self-attention similarity.65\nThe downside of these semantic approaches is that individual instances, especially those belonging66 to the same category, are not distinguished in the graph processing. This information is essential in67 tasks such as capturing human-object interactions, instance segmentation or tracking.68\nAlternatively, multiple methods, including ours, favour modeling instance interactions by defining69 spatial nodes associated with certain locations. We distinguish between them by how they extract70 the nodes from spatial location: as fixed regions or points [23, 24], or detected object boxes [25,71 26, 27, 28]. The method [5] creates nodes from every point in 2D convolutional features maps,72 while Non-Local [29] uses self-attention [30] between all spatio-temporal positions to capture distant73 interactions. Further, [3] extract nodes from larger fixed regions at different scales and processes74 them recurrently. Recent methods based on Transformer [31, 6, 32] also model the interactions75 between fixed locations on a grid using self-attention. In [7], nodes are created from object boxes76 extracted by an external detector and are processed using two different graph structures, one given by77 location and one given by nodes similarity. Hybrid approaches use nodes corresponding to points and78 object features [33, 34] or propagate over both semantic and spatial nodes [35, 36, 37].79\nHowever, methods that rely on external modules trained on additional data, such as object detectors,80 are too dependent on the module\u2019s performance. They are unable to adapt to the current problem,81 being limited to the set of pre-defined annotations designed for another task. Differently, our module82 is optimized to discover regions useful for the current task, using only the video classification signal.83\nConcurrently, the method [38] uses multiple position-aware nodes that take into account the spatial84 structure. This makes it more suitable for capturing instances, but the nodes have associated a static85 learned location, where each one is biased towards a specific position regardless of the input. On the86\nother hand, we dynamically assign a location for each node, based on the input, making the method87 more flexible to adapt to new scenes.88\nDynamic Networks. Several works use second-order computations by dynamically predicting89 different parts of their model from the input, instead of directly optimising parameters. Our work90 is related to STN [39] that aggregates features by interpolating from an area given by a predicted91 transformation and to the differentiable pooling used in some object detectors [40, 41, 42]. The92 method [43], replaces the parameters in a standard convolution with weights predicted from the93 input, resulting in a dynamically generated filter. Deformable convolutions [44, 45] predict, based94 on the input, an offset for each position in the convolutional kernel. Similar, [46] use the same idea95 of predicting offsets but in a graph formulation. The common topic of these methods is to predict96 dynamically a support for all points in a convolutional operation while we dynamically generate97 the input for a set of nodes designed to process high-level interactions. Related ideas, involving98 high-level processing of a small set of powerful modules, is also highlighted in [47] and [38].99\nUnsupervised Object Representations. There is an entire area of work devoted to extracting100 representations centered on objects [48] in a fully unsupervised setting [49, 50, 51, 22]. They are101 successful in leveraging a reconstruction task to decompose the scene into objects, for synthetic102 images. In [52] it is shown that representations learned from unsupervised decomposition are also103 helpful in relational reasoning tasks. Our goal is to relate spatio-temporal entities, but without104 enforcing a clear decomposition of the scene into objects. This allows us to use a simpler but effective105 method that learns from classification supervision of real-world videos and obtain representations106 that are correlated to objects.107\nActivity Recognition. Video classification has been influenced by methods designed for 2D images108 [53, 54, 55, 56]. More powerful 3D convolutional networks have been later proposed [57], while109 other methods factorise the 3D convolutions [58, 59, 60] bringing both computational speed and110 accuracy. Methods like TSM [61] and [62] showed that a simple shift in the convolutional features111 results in improved accuracy at a low computational budget.112\n3 Dynamic Salient Regions GNNs113\nWe investigate how to create node representations that are useful for modeling visual interactions114 between various entities in space and time using GNNs. Our proposed Dynamic Salient Regions115 GNN model (DyReg-GNN) learns to dynamically assign each node to a certain interesting region.116 By dynamic, we mean that we have a fixed number N of regions that change their position and size117\naccording to the input at each time step. The regions assigned to each of the N nodes can change118 from one moment of time to the next depending on their saliency.119\nThe main architecture of our DyReg-GNN model is illustrated in Figure 1. Our model receives feature120 volume X \u2208 RT\u00d7H\u00d7W\u00d7C and at each time step t we predict the location and size of N regions.121 From these regions, a differentiable pooling operation creates graph nodes that are processed by a122 GNN and then are projected to their initial position. This module can be inserted at any intermediate123 level in a standard convolutional model.124\n3.1 Node Region Generation125\nWe want to attend only to a few most relevant entities in the scene, thus a small number of nodes are126 used in DyReg-GNN (in our experiments N = 9) and it is crucial to assign them to the most salient127 regions. The number of nodes is a hyperparameter that we choose such that it exceeds the expected128 number of relevant entities in the scene, to increase the robustness of the model. Thus, we propose a129 global processing (shown in Figure 1 B) that aggregates the entire input features to produce regions130 defined by parameters indicating their location (\u2206x,\u2206y) and size (w, h).131\nTo generate N salient regions, we process the input Xt using position-aware functions f and132 {gi}i\u22081,N that retain spatial information. Nodes should be consistent across time, thus we generate133 their regions in the same way at all time steps, by sharing in time the parameters of f and {gi}. The134 function f is a convolutional network that highlights the important regions from the input.135\nMt = f(Xt) \u2208 RH \u2032\u00d7W \u2032\u00d7C\u2032 (1)\nFor each node i, we generate a latent representation of its associated region using the {gi} functions.136 Each gi has the same architecture, but different parameters for each node and could be instantiated137 as a fully connected network or as global pooling enriched with spatial positional information. We138 generate the node regions from a global view to make the decision as informed as possible.139\nm\u0302i,t = gi(Mt) \u2208 RC \u2032 ,\u2200i \u2208 1, N (2)\nEach of the N latent representations is processed independently, with a GRU [63] recurrent network140 (shared between nodes), to take into account the past regions\u2019 representations.141\nzi,t = GRU(zi,t\u22121, m\u0302i,t) \u2208 RC \u2032 ,\u2200i \u2208 1, N (3)\nAt each time step, the final parameters are obtained by a linear projection Wo \u2208 RC \u2032\u00d74, transformed142 by a function \u03b1 to control the initialisation of the position and size (e.g. regions would start at143 reference points either in the center of the frame or arranged on a grid). For more details about how144 to set the transformation \u03b1 we refer to the Supplemental Materials.145\noi,t = (\u2206xi,t,\u2206yi,t, wi,t, hi,t) = \u03b1(Wozi,t) \u2208 R4 (4)\n3.2 Node Features Extraction146\nThe following operations are applied independently at each time step thus, in the current subsection,147 we ignore the time index for clarity. We extract the features corresponding to each region i using a148 differentiable pooling w.r.t. the predicted region parameters oi. All the input spatial locations p \u2208 R2149 are interpolated according to the kernel function K(i)(p) as presented in Figure 1 C.150 We present the operation for a single axis since the kernel is separable, acting in the same way on151 both axes:152\nK(i)(px, py) = k(i)x (px)k(i)y (py) \u2208 R (5)\nWe define the center of the estimated region ci,x + \u2206xi, where ci,x is a fixed reference point for node153 i (located in the frame\u2019s center or arranged on a grid). The values of the kernel decrease with the154 distance to the center and is non-zero up to a maximal distance of wi, where wi and \u2206xi are the155 predicted parameters from Eq. 4.156\nk(i)x (px) = max(0, wi \u2212 |ci,x + \u2206xi \u2212 px|) (6)\nFor each time step t, node i is created by interpolating all points in the input Xt using the kernel157 function. By modifying (\u2206xi,\u2206yi) the network controls the location of the regions, while (hi, wi)158 parameters indicate their size.159\nvi = W\u2211 px=1 H\u2211 py=1 K(i)(px, py)xpx,py \u2208 RC (7)\nSetting wi = 1 leads to standard bilinear interpolation, but optimising it allows the model to adapt160 region\u2019s size and we observe that larger ones result in a more stable optimisation (see node size161 ablations from Supp. Material).162\nThe position of the region associated with each node should be taken into account. It helps the163 relational processing by providing an identity for the node and is also useful in tasks that require164 positional information. We achieve this by computing a positional embedding for each node i using a165 linear projection of the kernel Ki into the same space as the feature vector vi and summing them.166\nKey Properties. By construction, the nodes in our method are localized, meaning that they are167 clearly associated with a location: they pool information from clearly delimited area in space and they168 maintain position information from the positional embedding. These two aspects could be helpful in169 tasks involving spatio-temporal reasoning.170\nThe dynamic aspect refers to the key capability of adapting the region\u2019s position and size according171 to the saliency of the input at each time step. This is done by predicting the regions from the input172 with the operations from equations (1\u20134).173\nAn essential aspect of this method is that the final classification loss is differentiable with respect174 to regions\u2019 parameters as the gradients are passing from the nodes outputs vi through the kernels175 ki to the parameters wi and \u2206xi. This allows us to learn regions from the final loss, without direct176 supervision for the region generation. Thus the method has more flexibility in learning relevant177 regions as appropriate for the task.178\n3.3 Graph Processing179\nFor processing the nodes\u2019 features, different spatio-temporal GNNs could be used. Generally, they180 follow a framework [12] of sending messages between connected nodes, aggregating [64, 65] and181 updating them.182\nThe specific message-passing mechanism is not the focus of the current work, thus we follow a183 general formulation similar to [3] for recurrent spatio-temporal graph processing. It uses two different184 stages: one happening between all the nodes at a single time step and the other one updating each185 node across time. For each time step t, we send messages between each pair of two nodes, computed186 as an MLP (with shared parameters) and aggregates them using a dot product attention a(vi, vj) \u2208 R.187\nvi,t = N\u2211 j=1 a(vj,t,vi,t)MLP([vj,t;vi,t]) \u2208 RC (8)\nWe incorporate temporal information through a shared recurrent function across time, applied188 independently for each node.189\nv\u0302i,t+1 = GRU(v\u0302i,t,vi,t) \u2208 RC (9) The GRU output represents the updated nodes\u2019 features and the two steps are repeated K = 3 times.190\n3.4 Graph Re-Mapping191\nTo use our method as a module inside any backbone, we produce an output with the same shape as192 the convolutional input Xt \u2208 RH\u00d7W\u00d7C . The resulting features of each node are sent to all locations193 in the input according to the weights used in the initial pooling from Section 3.2.194\nypx,py,t = N\u2211 i=1 K(i)t (px, py)v\u0302i,t \u2208 RC (10)\nWhile much effort is put into the creation of different video datasets used in the literature, such196 as Kinetics [57] or Charades [66], it has been argued [67] that they contain biases that make them197 solvable without complex spatio-temporal reasoning. CATER [67] is proposed to alleviate this, but198 it is too small (5500 videos) and still has biases that make the last few frames sufficient for good199 performance [68]. We test our model on two video classification datasets that seem to offer the best200 advantages, being large enough and requiring abilities to model complex interactions. We evaluate201 on real-world datasets, Something-Something-V1&V2 [69], while we also test on a variant of the202 SyncMNIST [3] dataset that is challenging and requires spatio-temporal reasoning, while allowing203 fast experimentation.204\n4.1 Human-Object Interactions Experiments205\nSomething-Something-V1&V2 [69] datasets classify scenes involving human-object complex in-206 teractions. They consist of 86K / 169K training videos and 11K / 25K validation videos, having207 174 classes. Unless otherwise specified, all experiments on Something-Something datasets use208 TSM-ResNet-50 [61] as a backbone and we add instances of our module at multiple stages.209\nStudying the Importance of Salient Regions Discovery. We test the importance of the dynamic210 regions for GNNs vision methods by training models where we replace the predicted regions with the211 same number of fixed regions on a grid (GNN + Fixed Regions) or boxes (GNN + Detector) as given212 by a Faster R-CNN [70] trained on MSCOCO [71].213\nThe detector based model has comparable results to the one with fixed regions, seemingly being214 unable to fully benefit from the correctly identified objects. The relative weaker performance of this215 model could be due to the fact that the pre-trained detector is not well aligned to the actual salient216 regions that are relevant for the classification problem.217\nOn the other hand, this weakness is not applicable for DyReg-GNN that learns suitable regions for the218 current task and it obtains the best performance as seen in Table 1. Not only that it does not require219 object annotations, but it is also more computationally efficient. Running the detector on a video of220 size 224\u00d7 224 would add 39.7 GFLOPS on its own, comparing to the 1.6G of three DyReg-GNN221 modules, from which 0.2G represents the regions prediction.222\nOverall, our method, with unsupervised regions obtains superior performance in terms of accuracy223 and computational efficiency representing a suitable choice for relational processing of a video.224\nObject-centric representations. The nodes represent the core processing units and their localiza-225 tion enforces a clear decision on what specific regions to focus on while completely ignoring the rest,226 as a form of hard attention. Different from other works [72], our hard attention formulation is differen-227 tiable. To better understand what elements influence the model predictions, we could inspect the pre-228 dicted kernels, thus introducing another layer of interpretability to the model, on top of the capabilities229 offered by the convolutional backbone. Visualisations of our nodes\u2019 regions reveal that generally, they230 cover the objects in the scene. For example, in the first row of Figure 3 the nodes are placed around231 the phone in the first frames and then separate into two groups, one for the phone one for the hand.232\nTable 3: Results on val. set of Smt-Smt-V1. Our model achieves competitive results compared to recent works (best results in red), while it outperforms all other graph-based methods (best results in blue).\nModel BB #F Top 1 Top 5\nno n-\nG ra\nph TSM [61] R50 16 48.4 78.1 S3D [58] R50 64 48.2 78.7 GST [73] R50 16 48.6 77.9 SmallBig [74] R50 16 50.0 79.8 STM [75] R50 16 50.7 80.4 MSNet [76] R50 16 52.1 82.3\nG ra\nph ORN [14] R50 8 36.0 - NL I3D [7] R50 32 44.4 76.0 NL GCN [7] R50 32 46.1 76.8 TRG [77] R50 16 48.1 80.4 RSTG [3] R50 32 49.2 78.8\nTSM+DyReg-r3-4-5 R50 16 49.9 79.0\nTable 4: Results on val. set of Smt-Smt-V2., in comparisons to recent works. DyReg-GNN improves the TSM-ResNet50 backbone when using either one (r4) or three (r3-4-5) modules of graph processing and it obtains top results.\nModel BB Top 1 Top 5\nTRG [77] R50 59.8 87.4 GST [73] R50 62.6 87.9 v-DP [78] D121 62.9 88.0 SmallBig [74] R50 63.8 88.9 STM [75] R50 64.2 89.8 MSNet [76] R50 64.7 89.4 TSM [61] R50 63.4 88.5\nTSM+DyReg-r4 R50 64.3 88.9 TSM+DyReg-r3-4-5 R50 64.8 89.4\nThe localized nodes make our model capable of discovering salient regions, leading to object-centric233 node representations. We quantify this capacity by measuring the mean L2 distance (normalised to234 the size of the input) between the predicted regions and ground-truth (gt.) objects given by [27]. The235 metric is completely defined in the Supp. Materials. We observed that the score improves during the236 learning process (it reaches 0.129 starting from 0.201), although the model is not optimized for this237 task. This suggests that the model actually learns object-centric representations.238\nIn Table 1 we also compare the final L2 distance of our best DyReg-GNN model to an object detector239 and to fixed grid regions. Although our method is not designed and supervised to find object regions,240 we observe that it is able to predict locations that are fairly close to gt. objects. The L2 distance is sim-241 ilar to the one obtained by an external model (0.129 vs 0.125), trained especially for detecting objects.242\nWe observe that learning the regions\u2019 size is important for the stability of the optimisation and thus for243 the final performance (see Tab.5 and Supp. Material - Regions\u2019 Size section). However, the predicted244 size is not as well aligned with the size of the true objects. This gives us a hint that for the action245 classification task it is important to have good region locations, but their size is less relevant. We246 leave a more thoroughly investigation for futures work.247\nThese experiments prove that the high-level classification task is well inter-related with the discovery248 of salient regions and that, in turn, these regions improve the relational processing in the recognition249 task. First, we show that DyReg-GNN\u2019s region obtain superior accuracy and efficiency than other250 methods of extracting nodes and second, these regions are well correlated to gt. object locations.251\nComparison to recent methods. DyReg-GNN can be used with any convolutional model and we252 show that it consistently boosts the performance of multiple backbones(Table 2). We compare to253 recent methods from the literature in Table 3 and Table 4. Our method improves the accuracy over the254 TSM-ResNet50 backbone on both Smt-Smt-V1 and Smt-Smt-V2 by 1.5% and 1.4% respectively and255 achieves competitive results. Compared to all the other graph based methods we obtain superior re-256 sults, showing that our discovery of dynamic regions is effective for space-time relational processing.257\nImplementation Details Unless otherwise specified, we use TSM-ResNet50 (pre-trained on Ima-258 geNet [79]) as our backbone and add instances of our module in the last three stages. To benefit from259 ImageNet pre-training, we add our graph module as a residual connection. We noticed that models260 using multiple graphs have problems learning to adapt the regions from certain layers. We fix this261 by training models containing a single graph at each single considered stage, as the optimisation262 process is smoother for a single module, and distill their learned offsets into the bigger model. The263 distillation is done for the first 10% of the training iterations to kick-start the optimization process264 and then continue the learning process using only the video classification signal.265\nIn all experiments we follow the training setting of [61], using 16 frames resized to have the shorter266 side of size 256, and randomly sample a crop of size 224\u00d7 224. For the evaluations, we follow the267 setting in [61] of taking 3 spatial crops of size 256\u00d7 256 with 2 temporal samplings and averaging268 their results. For training, we use SGD optimizer with learning rate 0.001 and momentum 0.9, using269\na total batch-size of 10, trained on two GPUs. We decrease the learning rate by a factor of 10 three270 times when the optimisation reaches a plateau.271\n4.2 Synthetic Experiments272 SyncMNIST is a synthetic dataset involving digits that move on a black background, some in a273 random manner, while some move synchronously. The task is to identify the digits that move in the274 same way. We use a harder variant of the dataset (MultiSyncMNIST), where the videos could include275 multiple digits of the same class. The challenge consists in finding useful entities and model their276 relationships while being able to distinguish between instances of the same class. Each video contain277 5 digits and the goal is to find the smallest and the largest digit class among the subset that moves in278 the same way. This results in a video classification task with 56 classes. The dataset contains 600k279 training videos and 10k validation videos with 10 frames each.280\nStudying the Importance of Dynamic Nodes. We validate our assumption that the nodes should281 be dynamic, meaning that their regions position and size should be adapted according to the input at282 each time step. We investigate (Table. 5) different types of localized nodes, each adapting to the input283 to a varying degree, and show the benefits of our design choices. We experiment with variants of284 our model, all having the same backbone (2D ResNet-18 [80]), the same graph processing and same285 pre-determined number of regions, but we constrain the node regions in different ways.286\nFixed Model extracts node features from regions arranged on a grid, with a fix location and size.287\nStatic Model investigates the importance of dynamic regions by optimising regions based on the288 whole dataset but do not take into account the current input. Effectively, the features zi from Eq. 4289 become learnable parameters.290\nConstant-Time Model has regions adapted to the current video but they do not change in time.291\nDyReg-GNN Model predicts regions defined by location and size, and we can either pre-determine292 a fixed size for all the regions (Position-Only Model) or directly predict it from the input as in our293 complete model (DyReg-GNN Model).294\nThese experiments (Table 5), show that the fixed region approach (Fixed Model) achieves the worst295 results, slightly improving when the regions are allowed to change according to the learned statistics of296 the dataset (Static model). Adapting to the input is shown to be beneficial, the performance improving297 even when the regions are invariant in time (Constant-Time Model), and further more when predicting298 different regions at every time steps (Position-Only). The best performance is achieved when both299 the location and the size of the regions are dynamically predicted from the input (DyReg-GNN).300\nIn Figure 2 we show examples of the kernels obtained for each of these models. We observe that the301 Static Model\u2019s kernels are learned to be arranged uniformly on a grid, to cover all possible movements302 in the scene, while the Constant-Time Model\u2019s kernels are adapted for each video such that they303 cover the main area where the digits move in the current video. The full DyReg-GNN Model learns304 to reduce the size of its regions and we observe that they closely follow the movement of the digits.305\nThe previous experiments show that performance increases when the model becomes more dynamic,306 proving that our model benefits from nodes that are adapted to a higher degree to the current input.307\nTable 5: Ablation of dynamic nodes on MultiSyncMNIST. It is crucial to have regions that adapt based on the input (Dynamic), both their position (Pos.) and size at each time step.\nModel Optimise Time Dynamic Acc Pos. Varying Pos. Size\nFixed 78.85 Static X 81.48 Ct-Time X X 86.77 Pos-Only X X X 93.41 DyReg-GNN X X X X 95.09\nTable 6: Semantic vs spatial nodes on MultiSyncMNIST. The localized (spatial) node regions of DyReg-GNN are better suited than semantic nodes\u2019 maps obtained by the Semantic Model.\nModel Params (M) Acc\nResNet-18 2.79 52.29 Fixed 2.82 78.85 Semantic 2.85 82.41\nDyReg-GNN-Lite 2.83 91.43 DyReg-GNN 3.08 95.09\nStudying the Importance of Localized Nodes. We argue that nodes should pool information from308 different locations according to the input, such that the extracted features correspond to meaningful309 entities. Depending on the goal, we could balance between semantic nodes globally extracted from310 all spatial positions or localized (spatial) nodes that are obtained from well-delimited regions.311\nSemantic Model creates nodes similar to [4, 19] where each node extracts features from all the spatial312 locations and could represent a semantic concept. Each node is extracted by a global average pooling313 where the weights at every position p are directly predicted from the input features at that location.314 Practically, we replace the spatially delimited kernel used in our model with this global attention map.315\nA major downside of this approach is that it does not distinguish between positions with the same316 features, making it harder to reason about different instances. Figure 2.C shows the attention map of317 a single node and we observe that it has equally high activations for both instances of the same digit,318 thus making it hard to distinguish between them.319\nThis limitation does not exist in our DyReg-GNN model, as it predicts localized nodes that favour the320 modeling of instances. For comparison, we use two variants with a different number of parameters321 and show that they clearly outperform the semantic model (Table 6). These experiments prove that322 in cases that involve spatial reasoning of entities, such as the current task, DyReg-GNN is a perfect323 choice, showing its benefits for spatio-temporal modeling.324\nImplementation details. All models share the ResNet-18 backbone with 3 stages, where the graph325 receives the features from the second stage and sends its output to the third stage. We useN = 9 graph326 nodes and repeat the graph propagation for three iterations. In our main model, f from Eq. 1 is a small327 convolutional network while g is a fully connected layer. For the lighter model that implements g as a328 global pooling enriched with spatial positional information, we refer to the Supp. Materials. The graph329 offsets are initialized such that all the nodes\u2019 regions start in the center of the frame. In all experiments,330 we use SGD optimizer with learning rate 0.001 and momentum 0.9, trained on a single GPU.331\nKey Results. In the previous section, we experimentally validated that: 1. DyReg-GNN consistently332 improves multiple backbones (Table 2) obtaining competitive results (Table 3, 4); 2. learned dynamic333 regions are crucial for good performance (Table 5) and 3. these regions are preferable to fixed regions334 or external object detectors for space-time GNNs (Table 1); 4. predicted nodes correspond to salient335 regions (Fig. 2-3) and are well correlated with objects (Table 1).336\n5 Conclusions337\nWe propose Dynamic Salient Regions Graph Neural Networks (DyReg-GNN), a relational model338 for processing spatio-temporal data (videos), that augments visual GNNs by learning to predict339 localized nodes, adapted for the current scene. This novel method enhances the relational processing340 of spatio-temporal GNNs and we experimentally prove that it is superior to having nodes anchored in341 fixed predefined regions or linked to external pre-trained object detectors. Although we do not use342 region level supervision, the learning dynamics of high-level classification produces salient regions343 that are well correlated with object instances. We believe that our method of learning dynamic,344 localized nodes is a valuable direction that could lead to further advances to the growing number of345 powerful relational models in spatio-temporal domains.346\nReferences347 [1] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective348 receptive field in deep convolutional neural networks. In D. D. Lee, M. Sugiyama, U. V.349 Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems350 29, pages 4898\u20134906. Curran Associates, Inc., 2016.351\n[2] Sijie Yan, Yuanjun Xiong, and Dahua Lin. Spatial temporal graph convolutional networks for352 skeleton-based action recognition. In Thirty-Second AAAI Conference on Artificial Intelligence,353 2018.354\n[3] Andrei Nicolicioiu, Iulia Duta, and Marius Leordeanu. Recurrent space-time graph neural355 networks. In Advances in Neural Information Processing Systems 32, pages 12838\u201312850.356 Curran Associates, Inc., 2019.357\n[4] Y. Chen, M. Rohrbach, Z. Yan, Y. Shuicheng, J. Feng, and Y. Kalantidis. Graph-based global rea-358 soning networks. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition359 (CVPR), pages 433\u2013442, 2019.360\n[5] Adam Santoro, David Raposo, David G Barrett, Mateusz Malinowski, Razvan Pascanu, Peter361 Battaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. In362 Advances in Neural Information Processing Systems 30, pages 4967\u20134976, 2017.363\n[6] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,364 Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,365 Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image366 recognition at scale. In International Conference on Learning Representations, 2021.367\n[7] Xiaolong Wang and Abhinav Gupta. Videos as space-time region graphs. In Proceedings of the368 European Conference on Computer Vision (ECCV), pages 399\u2013417, 2018.369\n[8] Elizabeth S Spelke. Core knowledge. American psychologist, 55(11):1233, 2000.370\n[9] Bogdan Alexe, Thomas Deselaers, and Vittorio Ferrari. What is an object? In 2010 IEEE371 Computer Society Conference on Computer Vision and Pattern Recognition, pages 73\u201380, 2010.372\n[10] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally373 connected networks on graphs. CoRR, abs/1312.6203, 2013.374\n[11] Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction375 networks for learning about objects, relations and physics. In Advances in neural information376 processing systems, pages 4502\u20134510, 2016.377\n[12] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl.378 Neural message passing for quantum chemistry. In Doina Precup and Yee Whye Teh, edi-379 tors, Proceedings of the 34th International Conference on Machine Learning, volume 70 of380 Proceedings of Machine Learning Research, pages 1263\u20131272, 2017.381\n[13] Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. Learning deep382 generative models of graphs, 2018.383\n[14] Fabien Baradel, Natalia Neverova, Christian Wolf, Julien Mille, and Greg Mori. Object level384 visual reasoning in videos. In ECCV, June 2018.385\n[15] Siyuan Qi, Wenguan Wang, Baoxiong Jia, Jianbing Shen, and Song-Chun Zhu. Learning386 human-object interactions by graph parsing neural networks. In Proceedings of the European387 Conference on Computer Vision (ECCV), pages 401\u2013417, 2018.388\n[16] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius389 Zambaldi, Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan390 Faulkner, et al. Relational inductive biases, deep learning, and graph networks. arXiv preprint391 arXiv:1806.01261, 2018.392\n[17] Keyulu Xu, Jingling Li, Mozhi Zhang, Simon S. Du, Ken ichi Kawarabayashi, and Stefanie393 Jegelka. What can neural networks reason about? In International Conference on Learning394 Representations, 2020.395\n[18] Yin Li and Abhinav Gupta. Beyond grids: Learning graph representations for visual recognition.396 In Advances in Neural Information Processing Systems, pages 9225\u20139235, 2018.397\n[19] Xiaodan Liang, Zhiting Hu, Hao Zhang, Liang Lin, and Eric P Xing. Symbolic graph reasoning398 meets convolutions. In Advances in Neural Information Processing Systems 31, pages 1853\u2013399 1863. 2018.400\n[20] Songyang Zhang, Xuming He, and Shipeng Yan. Latentgnn: Learning efficient non-local401 relations for visual recognition. In International Conference on Machine Learning, pages402 7374\u20137383. PMLR, 2019.403\n[21] Thomas Kipf, Elise van der Pol, and Max Welling. Contrastive learning of structured world404 models. In International Conference on Learning Representations, 2020.405\n[22] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg406 Heigold, Jakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with407 slot attention. arXiv preprint arXiv:2006.15055, 2020.408\n[23] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, and Jiashi Feng. A\u02c6 2-nets:409 Double attention networks. In Advances in Neural Information Processing Systems, pages410 350\u2013359, 2018.411\n[24] J. Gao, T. Zhang, and C. Xu. Graph convolutional tracking. In 2019 IEEE/CVF Conference on412 Computer Vision and Pattern Recognition (CVPR), pages 4644\u20134654, 2019.413\n[25] Roei Herzig, Elad Levi, Huijuan Xu, Hang Gao, Eli Brosh, Xiaolong Wang, Amir Globerson,414 and Trevor Darrell. Spatio-temporal action graph networks. In Proceedings of the IEEE415 International Conference on Computer Vision Workshops, pages 0\u20130, 2019.416\n[26] Yubo Zhang, Pavel Tokmakov, Martial Hebert, and Cordelia Schmid. A structured model for417 action detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern418 Recognition, pages 9975\u20139984, 2019.419\n[27] Joanna Materzynska, Tete Xiao, Roei Herzig, Huijuan Xu, Xiaolong Wang, and Trevor Darrell.420 Something-else: Compositional action recognition with spatial-temporal interaction networks.421 In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020.422\n[28] Moshiko Raboh, Roei Herzig, Jonathan Berant, Gal Chechik, and Amir Globerson. Differen-423 tiable scene graphs. In Proceedings of the IEEE/CVF Winter Conference on Applications of424 Computer Vision (WACV), March 2020.425\n[29] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks.426 In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 1,427 page 4, 2018.428\n[30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,429 \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-430 tion Processing Systems, pages 5998\u20136008, 2017.431\n[31] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and432 Jonathon Shlens. Stand-alone self-attention in vision models. arXiv preprint arXiv:1906.05909,433 2019.434\n[32] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and435 Sergey Zagoruyko. End-to-end object detection with transformers. In European Conference on436 Computer Vision, pages 213\u2013229, 2020.437\n[33] Chen Sun, Abhinav Shrivastava, Carl Vondrick, Kevin Murphy, Rahul Sukthankar, and Cordelia438 Schmid. Actor-centric relation network. In Proceedings of the European Conference on439 Computer Vision (ECCV), pages 318\u2013334, 2018.440\n[34] Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew Zisserman. Video action transformer441 network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,442 pages 244\u2013253, 2019.443\n[35] Xinlei Chen, Li-Jia Li, Li Fei-Fei, and Abhinav Gupta. Iterative visual reasoning beyond444 convolutions. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages445 7239\u20137248, 2018.446\n[36] Effrosyni Mavroudi, Benjam\u0131n B\u00e9jar, and Ren\u00e9 Vidal. Representation learning on visual-447 symbolic graphs for video understanding. In The European Conference on Computer Vision448 (ECCV), 2020.449\n[37] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual450 attention network for scene segmentation. In Proceedings of the IEEE Conference on Computer451 Vision and Pattern Recognition, pages 3146\u20133154, 2019.452\n[38] Nasim Rahaman, Anirudh Goyal, Muhammad Waleed Gondal, Manuel Wuthrich, Stefan Bauer,453 Yash Sharma, Yoshua Bengio, and Bernhard Sch\u00f6lkopf. S2rms: Spatially structured recurrent454 modules. arXiv preprint arXiv:2007.06533, 2020.455\n[39] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial trans-456 former networks. In Advances in Neural Information Processing Systems, volume 28, pages457 2017\u20132025, 2015.458\n[40] Jifeng Dai, Kaiming He, and Jian Sun. Instance-aware semantic segmentation via multi-task459 network cascades. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),460 pages 3150\u20133158, 2016.461\n[41] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of462 the IEEE international conference on computer vision, pages 2961\u20132969, 2017.463\n[42] Borui Jiang, Ruixuan Luo, Jiayuan Mao, Tete Xiao, and Yuning Jiang. Acquisition of localiza-464 tion confidence for accurate object detection. In Proceedings of the European Conference on465 Computer Vision (ECCV), pages 784\u2013799, 2018.466\n[43] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V Gool. Dynamic filter networks. In467 Advances in Neural Information Processing Systems, pages 667\u2013675, 2016.468\n[44] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei.469 Deformable convolutional networks. In Proceedings of the IEEE international conference on470 computer vision, pages 764\u2013773, 2017.471\n[45] Xizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. Deformable convnets v2: More deformable,472 better results. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni-473 tion, pages 9308\u20139316, 2019.474\n[46] Li Zhang, Dan Xu, Anurag Arnab, and Philip HS Torr. Dynamic graph message passing network.475 In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020.476\n[47] Anirudh Goyal, Alex Lamb, Jordan Hoffmann, Shagun Sodhani, Sergey Levine, Yoshua Bengio,477 and Bernhard Sch\u00f6lkopf. Recurrent independent mechanisms. arXiv preprint arXiv:1909.10893,478 2019.479\n[48] Klaus Greff, Sjoerd van Steenkiste, and J\u00fcrgen Schmidhuber. On the binding problem in480 artificial neural networks. arXiv preprint arXiv:2012.05208, 2020.481\n[49] Klaus Greff, Rapha\u00ebl Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess,482 Daniel Zoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object rep-483 resentation learning with iterative variational inference. In Kamalika Chaudhuri and Ruslan484 Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning,485 volume 97 of Proceedings of Machine Learning Research, pages 2424\u20132433. PMLR, 09\u201315486 Jun 2019.487\n[50] Christopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt488 Botvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and representa-489 tion. arXiv preprint arXiv:1901.11390, 2019.490\n[51] Zhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong491 Jiang, and Sungjin Ahn. Space: Unsupervised object-oriented scene representation via spatial492 attention and decomposition. In International Conference on Learning Representations, 2020.493\n[52] Qian Huang, Horace He, Abhay Singh, Yan Zhang, Ser-Nam Lim, and Austin R. Benson. Better494 set representations for relational reasoning. In Hugo Larochelle, Marc\u2019Aurelio Ranzato, Raia495 Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information496 Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020,497 NeurIPS 2020, December 6-12, 2020, virtual, 2020.498\n[53] Joe Yue-Hei Ng, Matthew Hausknecht, Sudheendra Vijayanarasimhan, Oriol Vinyals, Rajat499 Monga, and George Toderici. Beyond short snippets: Deep networks for video classification.500 In Proceedings of the IEEE conference on computer vision and pattern recognition, pages501 4694\u20134702, 2015.502\n[54] Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini503 Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for504 visual recognition and description. In Proceedings of the IEEE conference on computer vision505 and pattern recognition, pages 2625\u20132634, 2015.506\n[55] Chih-Yao Ma, Min-Hung Chen, Zsolt Kira, and Ghassan AlRegib. Ts-lstm and temporal-507 inception: Exploiting spatiotemporal dynamics for activity recognition. Signal Processing:508 Image Communication, 2018.509\n[56] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba. Temporal relational reasoning510 in videos. In Proceedings of the European Conference on Computer Vision (ECCV), pages511 803\u2013818, 2018.512\n[57] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the513 kinetics dataset. In Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference514 on, pages 4724\u20134733. IEEE, 2017.515\n[58] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spa-516 tiotemporal feature learning: Speed-accuracy trade-offs in video classification. In Proceedings517 of the European Conference on Computer Vision (ECCV), pages 305\u2013321, 2018.518\n[59] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A519 closer look at spatiotemporal convolutions for action recognition. In Proceedings of the IEEE520 conference on Computer Vision and Pattern Recognition, pages 6450\u20136459, 2018.521\n[60] Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feiszli. Video classification with channel-522 separated convolutional networks. In Proceedings of the IEEE International Conference on523 Computer Vision, pages 5552\u20135561, 2019.524\n[61] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efficient video understand-525 ing. In Proceedings of the IEEE International Conference on Computer Vision, 2019.526\n[62] Linxi Fan, Shyamal Buch, Guanzhi Wang, Ryan Cao, Yuke Zhu, Juan Carlos Niebles, and527 Li Fei-Fei. Rubiksnet: Learnable 3d-shift for efficient video action recognition. In Proceedings528 of the European Conference on Computer Vision (ECCV), 2020.529\n[63] Kyunghyun Cho, Bart van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares,530 Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoder\u2013531 decoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical532 Methods in Natural Language Processing (EMNLP), pages 1724\u20131734, 2014.533\n[64] Petar Velic\u030ckovic\u0301, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li\u00f2, and Yoshua534 Bengio. Graph attention networks. In International Conference on Learning Representations,535 2018.536\n[65] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural537 networks? In International Conference on Learning Representations, 2019.538\n[66] Gunnar A. Sigurdsson, G\u00fcl Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav539 Gupta. Hollywood in homes: Crowdsourcing data collection for activity understanding. In540 European Conference on Computer Vision, 2016.541\n[67] Rohit Girdhar and Deva Ramanan. Cater: A diagnostic dataset for compositional actions and542 temporal reasoning. In International Conference on Learning Representations, 2020.543\n[68] Honglu Zhou, Asim Kadav, Farley Lai, Alexandru Niculescu-Mizil, Martin Renqiang Min,544 Mubbasir Kapadia, and Hans Peter Graf. Hopper: Multi-hop transformer for spatiotemporal545 reasoning. In International Conference on Learning Representations, 2021.546\n[69] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne547 Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag,548 et al. The\" something something\" video database for learning and evaluating visual common549 sense. In ICCV, volume 1, page 3, 2017.550\n[70] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time551 object detection with region proposal networks. In Advances in neural information processing552 systems, pages 91\u201399, 2015.553\n[71] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr554 Dollar, and Larry Zitnick. Microsoft coco: Common objects in context. In ECCV. European555 Conference on Computer Vision, September 2014.556\n[72] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov,557 Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with558 visual attention. In International conference on machine learning, pages 2048\u20132057. PMLR,559 2015.560\n[73] Chenxu Luo and Alan Yuille. Grouped spatial-temporal aggretation for efficient action recogni-561 tion. In Proceedings of the IEEE International Conference on Computer Vision, 2019.562\n[74] X. Li, Yali Wang, Zhipeng Zhou, and Yu Qiao. Smallbignet: Integrating core and contextual563 views for video classification. 2020 IEEE/CVF Conference on Computer Vision and Pattern564 Recognition (CVPR), pages 1089\u20131098, 2020.565\n[75] Boyuan Jiang, MengMeng Wang, Weihao Gan, Wei Wu, and Junjie Yan. Stm: Spatiotem-566 poral and motion encoding for action recognition. In Proceedings of the IEEE International567 Conference on Computer Vision, pages 2000\u20132009, 2019.568\n[76] Heeseung Kwon, Manjin Kim, Suha Kwak, and Minsu Cho. Motionsqueeze: Neural motion569 feature learning for video understanding. In European Conference on Computer Vision, pages570 345\u2013362. Springer, 2020.571\n[77] J. Zhang, F. Shen, Xing Xu, and H. Shen. Temporal reasoning graph for activity recognition.572 IEEE Transactions on Image Processing, 29:5491\u20135506, 2020.573\n[78] Yizhou Zhou, Xiaoyan Sun, Chong Luo, Zheng-Jun Zha, and Wenjun Zeng. Spatiotemporal574 fusion in 3d cnns: A probabilistic view. In Proceedings of the IEEE/CVF Conference on575 Computer Vision and Pattern Recognition, pages 9829\u20139838, 2020.576\n[79] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng577 Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei.578 ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision579 (IJCV), pages 211\u2013252, 2015.580\n[80] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image581 recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),582 pages 770\u2013778, 2016.583\nChecklist584\n1. For all authors...585 (a) Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s586 contributions and scope? [Yes] We summarise the arguments we make in the paper for587 each of our claims as follows:588 Claim 1: \"a novel method to augment spatio-temporal GNNs by an additional capabil-589 ity: that of learning to create localized nodes suited for spatial reasoning, that adapt to590 the input\". Our method to create nodes used in a space-time GNN is different from any591 existing method by being localized, dynamic (able to adapt to the current input) and592 unsupervised at the region level. We clearly explain in the related work how prior work593 handle node creation.594 Claim 2: \"Our salient regions discovery enhance the relational processing\". We595 show improvements over multiple backbones and other methods on two real-world596 datasets (Smt-Smt V1 and V2) and a challenging synthetic dataset MultiSyncMNIST.597 We specifically compare to having fixed regions, or regions given by object detectors598 in Sec. 4.1.599 Claim 3: \"unsupervised salient regions discovery\". Our model is not supervised with600 region / object information but the discovered regions are correlated with objects as601 proven in Sec. 4.1.602 Claim 4: Dynamic nodes are important. We argue that it is important to have dynamic603 nodes: nodes that are able to adapt their location and size according to each frame.604 We validate experimentally both on Smt-Smt V2 (in Sec. 4.1) and more thoroughly on605 MultiSyncMNIST (in Sec.4.2)606 Claim 5: Localized nodes are important. We compare the proposed localized nodes vs607 semantic nodes similar to prior work in Sec. 4.2.608\n(b) Did you describe the limitations of your work? [Yes] We give details regarding the609 limitations in Supp. Material.610\n(c) Did you discuss any potential negative societal impacts of your work? [Yes] We discuss611 the impact in Supp. Material.612\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to613 them? [Yes]614\n2. If you are including theoretical results...615 (a) Did you state the full set of assumptions of all theoretical results? [N/A]616 (b) Did you include complete proofs of all theoretical results? [N/A]617\n3. If you ran experiments...618 (a) Did you include the code, data, and instructions needed to reproduce the main exper-619 imental results (either in the supplemental material or as a URL)? [Yes] The code is620 included in the Supp. Material. We will also release it publicly upon acceptance.621\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they622 were chosen)? [Yes]623\n(c) Did you report error bars (e.g., with respect to the random seed after running experi-624 ments multiple times)? [No]625\n(d) Did you include the total amount of compute and the type of resources used (e.g., type626 of GPUs, internal cluster, or cloud provider)? [Yes] We include the number of GPUs627 used for each experiment, but we could not estimate the total amount of compute.628\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...629 (a) If your work uses existing assets, did you cite the creators? [Yes]630 (b) Did you mention the license of the assets? [Yes]631 (c) Did you include any new assets either in the supplemental material or as a URL? [Yes]632 We include a new variant of SyncMNIST dataset.633 (d) Did you discuss whether and how consent was obtained from people whose data you\u2019re634 using/curating? [No]635 (e) Did you discuss whether the data you are using/curating contains personally identifiable636 information or offensive content? [No]637\n5. If you used crowdsourcing or conducted research with human subjects...638 (a) Did you include the full text of instructions given to participants and screenshots, if639 applicable? [N/A]640 (b) Did you describe any potential participant risks, with links to Institutional Review641 Board (IRB) approvals, if applicable? [N/A]642 (c) Did you include the estimated hourly wage paid to participants and the total amount643 spent on participant compensation? [N/A]644"
      }
    ],
    "year": 2021,
    "abstractText": "Graph Neural Networks are perfectly suited to capture latent interactions between 1 various entities in the spatio-temporal domain (e.g. videos). However, when an 2 explicit structure is not available, it is not obvious what atomic elements should 3 be represented as nodes. Current works generally use pre-trained object detectors 4 or fixed, predefined regions to extract graph nodes. Improving upon this, our 5 proposed model learns nodes that dynamically attach to well-delimited salient 6 regions, which are relevant for a higher-level task, without using any object-level 7 supervision. Constructing these localized, adaptive nodes gives our model inductive 8 bias towards object-centric representations and we show that it discovers regions 9 that are well correlated with objects in the video. In extensive ablation studies 10 and experiments on two challenging datasets, we show superior performance to 11 previous graph neural networks models for video classification. 12",
    "creator": "LaTeX with hyperref"
  },
  "output": [
    [
      "1. **W1) Choice of benchmark dataset (Something-Something):** I think that the proposed method would be useful in real-life applications if it could significantly improve upon a fixed grid approach in real datasets, without the added overhead of an object detector. I appreciate that the authors compared these 3 settings. However, I feel that the chosen real-life dataset is too limited in terms of the richness of scenes. This does not only limit the expected impact from using salient regions instead of grids, but might also be an easy dataset for the unsupervised region discovery. For example, most scenes have only a few objects and a single person (hands) interacting with them. Therefore, although action recognition in SthSth is indeed a challenging spatio-temporal reasoning task, finding salient regions is not as hard, since most of the foreground blobs are relevant. We can see that from the experimental results as well. In Table 1, we observe that using a fixed grid vs an object detector vs the proposed salient nodes yields an accuracy of 64.1 vs 64.0 vs 64.8. The fact that object detections do not improve over the grid might be because the grid already captures all the relevant objects and there are not many irrelevant grid cells.",
      "2. **W2) Missing ablations:** Was there an ablation about not using the GRU when generating nodes over time (Eq. 3)? Also, what is the impact of the attention component in Eq. 8? Could it be that grid-based GNNs without attention would be much worse than the proposed DyReg-GNN? Did the authors experiment with different number of GNN layers or different loss scalar weight \\lambda?",
      "3. **W3) Evaluation of alignment of generated regions with ground-truth objects:** Why (only) use the L2 distance for comparing regions with ground-truth object regions, and not IoU as well (Table 1)? Also, an additional reported metric could be the number of salient regions covering each object instance. The authors acknowledged this limitation, namely that because of the fixed number of nodes, a lot of the generated regions can focus on the same object. Object recall is also an important metric. It would be also interesting to have a quantitative evaluation on whether the model learns some form of tracking, although it is not the primary goal.",
      "4. **W4) Comparison with SOTA:** The approach improves marginally improved results compared to RSTG (same? GNN applied on feature grids) - 49.2 vs 49.9%. Also, the method is not state-of-the-art when compared to non-graph methods that use the same backbone and number of frames.  Question: does the RSTG[3] also use the TSM module?"
    ],
    [
      "1. No weaknesses mentioned."
    ],
    [
      "1. \"similar ideas have been proposed in previous methods like OP3[1] and V-CDN[2], where they predict object masks / keypoints and use graph nets to model dynamics, all without supervision.\"",
      "2. \"The main metric considered is the classification accuracy, on which using dynamically adapted regions only slightly outperforms using fixed regions (Table 1).\"",
      "3. \"Also, in Appendix Figure 5, it seems that each node covers a specific region. These results lead to the question: Is it necessary to use dynamic regions?\"",
      "4. \"For object-centric representations to benefit interaction modeling, it is best if the representations are temporally consistent, i.e., same node capturing same object across time. However, I did not see much evalution on this temporal consistency aspect, and Figure 5 seems to suggest that if an object moves a long distance, it will be captured by different nodes.\"",
      "5. \"I was a bit confused about the visualization of node regions in Figures 3 and 4. In particular, the number of regions in Figure 4 is much smaller than the number of nodes, so what nodes are visualized?\"",
      "6. \"Also the learned position of nodes looks similar to keypoints [2-4], which can be predicted without supervision even for realistic images.\""
    ],
    [
      "1. \"The benefit of using static nodes (e.g. using objects) is that there is increased model interpretability and the model learns to establish between static entities in the dataset. Using convolutional features that change every time-step based on output from conv layers is not very interpretable.\"",
      "2. \"There is criticism of datasets in Section 4, some of which is valid. Kinetics does have a high spatial bias but does contain fine-grained classes (e.g. around basketball) that are far from being solved and it would be interesting to see how your model can separate actions for classes that involve similar objects (or salient regions).\"",
      "3. \"It is not clear that the relational processing enhaced i.e. better higher order relations are established.\"",
      "4. \"How did you pick 9 regions/objects? Perhaps to make the system completely dynamic, this can be a learned parameter as well.\""
    ]
  ],
  "review_num": 4,
  "item_num": [
    4,
    1,
    6,
    4
  ]
}