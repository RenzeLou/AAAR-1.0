{
  "ID": "_0kknjKJ6BH",
  "Title": "End-to-end reconstruction meets data-driven regularization for inverse problems",
  "URL": "https://openreview.net/forum?id=_0kknjKJ6BH",
  "paper_draft_url": "/references/pdf?id=Yy2CqW3WPY7",
  "Conferece": "NeurIPS_2021",
  "input": {
    "source": "CRF",
    "title": "End-to-end reconstruction meets data-driven regularization for inverse problems",
    "authors": [],
    "emails": [],
    "sections": [
      {
        "heading": null,
        "text": "We propose an unsupervised approach for learning end-to-end reconstruction oper-1 ators for ill-posed inverse problems. The proposed method combines the classical2 variational framework with iterative unrolling, which essentially seeks to mini-3 mize a weighted combination of the expected distortion in the measurement space4 and the Wasserstein-1 distance between the distributions of the reconstruction5 and ground-truth. More specifically, the regularizer in the variational setting is6 parametrized by a deep neural network and learned simultaneously with the un-7 rolled reconstruction operator. The variational problem is then initialized with8 the reconstruction of the unrolled operator and solved iteratively till convergence.9 Notably, it takes significantly fewer iterations to converge, thanks to the excellent10 initialization obtained via the unrolled operator. The resulting approach combines11 the computational efficiency of end-to-end unrolled reconstruction with the well-12 posedness and noise-stability guarantees of the variational setting. Moreover, we13 demonstrate with the example of X-ray computed tomography (CT) that our ap-14 proach outperforms state-of-the-art unsupervised methods, and that it outperforms15 or is on par with state-of-the-art supervised learned reconstruction approaches.16\n1 Introduction17\nInverse problems are ubiquitous in imaging applications, wherein one seeks to recover an unknown18 model parameter x \u2208 X from its incomplete and noisy measurement, given by19\ny\u03b4 = A(x) + e \u2208 Y. Here, the forward operator A : X\u2192 Y models the measurement process in the absence of noise, and20 e, with \u2016e\u20162 \u2264 \u03b4, denotes the measurement noise. For example, in computed tomography (CT), the21 forward operator computes line integrals of x over a predetermined set of lines in R3 and the goal22 is to reconstruct x from its projections along these lines. Without any further information about x,23 inverse problems are typically ill-posed, meaning that there could be several reconstructions that are24 consistent with the measurement, even without any noise.25\nThe variational framework circumvents ill-posedness by encoding prior knowledge about x via a26 regularization functionalR : X\u2192 R. In the variational setting, one solves27\nmin x\u2208X LY(y\u03b4,A(x)) + \u03bbR(x), (1)\nwhere LY : Y\u00d7 Y\u2192 R+ measures data-fidelity andR penalizes undesirable or unlikely solutions.28 The penalty \u03bb > 0 balances the regularization strength with the fidelity of the reconstruction. The29 variational problem (1) is said to be well-posed if it has a unique solution varying continuously in y\u03b4 .30\nThe success of deep learning in recent years has led to a surge of data-driven approaches for31 solving inverse problems [5], especially in imaging applications. These methods come broadly32\nSubmitted to 35th Conference on Neural Information Processing Systems (NeurIPS 2021). Do not distribute.\nin two flavors: (i) end-to-end trained models that aim to directly map the measurement to the33 corresponding parameter and (ii) learned regularization methods that seek to find a data-adaptive34 regularizer instead of handcrafting it. Techniques in both categories have their relative advantages35 and demerits. Specifically, end-to-end approaches offer fast reconstruction of astounding quality, but36 lack in terms of theoretical guarantees and need supervised data (i.e., pairs of input and target images)37 for training. On the contrary, learned regularization methods inherit the provable well-posedness38 properties of the variational setting and can be trained in an unsupervised manner, however the39 reconstruction entails solving a high-dimensional optimization problem, which is often slow and40 computationally demanding.41\nOur work derives ideas from learned optimization and adversarial machine learning, and makes an42 attempt to combine the best features of both aforementioned paradigms. In particular, the proposed43 method offers the flexibility of unsupervised training, produces fast reconstructions comparable to44 end-to-end supervised methods in quality, while enjoying the well-posedness and stability guarantees45 of the learned regularization framework. We first provide a brief overview of the literature on46 data-driven techniques for inverse problems before explaining our specific contributions in detail.47\n1.1 Related works48\nEnd-to-end fully learned methods for imaging inverse problems either map the measurement directly49 to the image [30, 19], or learn to eliminate the artifacts from a model-based technique [10]. Such50 approaches are data-intensive and may generalize poorly when trained on limited data. Iterative51 unrolling [17, 29, 2, 11, 16], with its origin in the seminal work by Gregor and LeCun on data-52 driven sparse coding [9], employs reconstruction networks that are inspired by optimization-based53 approaches and hence are interpretable. The unrolling paradigm enables one to encode the knowledge54 about the acquisition physics into the model architecture [3], thereby achieving data-efficiency.55 Nevertheless, end-to-end trained methods are supervised, and it is often challenging to obtain a large56 ensemble of paired data, especially in medical imaging applications.57\nLearned regularization methods, broadly speaking, aim to learn a data-driven regularizer in the58 variational setting. Some notable approaches in this paradigm include adversarial regularization (AR)59 [14] and its convex counterpart [18], network Tikhonov (NETT) [13], total deep variation (TDV)60 [12], etc., wherein one explicitly parametrizes the regularization functional using a neural network.61 The regularization by denoising (RED) approach aims to solve inverse problems by using a denoiser62 inside an algorithm for minimizing the variational objective [21, 22, 7]. The Plug-and-play (PnP)63 method [25] with a learned denoiser is also implicitly equivalent to data-driven regularization, subject64 to additional constraints on the denoiser [20]. The deep image prior technique [26] does not require65 training, but it seeks to regularize the solution by restricting it to be in the range of a deep generator66 and can thus be interpreted broadly as a deep learning-based regularization scheme. It is relatively67 easier to analyze learned regularization schemes using the machinery of classical functional analysis68 [24], but they fall short in terms of reconstruction quality. Moreover, these methods require one to69 solve a high-dimensional, potentially non-convex, variational problem, leading to slow reconstruction70 and lack of provable convergence.71\n1.2 Specific contributions72\nOur work seeks to combine iterative unrolling with data-adaptive regularization via an adversarial73 learning framework, and hence is referred to as unrolled adversarial regularization (UAR). The74 proposed approach learns a data-adaptive regularizer parametrized by a neural network, together with75 an iteratively unrolled reconstruction network that minimizes the corresponding expected variational76 loss in an adversarial setting. Unlike AR [14] where the undesirable images are taken as the pseudo-77 inverse reconstruction and kept fixed throughout the training, we update them with the output of78 the unrolled reconstruction network in each training step, and, in turn, use them to further improve79 the regularizer. Thanks to the Kantorovich-Rubinstein (KR) duality [4], the alternating learning80 strategy of the reconstruction and the regularizer networks is equivalent to minimizing the expected81 data-fidelity over the distribution of the measurements, penalized by the Wasserstein-1 distance82 between the distribution of the reconstruction and the ground-truth. Once trained, the reconstruction83 operator produces a fast, end-to-end reconstruction. We show that this efficient reconstruction can be84 improved further by a refinement step that involves running a few iterations of gradient-descent on the85 variational loss with the corresponding regularizer, starting from this initial estimate. The refinement86\nstep not only produces reconstructions that outperform state-of-the-art unsupervised methods and are87 competitive with supervised methods, but also facilitates a well-posedness and stability analyses akin88 to classical variational approaches [24]. Our theoretical results on the learned unrolled operator and89 the regularizer are corroborated by strong experimental evidence for the CT inverse problem.90\n2 The proposed unrolled adversarial regularization (UAR) approach91\nIn this section, we give a short mathematical background on optimal transport, followed by a detailed92 description of the UAR framework, including the training protocol and the network architectures.93\n2.1 Background on Optimal transport94\nOptimal transport theory [8, 27] has recently gained prominence in the context of measuring the95 distance between two probability distributions. In particular, given two probability distributions \u03c0196 and \u03c02 on Rn, the Wasserstein-1 distance between them is defined as97\nW1(\u03c01, \u03c02) := inf \u00b5\u2208\u03a0(\u03c01,\u03c02)\n\u222b \u2016x1 \u2212 x2\u20162 d\u00b5(x1,x2), (2)\nwhere \u03a0(\u03c01, \u03c02) denotes all transport plans having \u03c01 and \u03c02 as marginals. The Wasserstein distance98 has proven to be suitable for deep learning tasks, when the data is assumed to be concentrated on99 low-dimensional manifolds in Rn. It has been shown that in such cases, the Wasserstein distance100 provides a usable gradient during training [4], as opposed to other popular divergence measures.101\nBy the KR duality, the Wasserstein-1 distance can be computed equivalently by solving a maximiza-102 tion problem over the space of 1-Lipschitz functions (denoted by L1) as103\nW1(\u03c01, \u03c02) = sup R\u2208L1\n\u222b R(x1) d\u03c01(x1)\u2212 \u222b R(x2) d\u03c02(x2), (3)\nprovided that \u03c01 and \u03c02 have compact support [23]. Finally, we recall the definition of push-forward104 of probability measures, which is used extensively in our theoretical exposition. Given a probability105 measure \u03c0 on A and a measurable map T : A \u2192 B, we define the push-forward of \u03c0 by T (denoted106 as T#\u03c0) as a probability measure on B such that T#\u03c0(B) = \u03c0(T\u22121(B)), for all measurableB \u2282 B.107\n2.2 Training strategy and model parametrization for UAR108\nThe principal idea behind UAR is to learn an unrolled deep network G\u03c6 : Y\u2192 X for reconstruction,109 together with a regularization functionalR\u03b8 : X\u2192 R parametrized by another convolutional neural110 network (CNN). The role of R\u03b8 is to discern ground-truth images from images produced by G\u03c6,111 while G\u03c6 learns to minimize the variational loss withR\u03b8 as the regularizer. As the images produced112 by G\u03c6 gets better,R\u03b8 faces a progressively harder task of telling them apart from the ground-truth113 images, thus leading to an improved regularizer. On the other hand, as the regularizer improves, the114 quality of reconstructions obtained using G\u03c6 improves simultaneously. Consequently, G\u03c6 and R\u03b8115 helps each other improve as the training progresses via an alternating update scheme.116\n2.2.1 Adversarial training117\nLet us denote by \u03c0x the ground-truth distribution and by \u03c0y\u03b4 the distribution of the noisy measurement.118 The UAR algorithm trains G\u03c6 andR\u03b8 simultaneously starting from an appropriate initialization. At119 the kth iteration of training, the parameters \u03c6 of the reconstruction network are updated as120\n\u03c6k \u2208 arg min \u03c6 J (1) k (\u03c6), where J (1) k (\u03c6) := E\u03c0y\u03b4 [\u2225\u2225A(G\u03c6(y\u03b4))\u2212 y\u03b4\u2225\u222522 + \u03bbR\u03b8k(G\u03c6(y\u03b4))] , (4) for a fixed regularizer parameter \u03b8k. Subsequently, the regularizer parameters are updated as121\n\u03b8k+1 \u2208 arg max \u03b8:R\u03b8\u2208L1 J (2) k (\u03b8), where J (2) k (\u03b8) := E\u03c0y\u03b4\n[ R\u03b8 ( G\u03c6k(y\u03b4) )] \u2212 E\u03c0x [R\u03b8 (x)] . (5)\nThe learning protocol for UAR is unsupervised, since the loss functionals J (1)k (\u03c6) and J (2) k (\u03b8) can be122 computed based solely on the marginals \u03c0x and \u03c0y\u03b4 . The alternating update algorithm in (4) and (5)123\nAlgorithm 1 Learning unrolled adversarial regularization (UAR). 1. Input: Data-set {xi}Ni=1 \u223c \u03c0x and { yj }N j=1 \u223c \u03c0y\u03b4 , initial reconstruction network parameter\n\u03c6 and regularizer parameter \u03b8, batch-size nb = 1, penalty \u03bb = 0.1, gradient penalty \u03bbgp = 10.0, Adam optimizer parameters (\u03b21, \u03b22) = (0.50, 0.99). 2. (Learn a baseline regularizer) for mini-batch k = 1, 2, \u00b7 \u00b7 \u00b7 , 10 Nnb , do: \u2022 Sample xj \u223c \u03c0x, yj \u223c \u03c0y\u03b4 , and j \u223c uniform [0, 1]; for 1 \u2264 j \u2264 nb. Compute uj = A\u2020(yj)\nand x( )j = jxj + (1\u2212 j)uj . \u2022 \u03b8 \u2190 Adam\u03b7,\u03b21,\u03b22(\u03b8,\u2207 J\u03031(\u03b8)), where \u03b7 = 10\u22124, and\nJ\u03031(\u03b8) = 1\nnb nb\u2211 j=1 [ R\u03b8 (xj)\u2212R\u03b8 (uj) + \u03bbgp (\u2225\u2225\u2225\u2207R\u03b8 (x( )j )\u2225\u2225\u2225 2 \u2212 1 )2] .\n3. (Learn a baseline reconstruction operator) for mini-batch k = 1, 2, \u00b7 \u00b7 \u00b7 , 5 Nnb , do: \u2022 Sample yj \u223c \u03c0y\u03b4 , and compute J\u03032(\u03c6) = 1nb \u2211nb j=1 \u2225\u2225yj \u2212A (G\u03c6(yj))\u2225\u222522 + \u03bbR\u03b8 (G\u03c6(yj)). \u2022 \u03c6\u2190 Adam\u03b7,\u03b21,\u03b22(\u03c6,\u2207 J\u03032(\u03c6)), with \u03b7 = 10\u22124. 4. (Jointly trainR\u03b8 and G\u03c6 adversarially) for mini-batch k = 1, 2, \u00b7 \u00b7 \u00b7 , 25 Nnb , do: \u2022 Sample xj , yj , and j \u223c uniform [0, 1]; for 1 \u2264 j \u2264 nb. Compute uj = G\u03c6(yj) and x\n( ) j = jxj + (1\u2212 j)uj .\n\u2022 \u03b8 \u2190 Adam\u03b7,\u03b21,\u03b22(\u03b8,\u2207 J\u03031(\u03b8)), where J\u03031(\u03b8) is as in Step 2, with \u03b7 = 2\u00d7 10\u22125. \u2022 Update \u03c6\u2190 Adam\u03b7,\u03b21,\u03b22(\u03c6,\u2207 J\u03032(\u03c6)) twice, with J\u03032(\u03c6) as in Step 3, and \u03b7 = 2\u00d7 10\u22125.\n5. Output: The trained networks G\u03c6 andR\u03b8.\nessentially seeks to solve the min-max variational problem given by124\nmin \u03c6 max \u03b8:R\u03b8\u2208L1 E\u03c0 y\u03b4 \u2225\u2225A(G\u03c6(y\u03b4))\u2212 y\u03b4\u2225\u222522 + \u03bb(E\u03c0y\u03b4 [R\u03b8 (G\u03c6(y\u03b4))]\u2212 E\u03c0x [R\u03b8 (x)]) . (6) Thanks to KR duality in (3) and the definition of push-forward, (6) can be reformulated as125\nmin \u03c6 E\u03c0 y\u03b4 \u2225\u2225A(G\u03c6(y\u03b4))\u2212 y\u03b4\u2225\u222522 + \u03bbW1 ((G\u03c6)#\u03c0y\u03b4 , \u03c0x) . (7) We refer the reader to Section 3 for a mathematically rigorous statement of this equivalence as well126 as for a well-posedness theory of the problem in (7). Note that the equivalence of the alternating127 minimization procedure and the variational problem in (7) holds only if the regularizer is fully128 optimized in every iteration. Nevertheless, in practice, the reconstruction and regularizer networks129 are not fully optimized in every iteration. Instead, one refines the parameters by performing one (or a130 few) Adam updates on the corresponding loss functionals. Notably, if W1 ( (G\u03c6k)#\u03c0y\u03b4 , \u03c0x ) = 0, i.e.,131 the parameters of G are such that the reconstructed images match the ground-truth in distribution,132 the loss functional J (2)k (\u03b8) and its gradient vanish, leading to no further update of \u03b8. Thus, both133 networks stop updating when the outputs of G\u03c6 are indistinguishable from the ground-truth images.134 The concrete training steps are listed in Algorithm 1.135\n2.2.2 Iteratively unrolled reconstruction operator136\nThe objective of G\u03c6 is to approximate the minimizer of the variational loss withR\u03b8 as the regularizer.137 Therefore, an iterative unrolling strategy akin to [3] is adopted for parameterizing G\u03c6. Iterative138 unrolling seeks to mimic the variational minimizer via a primal-dual-style algorithm [6], with the139 proximal operators in the image and measurement spaces replaced with trainable CNNs. Although140 the variational loss in our case is non-convex, this parametrization for G\u03c6 is chosen because of its141 expressive power over a generic network. Initialized with x(0) = A\u2020 y\u03b4 and h(0) = 0, G\u03c6 produces142 a reconstruction x(L) by iteratively applying the CNNs \u039b\n\u03c6 (`) p and \u039b \u03c6 (`) d in X and Y, respectively:143\nh(`+1) = \u0393 \u03c6 (`) d\n( h(`), \u03c3(`)A(x`),y\u03b4 ) , andx(`+1) = \u039b\n\u03c6 (`) p\n( x(`), \u03c4 (`)A\u2217(h`+1) ) , 0 \u2264 ` \u2264 L\u22121.\nThe step-size parameters \u03c3(`) and \u03c4 (`) are also made learnable and initialized as \u03c3(`) = \u03c4 (`) = 0.01144 for each layer `. The number of layers L is typically much smaller (we take L = 20) than the number145 of iterations needed by an iterative primal-dual scheme to converge, thus expediting the reconstruction146 by two orders of magnitude once trained.147\nThe regularizerR\u03b8 is taken as a deep CNN with six convolutional layers, followed by one average-148 pooling and two dense layers in the end.149\n2.2.3 Variational regularization as a refinement step150\nThe unrolled operator G\u03c6\u2217 trained by solving the min-max problem in (6) provides reasonably good151 reconstruction when evaluated on X-ray CT, and already outperforms state-of-the-art unsupervised152 methods (c.f. Section 4). We demonstrate that the regularizer R\u03b8\u2217 obtained together with G\u03c6\u2217 by153 solving (6) can be used in the variational framework to further improve the quality of the end-to-end154 reconstruction G\u03c6\u2217(y\u03b4) for a given y\u03b4 \u2208 Y. Specifically, we solve the variational problem155\nmin x\u2208X \u2016A(x)\u2212 y\u03b4\u20162 + \u03bb\u2032\n( R\u03b8\u2217(x) + \u03c3\u2016x\u201622 ) , (8)\nwhere \u03bb\u2032, \u03c3 \u2265 0, by applying gradient descent, initialized with G\u03c6\u2217(y\u03b4). The additional Tikhonov156 term in (8) ensures coercivity of the overall regularizer, making it amenable to the standard well-157 posedness analysis [24]. Practically, it improves the stability of the gradient descent optimizer158 for (8). In practice, one essentially gets the same reconstruction with \u03c3 = 0 subject to early159 stopping (100 iterations). Notably, the fidelity term in (8) is the `2 distance, instead of the squared-`2160 fidelity. We have empirically observed that this choice of the fidelity term improves the quality161 of the reconstruction, possibly due to the higher gradient of the objective in the initial solution162 G\u03c6\u2217(y\u03b4). Since the end-to-end reconstruction gives an excellent initial point, it takes significantly163 fewer iterations for gradiet-descent to recover the optimal solution to (8), and therefore UAR retains164 its edge in reconstruction time over fully variational approaches with learned regularizers (e.g., AR165 [14] or its convex version [18]).166\n3 Theoretical results167\nThe theoretical properties of UAR are stated in this section and their proofs are provided in the168 supplementary document. Throughout this section, we assume that X = Rn and Y = Rk, and169\nA1. \u03c0x is compactly supported and \u03c0y\u03b4 is supported on a compact set K \u2282 Rk for every \u03b4 \u2265 0.170\nWe then consider the following problem:171\ninf \u03c6 sup R\u2208L1\nJ1 ( G\u03c6,R|\u03bb, \u03c0y\u03b4 ) := E\u03c0\ny\u03b4 \u2225\u2225y\u03b4 \u2212AG\u03c6(y\u03b4)\u2225\u222522 + \u03bb (E\u03c0y\u03b4 [R(G\u03c6(y\u03b4))]\u2212 E\u03c0x [R(x)]) . (9)\nProblem (9) is identical to the min-max variational problem defined in (6), with the only difference172 that the maximization in R is performed over the space of all 1-Lipschitz functions. Basically,173 we consider the theoretical limiting case where the neural networks R\u03b8 are expressive enough to174 approximate all functions in L1 with arbitrary accuracy. We make the following assumptions on G\u03c6:175\nA2. G\u03c6 is parametrized over a finite dimensional compact set K, i.e. \u03c6 \u2208 K.176 A3. G\u03c6n \u2192 G\u03c6 pointwise whenever \u03c6n \u2192 \u03c6.177 A4. sup\u03c6\u2208K \u2016G\u03c6\u2016\u221e <\u221e.178\nAssumptions A2-A4 are satisfied, for instance, when G\u03c6 is parametrized by a neural network whose179 weights are kept bounded during training. These assumptions apply to all results in this section.180\n3.1 Well-posedness of the adversarial loss181\nHere, we prove well-posedness and stability to noise for the optimal reconstructions. As a conse-182 quence of the KR duality, (9) can be equivalently expressed as183\ninf \u03c6 J2 ( G\u03c6|\u03bb, \u03c0y\u03b4 ) := E\u03c0 y\u03b4 \u2225\u2225y\u03b4 \u2212AG\u03c6(y\u03b4)\u2225\u222522 + \u03bbW1(\u03c0x, (G\u03c6)#\u03c0y\u03b4) . (10) In the next theorem, we prove this equivalence, showing the existence of an optimal G\u03c6 andR for (9).184\nTheorem 1. Problems (9) and (10) admit an optimal solution and185 inf \u03c6\nsup R\u2208L1\nJ1 ( G\u03c6,R|\u03bb, \u03c0y\u03b4 ) = inf \u03c6 J2 ( G\u03c6|\u03bb, \u03c0y\u03b4 ) . (11)\nMoreover, if (G\u03c6\u2217 ,R\u2217) is optimal for (9), then G\u03c6\u2217 is optimal for (10). Conversely, if G\u03c6\u2217 is optimal for186 (10), then (G\u03c6\u2217 ,R\u2217) is optimal for (9), for allR\u2217 \u2208 arg maxR\u2208L1 E\u03c0y\u03b4 [ R(G\u03c6\u2217(y\u03b4)) ] \u2212E\u03c0x [R(x)].187\nNext, we study the stability of the optimal reconstruction G\u03c6\u2217 to noise. We consider G\u03c6n , where188 \u03c6n \u2208 arg inf \u03c6 J2 ( G\u03c6|\u03bb, \u03c0y\u03b4n ) , (12)\nand show that G\u03c6n \u2192 G\u03c6\u2217 as \u03b4n \u2192 \u03b4, thus establishing noise-stability of the unrolled reconstruction.189 Theorem 2 (Stability to noise). Suppose, for given a sequence of noise levels \u03b4n \u2192 \u03b4 \u2208 [0,\u221e), it190 holds that \u03c0y\u03b4n \u2192 \u03c0y\u03b4 in total variation. Then, with \u03c6n as in (12), G\u03c6n \u2192 G\u03c6\u2217 up to sub-sequences.191\n3.2 Effect of \u03bb on the end-to-end reconstruction192\nIn order to analyze the effect of the parameter \u03bb in (10) on the resulting reconstruction G\u03c6\u2217 , it is193 convenient to introduce the following two sets:194\n\u03a6L := { \u03c6 : E\u03c0\ny\u03b4 \u2225\u2225y\u03b4 \u2212AG\u03c6(y\u03b4)\u2225\u222522 = 0} and \u03a6W := {\u03c6 : (G\u03c6)#\u03c0y\u03b4 = \u03c0x} . We assume that both \u03a6L and \u03a6W are non-empty, which is tantamount to asking that the parametriza-195 tion of the end-to-end reconstruction operator is expressive enough to approximate a right inverse196 of A (\u03a6L 6= \u2205) and a transport map from \u03c0y\u03b4 to \u03c0x (\u03a6W 6= \u2205), and therefore is not very restrictive197 (keeping in view the enormous approximation power of unrolled deep architectures).198 Proposition 1. Let G\u03c6\u2217 be a minimizer for (10). Then, it holds that199\n\u2022 E\u03c0y \u2225\u2225y\u03b4 \u2212AG\u03c6\u2217(y\u03b4)\u2225\u222522 \u2264 \u03bbW1(\u03c0x, (G\u03c6)#\u03c0y\u03b4), for every \u03c6 \u2208 \u03a6L.200\n\u2022 W1(\u03c0x, (G\u03c6\u2217)#\u03c0y\u03b4) \u2264 1\n\u03bb E\u03c0 y\u03b4 \u2225\u2225y\u03b4 \u2212AG\u03c6(y\u03b4)\u2225\u222522 , for every \u03c6 \u2208 \u03a6W.201 The previous proposition shows in a quantitative way that for small \u03bb, the optimal G\u03c6\u2217 has less202 expected distortion in the measurement space as the quantity E\u03c0\ny\u03b4 \u2225\u2225y\u03b4 \u2212AG\u03c6\u2217(y\u03b4)\u2225\u222522 is small.203 On the other hand, if \u03bb is large, then the optimal G\u03c6\u2217 maps \u03c0y\u03b4 is closer to \u03c0x as the quantity204 W(\u03c0x, (G\u03c6\u2217)#\u03c0y\u03b4) is small. Therefore, the regularization is stronger in this case.205 We extend this analysis by studying the behavior of the unrolled reconstruction as \u03bb converges to 0206 and to +\u221e. Consider a sequence of parameters \u03bbn > 0 and the minimizer of the objective in (10)207 with parameter \u03bbn:208\n\u03c6\u2032n \u2208 arg inf \u03c6\nJ2 ( G\u03c6|\u03bbn, \u03c0y\u03b4 ) . (13)\nTheorem 3. Suppose that \u03bbn \u2192 0 and \u03c6\u22171 \u2208 arg min \u03c6\u2208\u03a6L W1(\u03c0x, (G\u03c6)#\u03c0y\u03b4). Then, G\u03c6\u2032n \u2192 G\u03c6\u22171 up to209\nsub-sequences, and lim n\u2192\u221e inf \u03c6 J2 ( G\u03c6|\u03bbn, \u03c0y\u03b4 ) = W1(\u03c0x, (G\u03c6\u22171 )#\u03c0y\u03b4).210\nTheorem 4. Let \u03bbn \u2192 +\u221e and \u03c6\u22172 \u2208 arg min \u03c6\u2208\u03a6W E\u03c0 y\u03b4 \u2225\u2225y\u03b4 \u2212AG\u03c6(y\u03b4)\u2225\u222522. Then, G\u03c6\u2032n \u2192 G\u03c6\u22172 up to211 sub-sequences, and lim n\u2192\u221e inf \u03c6 J2 ( G\u03c6|\u03bbn, \u03c0y\u03b4 ) = E\u03c0 y\u03b4\n\u2225\u2225y\u03b4 \u2212AG\u03c6\u22172 (y\u03b4)\u2225\u222522.212 Theorems 3 and 4 characterize the optimal end-to-end reconstruction G\u03c6\u2217 as \u03bb \u2192 0 and \u03bb \u2192 \u221e,213 respectively. Specifically, if \u03bb\u2192 0, G\u03c6\u2217 minimizes the Wasserstein distance between reconstruction214 and ground-truth among all the reconstruction operators that achieve zero expected data-distortion. In215 particular, G\u03c6\u2217 is close to the right inverse of A that minimizes the Wasserstein distance. Therefore,216 when \u03bb is very small, we expect to obtain a reconstruction that is close to the unregularized solution217 in quality. If \u03bb \u2192 \u221e on the other hand, the operator G\u03c6\u2217 is close to a transport map between218 \u03c0x and \u03c0y\u03b4 , i.e., (G\u03c6\u2217)#\u03c0y\u03b4 = \u03c0x, which minimizes the expected data-distortion. Therefore the219 reconstruction produces realistic images, but they are not consistent with the measurement. These220 theoretical observations are corroborated by the numerical results (c.f. Section 4, Fig. 2). One has to221 thus select a \u03bb that optimally trades-off data-distortion with the Wasserstein distance to achieve the222 best reconstruction performance.223\n3.3 End-to-end reconstruction vis-\u00e0-vis the variational solution224\nThe goal of this section is two-fold. Firstly, we theoretically justify the fact that the end-to-end recon-225 struction performs well, despite minimizing the expected loss over the distribution \u03c0y\u03b4 . Secondly, we226 analyze the role of the regularizer in the variational setting in refining the end-to-end reconstruction.227\nIt is important to remark that the the end-to-end reconstruction is trained in on the expected variational228 loss computed using samples from \u03c0y\u03b4 and \u03c0x. Therefore, the end-to-end reconstruction cannot learn229 a point-wise correspondence between measurement and model parameter, but only a distributional230 correspondence. Despite that, the end-to-end reconstruction achieves excellent performance for a231 given measurement vector y. A justification of such phenomena is given by the next proposition.232 Proposition 2. Let (G\u03c6\u2217 ,R\u2217) be an optimal pair for (9) such thatR\u2217 \u2265 0 almost everywhere under233 (G\u03c6)# \u03c0y\u03b4 . Define M1 := E\u03c0y\u03b4\n\u2225\u2225y\u03b4 \u2212AG\u03c6\u2217(y\u03b4)\u2225\u222522 and M2 := W1(\u03c0x, (G\u03c6\u2217)#\u03c0y\u03b4). Then, the234 following two upper bounds hold for every \u03b7 > 0:235\n\u2022 P\u03c0 y\u03b4\n{ y\u03b4 : \u2225\u2225y\u03b4 \u2212AG\u03c6\u2217(y\u03b4)\u2225\u222522 \u2265 \u03b7} \u2264 M1\u03b7 .236 \u2022 Suppose,R\u2217(x) = 0 for \u03c0x-almost every x. Then, P(G\u03c6\u2217 )#\u03c0y\u03b4 { x : R\u2217(x) \u2265 \u03b7 } \u2264 M2\u03b7 .237\nProposition 2 provides an estimate in probability of the sets {y\u03b4 : \u2225\u2225y\u03b4 \u2212AG\u03c6\u2217(y\u03b4)\u2225\u222522 \u2265 \u03b7} and238\n{x : R\u2217(x) \u2265 \u03b7}. In particular, if M1 is small, then \u2225\u2225y\u03b4 \u2212AG\u03c6\u2217(y\u03b4)\u2225\u222522 is small in probability. If239 instead M2 is small, thenR\u2217(x) is small in probability on the support of (G\u03c6\u2217)#\u03c0y\u03b4 , implying that240 samples G\u03c6\u2217(y\u03b4) are difficult to distinguish from the ground-truth. We remark that the assumption241 R\u2217(x) = 0 can be justified using a data manifold assumption as in Section 3.3. of [14]. We242 now analyze the role of the regularizer R\u2217 in the optimization of the variational problem (8) that243 refines the end-to-end reconstruction G\u03c6\u2217 . We rely on a similar distributional analysis as the one244 performed in [14]. For \u03b7 > 0, consider the transformation by a gradient-descent step onR\u2217 given by245 g\u03b7(x) = x\u2212 \u03b7\u2207R\u2217(x). Using the shorthand \u03c0G\u2217 := (G\u03c6\u2217)#\u03c0y\u03b4 , and by denoting the distribution246 of g\u03b7(x) as \u03c0\u03b7 := (g\u03b7)#\u03c0G\u2217 for x \u223c \u03c0G\u2217 , we have the following theorem.247 Theorem 5 ([14]). Suppose that \u03b7 \u2192W1(\u03c0\u03b7, \u03c0x) is differentiable at \u03b7 = 0. Then, the derivative at248 \u03b7 = 0 satisfies dd\u03b7W1(\u03c0\u03b7, \u03c0x) \u2223\u2223\u2223 \u03b7=0 = \u2212E\u03c0G\u2217 \u2016\u2207R\u2217(x)\u201622.249\nThis theorem states that a gradient-descent step performed on R\u2217 at x = G\u03c6\u2217(y\u03b4) decreases the250 Wasserstein distance with respect to the ground-truth distribution \u03c0x. Therefore, if the gradient-251 descent step to solve the variational problem (8) is initialized with the reconstruction G\u03c6\u2217(y\u03b4), the252 next iterate gets pushed closer to the ground-truth distribution \u03c0x. We stress that this property holds253 because of the chosen initialization point, due to the relation between R\u2217 and G\u03c6\u2217 . For a different254 initialization, this property may not hold.255\n4 Numerical results256\nOn the application front, we consider the prototypical inverse problem of CT reconstruction from257 noisy sparse-view projections. The abdominal CT scans for 10 patients, made publicly available by258 the Mayo-Clinic for the low-dose CT grand challenge [15], were used in our numerical experiments.259 Specifically, 2250 2D slices of size 512 \u00d7 512 corresponding to 9 patients were used to train the260 models, while 128 slices from the remaining one patient were used for evaluation. The projections261 were simulated in ODL [1] using a parallel-beam geometry with 200 uniformly spaced angular262 positions of the source, with 400 lines per angle. Subsequently, Gaussian noise with standard263 deviation \u03c3e = 2.0 was added to the projection data to simulate noisy sinograms.264\nThe proposed UAR method is compared with two classical model-based approaches for CT, namely265 filtered back-projection (FBP) and total variation (TV). The LPD method [3] and U-net-based post-266 processing [10] of FBP are chosen as two supervised approaches for comparison. The AR approach267 [14] and its convex variant [18], referred to as adversarial convex regularizer (ACR) , are taken as the268 competing unsupervised approaches. For LPD and AR, we develop a PyTorch-based implementation269 based on their publicly available TensorFlow codes1 2, while for ACR, we use the publicly available270 PyTorch implementation3.271\nThe unrolled network G\u03c6 has 20 layers, with 5\u00d7 5 filters in both primal and dual spaces to increase272 the overall receptive field for sparse-view measurements. The hyper-parameters involved in training273 the UAR are specified in Algorithm 1. We found that first training a baseline regularizer and a274 corresponding baseline reconstruction operator helps stabilize the training process. Training the UAR275 model took approximately 30 hours on an NVIDIA Quadro RTX 6000 GPU (24 GB of memory).276\nThe average performance on the test images in terms of PSNR and SSIM [28] indicates that UAR277 (with \u03bb = 0.1) outperforms AR and ACR by 0.3 dB and 2.6 dB, approximately. We would like278 to emphasize that this gain was found to be consistent across all test images and not just realized279 on average. With the refinement step, UAR surpasses AR by almost 0.7 dB and becomes on par280 with U-net post-processing. The end-to-end UAR reconstruction is a couple of orders of magnitude281 faster than AR, while the reduction in reconstruction time is by a factor of 4 with the refinement.282 The reconstructions of a representative test image using the competing methods are shown in Fig.283 1 for a visual comparison. The effect of \u03bb on the reconstruction of UAR is demonstrated in Fig. 2,284 which confirms the theoretical results in Section 3.2. The refinement step also visibly improves the285 reconstruction quality of the end-to-end operator, as shown in Fig. 3.286\n5 Conclusions and limitations287\nTo the best of our knowledge, this work makes the first attempt to blend end-to-end reconstruction288 with data-driven regularization via an adversarial learning framework. Our UAR approach retains289 the fast reconstruction of the former together with provable guarantees of the latter. We rigorously290 analyze the proposed framework in terms of well-posedness, noise-stability, and the effect of the291 regularization penalty, and establish a link between the trained reconstruction operator and the292 corresponding variational objective. We show strong numerical evidence of the efficacy of the293 UAR approach for CT reconstruction, wherein it achieves the same performance as supervised294 data-driven post-processing and outperforms competing unsupervised techniques. Our work paves295 the way to better understand the role of adversarially learned regularizers in solving ill-posed inverse296 problems, although several important aspects need further investigation. Since the learned regularizer297 is non-convex, the performance of gradient-descent on the variational objective greatly depends on298 initialization. This problem is partly addressed by the unrolled reconstruction operator that efficiently299 computes a better initial point for gradient descent. However, the precise relationship between the300 end-to-end reconstruction and the variational minimizer for a given measurement vector remains301 elusive. Moreover, the quality of the reconstruction relies on the expressive power of neural networks302 and thus suffers from the curse of dimensionality. We believe that addressing such limitations will be303 important to better understand adversarial regularization methods.304\n1LPD: https://github.com/adler-j/learned_primal_dual. 2AR: https://github.com/lunz-s/DeepAdverserialRegulariser. 3ACR: https://github.com/Subhadip-1/data_driven_convex_regularization.\nReferences305 [1] J. Adler, H. Kohr, and O. \u00d6ktem. Operator discretization library (odl). Software available from306 https://github.com/odlgroup/odl, 2017.307\n[2] Jonas Adler and Ozan \u00d6ktem. Solving ill-posed inverse problems using iterative deep neural308 networks. Inverse Problems, 33(12), 2009.309\n[3] Jonas Adler and Ozan \u00d6ktem. Learned primal-dual reconstruction. IEEE transactions on310 medical imaging, 37(6):1322\u20131332, 2018.311\n[4] Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial312 networks. In Proceedings of the 34th International Conference on Machine Learning, pages313 214\u2013223, 2017.314\n[5] Simon Arridge, Peter Maass, Ozan \u00d6ktem, and Carola-Bibiane Sch\u00f6nlieb. Solving inverse315 problems using data-driven models. Acta Numerica, 28:1\u2013174, 2019.316\n[6] A. Chambolle and T. Pock. A first-order primal-dual algorithm for convex problems with317 applications to imaging. J. Math. Imaging and Vision, 40(1):120\u2013145, 2010.318\n[7] Stanley H Chan, Xiran Wang, and Omar A Elgendy. Plug-and-play admm for image restoration:319 Fixed-point convergence and applications. IEEE Transactions on Computational Imaging, 3(1):320 84\u201398, 2016.321\n[8] M. Cuturi and G. Peyr\u00e9. Computational Optimal Transport. Arxiv preprint arXiv:1803.00567,322 2019. https://arxiv.org/pdf/1803.00567.pdf.323\n[9] K. Gregor and Y. LeCun. Learning fast approximations of sparse coding. In Intl. Conf. on324 Machine Learning, 2010.325\n[10] K. H. Jin, M. T. McCann, E. Froustey, and M. Unser. Deep convolutional neural network for326 inverse problems in imaging. IEEE Transactions on Image Processing, 26(9):4509\u20134522, 2017.327\n[11] Erich Kobler, Teresa Klatzer, Kerstin Hammernik, and Thomas Pock. Variational networks:328 connecting variational methods and deep learning. In German conference on pattern recognition,329 pages 281\u2013293. Springer, 2017.330\n[12] Erich Kobler, Alexander Effland, Karl Kunisch, and Thomas Pock. Total deep variation for331 linear inverse problems. In Proceedings of the IEEE Conference on Computer Vision and332 Pattern Recognition, pages 7549\u20137558, 2020.333\n[13] Housen Li, Johannes Schwab, Stephan Antholzer, and Markus Haltmeier. NETT: solving334 inverse problems with deep neural networks. Inverse Problems, 36(6), 2020.335\n[14] Sebastian Lunz, Ozan \u00d6ktem, and Carola-Bibiane Sch\u00f6nlieb. Adversarial regularizers in inverse336 problems. In Advances in Neural Information Processing Systems, pages 8507\u20138516, 2018.337\n[15] C. McCollough. Tfg-207a-04: Overview of the low dose ct grand challenge. Medical Physics,338 43(6):3759\u20133760, 2014.339\n[16] Tim Meinhardt, Michael Moller, Caner Hazirbas, and Daniel Cremers. Learning proximal340 operators: Using denoising networks for regularizing inverse imaging problems. In Proceedings341 of the IEEE International Conference on Computer Vision, pages 1781\u20131790, 2017.342\n[17] V. Monga, Y. Li, and Y. Eldar. Algorithm unrolling: Interpretable, efficient deep learning for343 signal and image processing. arXiv preprint arXiv:1912.10557v3, 2019.344\n[18] S. Mukherjee, S. Dittmer, Z. Shumaylov, S. Lunz, O. \u00d6ktem, and C.-B. Sch\u00f6nlieb. Learned345 convex regularizers for inverse problems. arXiv preprint arXiv:2008.02839v2, 2021.346\n[19] Changheun Oh, Dongchan Kim, Jun-Young Chung, Yeji Han, and H. Park. Eter-net: End to end347 mr image reconstruction using recurrent neural network. In MLMIR@MICCAI, 2018.348\n[20] J.-C. Pesquet, A. Repetti, M. Terris, and Y. Wiaux. Learning maximally monotone operators for349 image recovery. arXiv preprint arXiv:2012.13247v2, Apr. 2021.350\n[21] E. T. Reehorst and P. Schniter. Regularization by denoising: clarifications and new interpreta-351 tions. IEEE Transactions on Computational Imaging, 5(1):52\u201367, 2019.352\n[22] Yaniv Romano, Michael Elad, and Peyman Milanfar. The little engine that could: Regularization353 by denoising (red). SIAM Journal on Imaging Sciences, 10(4):1804\u20131844, 2017.354\n[23] F. Santambrogio. Optimal Transport for Applied Mathematicians. Birkh\u00e4user Basel, 2015.355\n[24] Otmar Scherzer, Markus Grasmair, Harald Grossauer, Markus Haltmeier, and Frank Lenzen.356 Variational methods in imaging. Springer, 2009.357\n[25] Y. Sun, B. Wohlberg, and Kamilov U. S. An online plug-and-play algorithm for regularized358 image reconstruction. IEEE Transactions on Computational Imaging, 5(3):395\u2013408, 2019.359\n[26] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Deep image prior. In Proceedings of360 the IEEE Conference on Computer Vision and Pattern Recognition, pages 9446\u20139454, 2018.361\n[27] C. Villani. Optimal transport \u2014 Old and new, volume 338 of Grundlehren der mathematischen362 Wissenschaften. Springer Berlin Heidelberg, 2009. doi: 10.1007/978-3-540-71050-9.363\n[28] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: From364 error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600\u2013612,365 2004.366\n[29] Y. Yang, J. Sun, H. Li, and Z. Xu. Deep admm-net for compressive sensing mri. In Advances in367 Neural Information Processing Systems, 2016.368\n[30] B. Zhu, J. Z. Liu, S. F. Cauley, B. R. Rosen, and M. S. Rosen. Image reconstruction by369 domain-transform manifold learning. Nature, 555:487\u2013492, 2018.370\nChecklist371\n1. For all authors...372\n(a) Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s373 contributions and scope? [Yes]374\n(b) Did you describe the limitations of your work? [Yes] See Section 5.375 (c) Did you discuss any potential negative societal impacts of your work? [N/A]376 (d) Have you read the ethics review guidelines and ensured that your paper conforms to377 them? [Yes]378\n2. If you are including theoretical results...379\n(a) Did you state the full set of assumptions of all theoretical results? [Yes] Please refer to380 the beginning of Section 3.381\n(b) Did you include complete proofs of all theoretical results? [Yes] Complete proofs are382 included in the supplementary material. In Section 3 of the main paper, we discuss the383 relationship between our results and existing results in the literature.384\n3. If you ran experiments...385\n(a) Did you include the code, data, and instructions needed to reproduce the main ex-386 perimental results (either in the supplemental material or as a URL)? [Yes] Codes387 are included in the supplementary material along with instructions to reproduce the388 experimental results.389\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they390 were chosen)? [Yes] Please refer to Section 4 and Algorithm 1.391\n(c) Did you report error bars (e.g., with respect to the random seed after running experi-392 ments multiple times)? [Yes] Please refer to Section 4 and Table 1.393\n(d) Did you include the total amount of compute and the type of resources used (e.g., type394 of GPUs, internal cluster, or cloud provider)? [Yes] See Section 4.395\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...396\n(a) If your work uses existing assets, did you cite the creators? [Yes] Please refer to Section397 4 for references to the dataset and the codes used in this work.398\n(b) Did you mention the license of the assets? [Yes] The dataset was made public by399 Mayo-Clinic [15].400\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]401 Our code is included in the supplemental material.402\n(d) Did you discuss whether and how consent was obtained from people whose data you\u2019re403 using/curating? [Yes] The dataset and the codes used in this work are available publicly.404\n(e) Did you discuss whether the data you are using/curating contains personally identifiable405 information or offensive content? [Yes] In Section 4, we mention that the data used in406 this work are publicly available [15]. For the ease of running our scripts, we convert407 the raw data to numpy format, which does not contain any sensitive information, and408 make them available along with the supplementary material.409\n5. If you used crowdsourcing or conducted research with human subjects...410 (a) Did you include the full text of instructions given to participants and screenshots, if411 applicable? [N/A]412 (b) Did you describe any potential participant risks, with links to Institutional Review413 Board (IRB) approvals, if applicable? [N/A]414 (c) Did you include the estimated hourly wage paid to participants and the total amount415 spent on participant compensation? [N/A]416"
      }
    ],
    "year": 2021,
    "abstractText": "We propose an unsupervised approach for learning end-to-end reconstruction oper1 ators for ill-posed inverse problems. The proposed method combines the classical 2 variational framework with iterative unrolling, which essentially seeks to mini3 mize a weighted combination of the expected distortion in the measurement space 4 and the Wasserstein-1 distance between the distributions of the reconstruction 5 and ground-truth. More specifically, the regularizer in the variational setting is 6 parametrized by a deep neural network and learned simultaneously with the un7 rolled reconstruction operator. The variational problem is then initialized with 8 the reconstruction of the unrolled operator and solved iteratively till convergence. 9 Notably, it takes significantly fewer iterations to converge, thanks to the excellent 10 initialization obtained via the unrolled operator. The resulting approach combines 11 the computational efficiency of end-to-end unrolled reconstruction with the well12 posedness and noise-stability guarantees of the variational setting. Moreover, we 13 demonstrate with the example of X-ray computed tomography (CT) that our ap14 proach outperforms state-of-the-art unsupervised methods, and that it outperforms 15 or is on par with state-of-the-art supervised learned reconstruction approaches. 16",
    "creator": "LaTeX with hyperref"
  },
  "output": [
    [
      "1. \"First there are two well written past works that overlap too much with this paper. One that has been referenced here (see ref 12) by Erich Kobler et. al. at CVPR 2020 where they cover the variational approach in detail and it's application to CT.\"",
      "2. \"The second is the paper Sim, Byeongsu, et al. 'Optimal transport driven CycleGAN for unsupervised learning in inverse problems.' SIAM Journal on Imaging Sciences 13.4 (2020): 2281-2306. The above besides addressing optimal transport approaches for inverse problems, also addresses the adversarial training aspect and further looks at various applications, including CT.\"",
      "3. \"Second, only CT image reconstruction is demonstrated. probably it works with other modalities as well, but it hasn't been demonstrated in the main body of the paper.\""
    ],
    [
      "1. \"Supervised vs. unsupervised: While it is true that pairs of (image, measurement) data are not strictly required, the method does require ground-truth reference images for training. Therefore I believe it is misleading to consider this unsupervised.\"",
      "2. \"End-to-end vs variational reconstruction: The authors claim that a big benefit of their approach is the ability to use the end-to-end reconstruction as an initialization in a variational framework, where the regularizer previously learned can be used. While this is interesting, there are some comparative details missing. For example, how would this compare if I used some other end-to-end reconstruction (for example, the one in Ref [3]) with a regularizer that was separately trained in an adversarial fashion (for example, the one in Ref [14]). To what extent is it critical that they be coupled?\"",
      "3. \"In addition, based on Table 1, the number of iterations is reduced by a factor of about 4, while still slower than end-to-end alone by a factor of over 20. Therefore, it feels misleading to say that 'the refinement step involves running a few iterations'. How would the variational reconstruction perform (speed and quality) if initialized with a different point, e.g., the pseudo-inverse, or a different end-to-end reconstruction such as the simple U-Net?\"",
      "4. \"Experiments. The experiments are limited to a single dataset consisting of 9 patients used for training, one patient used for testing (it is unclear if Supporting Fig 2 is a different patient or a different slice of the same patient), and no explanation of validation data (was the test set actively used for validation?). To me this is very limiting in terms of empirical evidence. Since the paper title is quite general, it would be interesting to see performance for other inverse problems.\"",
      "5. \"I also would like to authors to clarify why they added Gaussian noise to the measurements. Based on the webpage for the low-dose CT challenge, I understand that the challenge organizers added Poisson noise to the projection data.\""
    ],
    [
      "1. \"the experiment seems inadequate with only one experiment on CT.\"",
      "2. \"the justification for the usage of unfolding networks as the 'generator' is somewhat missing.\""
    ],
    [
      "1. \"In its current form, UAR seems to share a lot of similarities with Wasserstein GANs, and thus parallels can be drawn with other works which have used similar techniques for image reconstruction. For example, there is some work on using a similar technique to UAR on MR images (Lei et al., 2020). Even though I think comparison with such works should be included, I think the originality of this paper is not significantly hampered, since the authors also include theoretical statements and proofs on the properties of UAR.\"",
      "2. \"I find it particularly interesting that that UAR is 2 orders of magnitude faster than AR, even though intuitively it would make sense to me that AR should converge faster (since the regularizer and the reconstruction are trained separately). I believe that this result merits further discussion in the experimental section.\"",
      "3. \"In Algorithm 1, it seems that the batch size is set as equal to 1. I am not sure if this is correct or a typo, because it seems quite a weird choice to make for batch size.\"",
      "4. \"Again in Algorithm 1, the way for loops are written implies looping over both mini-batches and epochs. I think it would be clearer if the epochs were written separately.\"",
      "5. \"Section 2.2.2 is used to define the reconstruction network G\u03d5, but I think the way it is placed breaks the flow of the paper a little, because Section 2.2.3 refers to a refinement step, which I believe would be better to mention directly after the original training process in Section 2.2.1. I would suggest either moving Section 2.2.2, or including its contents in the previous one.\"",
      "6. \"There is a small typo in line 143, for the definition of the reconstruction network.\"",
      "7. \"Overall, I believe this is a good and interesting submission. I have some slight reservations about the originality of the method proposed, but I think these are overcome by the theoretical analysis provided.\""
    ],
    [
      "1. \"the methodical contributions are limited.\"",
      "2. \"I found it hard to follow at some specific sections due to the insufficient details on the notations.\"",
      "3. \"the definition and meaning of \u03b4 in \u03c0y\u03b4 that is first presented in line 118 is not clearly stated.\"",
      "4. \"it would be better to introduce the 1-lipschitz constrained loss in Algorithm 1 with more details.\"",
      "5. \"some typos exist such as missing k for \u03d5k in Eq. 4.\""
    ]
  ],
  "review_num": 5,
  "item_num": [
    3,
    5,
    2,
    7,
    5
  ]
}