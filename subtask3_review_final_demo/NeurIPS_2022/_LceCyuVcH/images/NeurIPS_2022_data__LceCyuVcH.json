[{
  "caption": "Table 1: Statistics of datasets in our experiments",
  "captionBoundary": {
    "x1": 308.30902099609375,
    "x2": 501.3841857910156,
    "y1": 110.7265396118164,
    "y2": 116.72900390625
  },
  "figType": "Table",
  "imageText": ["MSR-VTT", "[60]", "Captioning;", "QA", "6,513", "/", "2,990", "MSR-VTT", "[60]", "Retrieval", "7,010", "/", "1,000", "MSVD", "[7]", "Question", "Answering", "30,933", "/", "13,157", "VaTeX", "v1.13", "[55]", "Captioning;", "Retrieval", "25,991", "/", "6,000", "YouCook2", "[72]", "Captioning", "10,337", "/", "3,492", "VLEP", "[23]", "Event", "Prediction", "20,142", "/", "4,192", "Dataset", "Task", "Split", "Count", "#", "train", "/", "#", "eval"],
  "name": "1",
  "page": 5,
  "regionBoundary": {
    "x1": 305.76,
    "x2": 505.44,
    "y1": 126.72,
    "y2": 208.32
  },
  "renderDpi": 150,
  "renderURL": "NeurIPS_2022_pic__LceCyuVcH-Table1-1.png"
}, {
  "caption": "Table 2: 10-shot video captioning results. B-4, R-L, M, C represents BLEU-4, ROUGE-L, METEOR and CIDEr. Avg C represents the average CIDEr score across all available benchmarks. ASR indicates whether the model has access to the ASR subtitles. BLIP and BLIPcap use the pretrained checkpoint and the finetuned checkpoint on COCO captioning. All results are averaged over three random seeds.",
  "captionBoundary": {
    "x1": 107.64099884033203,
    "x2": 504.0007019042969,
    "y1": 495.79052734375,
    "y2": 534.52001953125
  },
  "figType": "Table",
  "imageText": ["4https://huggingface.co/openai/clip-vit-large-patch14", "5https://github.com/salesforce/BLIP#finetuned-checkpoints", "6https://docs.allennlp.org/models/main/models/structured_prediction/predictors/srl/", "UniVL", "No", "42.0", "61.0", "29.0", "50.1", "11.2", "40.1", "17.6", "127.0", "22.8", "38.6", "22.3", "33.4", "70.2", "UniVL", "Yes", "-", "-", "-", "-", "16.6", "45.7", "21.6", "176.8", "23.7", "39.3", "22.7", "35.6", "106.2", "UniVL", "No", "2.1", "22.5", "9.5", "3.6", "3.3", "25.3", "11.6", "34.1", "1.7", "15.7", "8.0", "2.1", "13.3", "BLIP", "No", "27.7", "43.0", "23.0", "39.5", "0.7", "9.0", "3.4", "11.5", "13.5", "39.5", "15.4", "20.7", "23.9", "BLIPcap", "No", "21.6", "48.0", "22.7", "30.2", "3.7", "8.6", "3.8", "9.4", "20.7", "41.5", "17.4", "28.9", "22.8", "VidIL(ours)", "No", "26.0", "51.7", "24.7", "36.3", "2.6", "22.9", "9.5", "27.0", "22.2", "43.6", "20.0", "36.7", "33.3", "UniVL", "Yes", "-", "-", "-", "-", "4.3", "26.4", "12.2", "48.6", "2.7", "17.7", "10.2", "3.4", "26.0", "VidIL(ours)", "Yes", "-", "-", "-", "-", "10.7", "35.9", "19.4", "111.6", "23.2", "44.2", "20.6", "38.9", "75.3", "Fine-tuning", "Method", "ASR", "MSR-VTT", "Caption", "YouCook2", "Caption", "VaTex", "Caption", "Avg", "CB-4", "R-L", "M", "C", "B-4", "R-L", "M", "C", "B-4", "R-L", "M", "C", "Few-shot"],
  "name": "2",
  "page": 5,
  "regionBoundary": {
    "x1": 107.52,
    "x2": 510.24,
    "y1": 544.3199999999999,
    "y2": 721.92
  },
  "renderDpi": 150,
  "renderURL": "NeurIPS_2022_pic__LceCyuVcH-Table2-1.png"
}, {
  "caption": "Figure 1: Multiple levels of information in videos.",
  "captionBoundary": {
    "x1": 306.0,
    "x2": 503.9950866699219,
    "y1": 336.83251953125,
    "y2": 342.8349914550781
  },
  "figType": "Figure",
  "imageText": ["Videos", "contain", "rich", "semantics", "and", "temporal", "con-45", "tent", "at", "multiple", "granularities.", "Similar", "to", "static46", "images,", "videos", "depict", "objects,", "attributes,", "and47", "events.", "However,", "the", "sequence", "of", "frames", "fur-48", "ther", "conveys", "object", "state", "changes,", "actions,", "and49", "events.", "For", "example,", "in", "Figure", "1,", "the", "frame", "cap-50", "tions", "describe", "static", "visual", "features", "such", "as", "“a51", "person", "holding", "a", "green", "object", "in", "hand”", "for", "the52", "first", "frame.", "In", "contrast,", "the", "video", "clip", "could", "be53", "correctly", "captioned", "as", "\"a", "woman", "makes", "realis-54", "tic", "looking", "leaves", "and", "flowers", "for", "a", "cake\",", "or55", "represented", "as", "a", "collection", "of", "the", "objects", "and56", "events", "that", "occur", "at", "different", "timestamps", "in", "the57", "video", "clip,", "such", "as", "cutting", "mat", "and", "flowered", "de-58", "sign.", "Hence,", "to", "inform", "video-level", "description59", "and", "queries,", "we", "need", "to", "represent", "all", "of", "this", "in-60", "formation", "and", "its", "temporal", "ordering.61", "44"],
  "name": "1",
  "page": 1,
  "regionBoundary": {
    "x1": 91.2,
    "x2": 298.08,
    "y1": 154.07999999999998,
    "y2": 351.36
  },
  "renderDpi": 150,
  "renderURL": "NeurIPS_2022_pic__LceCyuVcH-Figure1-1.png"
}, {
  "caption": "Figure 4: Qualitative examples on video captioning. Grey boxes contains part of the video representation from our model. Blue boxes contains caption generation from different models. Green boxes contains ground truth annotations. Bold green text highlights the correct information that is not captured in baseline outputs which can be reasoned from our visual tokens and frame captions.",
  "captionBoundary": {
    "x1": 108.0,
    "x2": 505.6539306640625,
    "y1": 436.5775146484375,
    "y2": 475.3080139160156
  },
  "figType": "Figure",
  "imageText": ["commentator", "and", "add", "some", "gravy", "to", "the", "plate", "front", "of", "them", "Ours:", "A", "person", "is", "making", "a", "sign", "that", "says", "\"Drink", "Up\"", "with", "a", "wood", "burning", "kit.", "remove", "sausages", "from", "pan", "Ground", "Truth:", "Someone", "uses", "a", "wood", "burning", "tool", "to", "burn", "a", "design", "into", "a", "slice", "of", "wood", "and", "then", "begins", "to", "brush", "polyurethane", "unto", "it.", "UniVL:", "you", "'", "re", "ready", "to", "decorate", "your", "cake", "BLIP:", "a", "person", "holding", "a", "string", "with", "a", "small", "object", "in", "MSR-VTT", "Caption", "YouCook2", "Caption", "VaTex", "Caption", "Objects:", "...", "Events:", "...", "Attributes:", "First,", "tagging.", "Then,", "woodburning.", "After", "that,", "wood", "burning.", "Finally,", "turning", "on", "dial.", "Frame", "Captions:", "First,", "a", "piece", "of", "wood", "with", "words", "drink", "up", "written", "on", "it", "...", "UniVL:", "add", "the", "sausages", "to", "the", "pan", "Ours:", "take", "the", "sausages", "out", "of", "the", "pan", "Ground", "Truth:", "Objects:...", "Events:...", "Attributes:...", "Captions:...", "Subtitle:", "Now", "our", "sausages", "are", "pretty", "much", "cooks", "going", "to", "take", "those", "out", "all", "the", "time.", "And", "we're", "going", "to", "now,", "my", "cat", "gravy", "as", "source.", "2", "men", "are", "discussing", "sports", "on", "a", "talk", "show", "a", "man", "being", "interviewed", "on", "a", "tv", "show", "UniVL:", "a", "man", "is", "playing", "a", "man", "with", "a", "man", ".", "BLIP:", "a", "man", "in", "a", "suit", "and", "tie", "sitting", "on", "a", "couch", "Ours:", "an", "interview", "with", "a", "sports", "Ground", "Truths:", "Objects:", "First,", "interview.", "Then,", "cable", "television.", "After", "that,", "television", "program.", "Finally,", "sports", "commentator.", "Events:", "...", "Attributes:", "...", "Frame", "Captions:", "..."],
  "name": "4",
  "page": 6,
  "regionBoundary": {
    "x1": 108.0,
    "x2": 504.0,
    "y1": 281.76,
    "y2": 422.4
  },
  "renderDpi": 150,
  "renderURL": "NeurIPS_2022_pic__LceCyuVcH-Figure4-1.png"
}, {
  "caption": "Figure 5: Prompt for VLEP task.",
  "captionBoundary": {
    "x1": 244.98500061035156,
    "x2": 367.0151062011719,
    "y1": 422.6925354003906,
    "y2": 428.69500732421875
  },
  "figType": "Figure",
  "imageText": ["Task:", "Video", "Language", "Event", "Prediction", "(VLEP)", "Few-shot", "Context", "Instruction", "Vlep", "Task", "Query", "Task", "Query", "Predict", "what", "is", "more", "likely", "to", "happen", "next", "based", "on", "the", "frame", "captions", "and", "dialogue.", "Example:", "Question:", "what", "is", "a", "little", "kid", "playing", "in", "with", "his", "toys?", "Answer:", "<example", "answer>", "or", "None", "Question:", "What", "is", "more", "likely", "to", "happen", "next?", "A:", "He", "puts", "the", "hot", "sauce", "on", "the", "food.", "B:", "The", "chef", "will", "serve", "the", "food.", "Answer:", "The", "chef", "will", "serve", "the", "food.", "Frame", "Captions:", "First,", "a", "man", "in", "a", "black", "shirt", "is", "serving", "food.", "Then,", "a", "group", "of", "people", "in", "a", "kitchen", "preparing", "food", "...", "Dialogue:", "I", "took", "a", "portion", "of", "it,", "you", "can", "smell", "the", "onions.", "You", "can", "smell", "kind", "of", "a", "spice", "like", "cinnamon", "a", "little", "bit", "..."],
  "name": "5",
  "page": 7,
  "regionBoundary": {
    "x1": 116.64,
    "x2": 492.0,
    "y1": 337.91999999999996,
    "y2": 416.15999999999997
  },
  "renderDpi": 150,
  "renderURL": "NeurIPS_2022_pic__LceCyuVcH-Figure5-1.png"
}, {
  "caption": "Table 4: Accuracy (%) on VLEP hidden test set.",
  "captionBoundary": {
    "x1": 384.8399963378906,
    "x2": 503.99713134765625,
    "y1": 75.0385513305664,
    "y2": 91.95001220703125
  },
  "figType": "Table",
  "imageText": ["Human", "-", "90.5", "VLEP", "[23]", "20142", "67.5", "MERLOT", "[66]", "20142", "68.4", "VidIL(ours)", "10-shot", "72.0", "Method", "#videoFT", "Acc"],
  "name": "4",
  "page": 7,
  "regionBoundary": {
    "x1": 384.96,
    "x2": 506.4,
    "y1": 101.75999999999999,
    "y2": 169.44
  },
  "renderDpi": 150,
  "renderURL": "NeurIPS_2022_pic__LceCyuVcH-Table4-1.png"
}, {
  "caption": "Table 5: Semi-supervised text-video retrieval with 10 labeled examples. Vlabel or Vunlabel are the number of labeled and unlabeled videos, respectively. t_R1 and t_R denote video-to-text Recall@1 and 5. v_R1 and v_R5 denote text-to-video Recall@1 and 5.",
  "captionBoundary": {
    "x1": 107.69100189208984,
    "x2": 504.751708984375,
    "y1": 453.17352294921875,
    "y2": 480.9939880371094
  },
  "figType": "Table",
  "imageText": ["BLIP", "-", "-", "33.2", "57.2", "40.5", "62.8", "-", "28.2", "53.4", "34.0", "58.6", "BLIP", "UniVL", "10", "/", "7010", "33.1", "57.3", "33.6", "57.7", "10", "/", "22685", "25.5", "47.7", "26.1", "49.1", "BLIP", "BLIP", "10", "/", "7010", "35.6", "60.8", "39.8", "60.4", "10", "/", "22685", "26.3", "50.5", "29.3", "53.6", "BLIP", "BLIPcap", "10", "/", "7010", "35.3", "58.0", "39.1", "63.3", "10", "/", "22685", "23.9", "46.8", "27.5", "49.7", "BLIP", "VidIL(ours)", "10", "/", "7010", "39.6", "64.5", "40.8", "65.2", "10", "/", "22685", "33.3", "59.1", "33.7", "59.5", "BLIP", "Ground", "Truth", "7010", "/", "0", "43.6", "66.2", "43.1", "67.2", "22685", "/", "0", "40.1", "66.4", "40.1", "66.6", "ALPRO", "[25]", "Ground", "Truth", "140200", "/", "0", "32.0", "60.6", "33.9", "60.7", "-", "-", "-", "-", "-", "DRL", "[54]", "Ground", "Truth", "180000", "/", "0", "54.1", "77.4", "52.9", "78.5", "-", "-", "-", "-", "-", "Model", "Pseudo", "Label", "MSR-VTT", "Retrieval", "VaTex", "Retrieval", "Vlabel/Vunlabel", "t_R1", "t_R5", "v_R1", "v_R5", "Vlabel/Vunlabel", "t_R1", "t_R5", "v_R1", "v_R5"],
  "name": "5",
  "page": 7,
  "regionBoundary": {
    "x1": 108.0,
    "x2": 504.0,
    "y1": 490.56,
    "y2": 593.28
  },
  "renderDpi": 150,
  "renderURL": "NeurIPS_2022_pic__LceCyuVcH-Table5-1.png"
}, {
  "caption": "Table 3: Video QA results. BLIPV QA is finetuned on VQA [4]. ♠ indicates concurrent work. PT, FT indicates pretraining and finetuning.",
  "captionBoundary": {
    "x1": 107.69100189208984,
    "x2": 378.93109130859375,
    "y1": 74.4555435180664,
    "y2": 91.36700439453125
  },
  "figType": "Table",
  "imageText": ["ALPRO", "[25]", "2M", "full-shot", "42.1", "45.9", "Method", "#videoPT", "#videoFT", "MSR-VTT", "MSVD", "BLIP", "0", "0-shot", "0.55", "0.45", "BLIP", "0", "5-shot", "0.84", "0.53", "BLIPV", "QA", "[26]", "0", "0-shot", "19.2", "35.2", "VidIL(ours)", "0", "5-shot", "21.2", "39.1", "♠Flamingo-3B", "[2]", "27M", "4-shot", "14.9", "33.0", "♠Flamingo-3B", "[2]", "27M", "8-shot", "19.6", "37.0", "♠Flamingo-80B", "[2]", "27M", "4-shot", "23.9", "41.7", "♠Flamingo-80B", "[2]", "27M", "8-shot", "27.6", "45.5"],
  "name": "3",
  "page": 7,
  "regionBoundary": {
    "x1": 109.92,
    "x2": 376.32,
    "y1": 101.28,
    "y2": 216.48
  },
  "renderDpi": 150,
  "renderURL": "NeurIPS_2022_pic__LceCyuVcH-Table3-1.png"
}, {
  "caption": "Figure 2: Overview of VidIL framework. We represent a video in a unified textural representation containing three semantic levels: visual token level, frame level, and video level. At visual token level, we extract salient objects, events, attributes for each sampled frame. At frame level, we perform image captioning and filtering. And at video level, we construct the video representation by aggregating the visual tokens, frame captions and other text modalities such as ASR, using a few-shot temporal-aware prompt. We then feed the prompt to a pre-trained language model together with task-specific instructions to generate target text for a variety of video-language tasks.",
  "captionBoundary": {
    "x1": 107.64099884033203,
    "x2": 504.1695251464844,
    "y1": 338.3515319824219,
    "y2": 409.8089904785156
  },
  "figType": "Figure",
  "imageText": ["Generate", "a", "video", "caption", "based", "on", "the", "objects,", "events,", "subtitle", "and", "frame", "captions.", "Example:", "Input", "Speech", "Question:", "what", "is", "a", "little", "kid", "playing", "in", "with", "his", "toys?", "Answer:", "<example", "answer>", "or", "None", "Question:", "what", "is", "a", "little", "kid", "playing", "in", "with", "his", "toys?", "Answer:", "<example", "answer>", "or", "None", "Task", "Query", "Task", "Query", "Video", "Caption:", "<example", "caption>", "or", "None", "Video", "Caption:", "<example", "caption>", "or", "None", "Task:", "Video", "Question", "Answering", "Visual", "Token", "Level", "Task:", "Video", "Captioning", "Frame", "Level", "Language", "Model", "Target", "Output", "Few-shot", "Context", "Objects:", "First,", "bath", "toy.", "Then,", "toddler.", "After", "that,", "adhesive", "bandage", "...", "Events:", "First,", "playing", "with", "bare", "feet.", "Then,", "child", "wearing", "sandal", "...", "Attributes:", "First,", "red", "toy.", "Then,", "diaper", "changing.", "After", "that,", "band", "aids", "...", "Frame", "Captions:", "First,", "a", "toddler", "playing", "in", "a", "bathtub", "filled", "with", "toys.", "Then,", "a", "toddler", "playing", "in", "a", "bathtub", "with", "toys.", "Finally,", "a", "boy", "sitting", "on", "the", "floor", "with", "his", "foot", "in", "a", "toilet", "Subtitle:", "<ASR", "Transcript>", "Video", "Level", "Task", "Instruction", "Output", "Answer", "Output", "Caption", "Frame", "Caption", "Frm1:", "a", "toddler", "playing", "in", "a", "bathtub", "filled", "with", "toys", "Frm2:", "a", "toddler", "playing", "in", "a", "bathtub", "with", "toys", "Frm3:", "<filtered", "out>", "FrmN:", "a", "boy", "sitting", "on", "the", "floor", "with", "his", "foot", "in", "a", "toilet", "Answer", "the", "question", "based", "on", "the", "objects,", "events,", "subtitle", "and", "frame", "captions.", "Example:", "Temporal-Aware", "Few-shot", "Prompt", "bathtub", "a", "boy", "in", "the", "tub", "with", "toys", "and", "one", "getting", "his", "foot", "bandaged", "ASR", "Frm1:", "bath", "toy,", "bathtub,", "toddler", "Frm2:", "adhesive", "bandage,", "band-aid,", "sink", "...", "FrmN:", "shoe", "care,", "adhesive", "bandage,", "snakebite", "...", "Attributes", "EventsObjects", "Semantic", "Role", "Labeling", "Frame", "Caption", "Generation", "and", "Filtering", "Model", "Image-", "Language", "Model", "Visual", "Tokenization", "Image-", "Language", "Sampled", "Frames", "Sparse", "Sampling", "Input", "Video"],
  "name": "2",
  "page": 3,
  "regionBoundary": {
    "x1": 110.88,
    "x2": 501.12,
    "y1": 72.0,
    "y2": 325.92
  },
  "renderDpi": 150,
  "renderURL": "NeurIPS_2022_pic__LceCyuVcH-Figure2-1.png"
}, {
  "caption": "Figure 3: Temporal-aware prompt successfully distinguishes the Sunset and Sunrise scenario from the temporal ordering change of objects and frame captions, while the static prompt fails.",
  "captionBoundary": {
    "x1": 108.0,
    "x2": 504.00238037109375,
    "y1": 195.15853881835938,
    "y2": 212.07000732421875
  },
  "figType": "Figure",
  "imageText": ["night", "sky.", "generation:", "generation:", "The", "sun", "is", "setting", "over", "the", "ocean,", "and", "the", "stars", "are", "shining", "at", "night.", "The", "stars", "are", "shining", "at", "night,", "and", "the", "sun", "is", "rising", "over", "the", "ocean.", "The", "sun", "is", "setting", "over", "the", "ocean", "and", "the", "stars", "are", "shining", "at", "night.", "The", "sun", "is", "setting", "over", "the", "ocean", "as", "the", "stars", "start", "to", "shine", "in", "the", "change", "ordering", "of", "the", "objects", "and", "frame", "captions", "Expected", "caption:", "Sunset", "Expected", "caption:", "Sunrise", "Temporal-", "Aware", "Prompt", "Static", "Prompt", "Objects:", "First,", "sun", "moving.", "Then,", "night", "sky.", "Frame", "Captions:", "First,", "sun", "over", "an", "ocean.", "Then,", "stars", "shining", "at", "night.", "Video", "Caption:", "Objects:", "First,", "night", "sky.", "Then,", "sun", "moving.", "Frame", "Captions:", "First,", "stars", "shining", "at", "night.", "Then,", "sun", "over", "an", "ocean.", "Video", "Caption:", "Objects:", "night", "sky,", "sun", "moving", "Frame", "Captions:", "stars", "shining", "at", "night,", "sun", "over", "an", "ocean", "Video", "Caption:", "Objects:", "sun", "moving,", "night", "sky", "Frame", "Captions:", "sun", "over", "an", "ocean,", "stars", "shining", "at", "night", "Video", "Caption:"],
  "name": "3",
  "page": 4,
  "regionBoundary": {
    "x1": 108.96,
    "x2": 478.08,
    "y1": 76.32,
    "y2": 181.92
  },
  "renderDpi": 150,
  "renderURL": "NeurIPS_2022_pic__LceCyuVcH-Figure3-1.png"
}]