{
  "ID": "_LceCyuVcH",
  "Title": "Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners",
  "URL": "https://openreview.net/forum?id=_LceCyuVcH",
  "paper_draft_url": "/references/pdf?id=9CrQedkz_",
  "Conferece": "NeurIPS_2022",
  "input": {
    "source": "CRF",
    "title": "Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners",
    "authors": [],
    "emails": [],
    "sections": [
      {
        "heading": null,
        "text": "The goal of this work is to build flexible video-language models that can gener-1 alize to various video-to-text tasks from few examples, such as domain-specific2 captioning, question answering, and future event prediction. Existing few-shot3 video-language learners focus exclusively on the encoder, resulting in the absence4 of a video-to-text decoder to handle generative tasks. Video captioners have been5 pretrained on large-scale video-language datasets, but they rely heavily on fine-6 tuning and lack the ability to generate text for unseen tasks in a few-shot setting.7 We propose VidIL, a few-shot Video-language Learner via Image and Language8 models, which demonstrates strong performance on few-shot video-to-text tasks9 without the necessity of pretraining or finetuning on any video datasets. We use the10 image-language models to translate the video content into frame captions, object,11 attribute, and event phrases, and compose them into a temporal structure template.12 We then instruct a language model, with a prompt containing a few in-context13 examples, to generate a target output from the composed content. The flexibility of14 prompting allows the model to capture any form of text input, such as automatic15 speech recognition (ASR) transcripts. Our experiments demonstrate the power16 of language models in understanding videos on a wide variety of video-language17 tasks, including video captioning, video question answering, video caption retrieval,18 and video future event prediction. Especially, on video future event prediction,19 our few-shot model significantly outperforms state-of-the-art supervised models20 trained on large-scale video datasets.21\n1 Introduction22\nOne major gap between artificial intelligence and human intelligence lies in their abilities to generalize23 and perform well on new tasks with limited annotations. Recent advances in large-scale pre-trained24 generative language models [43, 6, 69, 24] have shown promising few-shot capabilities [70, 41, 61]25 in understanding natural language. However, few-shot video-language understanding is still in its26 infancy. A particular limitation of most recent video-language pretraining frameworks [28, 21, 59,27 66, 65, 25, 62] is that they are encoder-only, which means they do not have the ability to generate28 text from videos for purposes such as captioning [60, 55], question answering [58], and future29 prediction [23]. Meanwhile, unified video-language models [34, 47] that are capable of language30 decoding still rely heavily on finetuning using a large number of manually annotated video-text31 pairs, therefore cannot adapt quickly to unseen tasks. Few-shot video-to-text decoding is challenging32 because the natural language supervision for learning video-language representation is typically based33 on subtitles and automatic speech recognition (ASR) transcripts [37, 66], which differ significantly34 from downstream tasks in terms of distribution and may have poor semantic alignment across vision35 and text modalities.36\nSubmitted to 36th Conference on Neural Information Processing Systems (NeurIPS 2022). Do not distribute.\nWe propose to address this problem by harnessing the few-shot power of large-scale language models,37 such as GPT-3 [6] and InstructGPT [38]. Our inspiration is derived from the fact that humans are38 excellent visual storytellers [15], with the ability to piece together a coherent story from a few39 isolated images. To mimic this, we propose VidIL, a few-shot Video-language Learner via Image40 and Language models, to use image models to provide information about the visual content in the41 video (as well as optionally use ASR to represent speech), and then we instruct language models to42 interpret a video-based summary, answer, or other target output for diverse video-language tasks.43\nVisual Token Level\nFrame Level\ncake decorating, sugar paste, clay animation, play-doh\na person holding a green object in their hand\na person cutting a piece of paper with a pair of scissors\nOurs: a person is making a cake out of fondant and clay, and then decorating it with doilies and leaves Ground Truth: a woman makes realistic looking leaves and flowers for a cake Ground Truth 2: a woman creating a fondant baby and flower\nObjects Events\nFrame Captions\nVideo Captions Video Level\nInput Video\nLanguage Model\nImage-Language Model\na person is putting a green leaf on a baby's head\nAttributes cutting mat, woman shaped cake, cake is made, flowered design made of fondant, edging, rubbing, paper doilies, green goo\nTo address these requirements, we propose to decompose a video into three levels: the video output,62 frame captions, and visual tokens (including object, event, attribute). One major benefit from this63 hierarchical video representation is that we can separate the visual and temporal dimensions of a video.64 We leverage well-trained image-language foundational models at lower levels to collect salient visual65 features from the sparsely sampled frames. Specifically, we leverage pretrained image-language66 contrastive model CLIP [42] to perform visual tokenization, based on the similarity score between67 frames and tokens of objects, events and attributes. The tokenization is done under the guidance68 of semantics role labeling [14], which provides us with candidate events with involved objects and69 related attributes. Next, in order to capture the overall semantics at the frame level, we employ the70 pretrained image captioner in image-language model BLIP [26] to obtain frame captions. We then71 instruct pretrained language models using in-context learning [38, 13, 49, 46] to interpret visual72 tokens and frame captions into the target text content. In detail, we temporally order visual tokens73 and frame captions using specially designed prompts such as \u201cFirst...Then...Finally\u201d, to instruct the74 pretrained language model to track the changes of objects, events, attributes and frame semantics75 along the temporal dimension.76\nWe show that our approach outperforms both video-language and image-language state-of-the-art77 baselines on few-shot video captioning and question answering tasks. Moreover, on the video-78 language event prediction task, we significantly outperform fully-supervised models while using only79 10 labeled examples. We further demonstrate that our generative model can benefit broader video-80 language understanding tasks, such as text-video retrieval, via pseudo label generation. Additionally,81 we show that our model is highly flexible in adding new modalities, such as ASR transcripts.82\n2 Related Work83\n2.1 Image-Language Models and Their Applications on Video-Language Tasks84\nLarge-scale image-language pretraining models optimize image-text matching through contrastive85 learning such as CLIP [42] or by learning image-text alignments [63, 27, 56, 64, 33, 50, 8, 29, 71,86 68, 17, 18, 16]. Recently, BLIP [26] proposes an image captioner pretrained with filtering in order87 to achieve optimal image-text alignment and has shown promising performance on various image-88 language tasks. However, video-language pretraining [25, 34, 28, 36, 3, 1, 40] is still hindered by89 noisy and domain-specific video datasets [22, 37]. Naturally, researchers start to explore transferring90\nthe rich knowledge from image models to videos. Different from the traditional way of representing91 videos by 3D dense features [12], recent work [21, 25] proves that sparse sampling is an effective way92 to represent videos, which facilitates applying pre-trained image-language models to video-language93 tasks [35, 11]. Specifically, the image-language model BLIP [26] sets new state-of-the-art on zero-shot94 retrieval-style video-language tasks, such as video retrieval and video question answering. However,95 for generation-style tasks such as domain-specific video captioning, video-language model such as96 UniVL [34] still leads the performance but highly rely on fine-tuning. In this work, we extend the idea97 of leveraging image-language models to a wide variety of video-to-text generation tasks. We further98 connect image-language models (ILM) with language models (LM) which empowers our model with99 strong generalization ability. We show that the knowledge from both image-language pretraining and100 language-only pretraining can benefit video-language understanding in various aspects.101\n2.2 Unifying Multi-Modal Tasks with Language Models102\nConnecting different modalities with an unified representation has been paid much attention to103 recently. Text-only generation models, such as T5 [44], has been extended to vision-language tasks104 by text generation conditioned on visual features [9, 51, 48, 73, 53]. In order to fully leverage105 the generalization power from pretained language models, [61] represents images using text in a106 fully symbolic way. [32] includes more modalities such as video and audio, but requires annotated107 video-text data to jointly training the language model with the video and audio tokenizer. In this work,108 we propose a temporal-aware hierarchical representation for describing a video textually. To our109 knowledge, we are the first work to leverage prompting a frozen language model for tackling few-shot110 video-language tasks with a unified textual representation. Concurrent work Socratic [67] uses a111 zero-shot language-based world-state history to represent long videos with given time stamps, while112 our model can quickly adapt to different video and text distributions with few examples. Furthermore,113 we show that by injecting temporal markers to the prompt we can make a pre-trained language model114 understand fine-grained temporal dynamics in video events. Compared with the concurrent work115 Flamingo [2], which requires dedicated vision-language post-pretraining, our framework does not116 require to pretrain on any videos. Our framework is simple and highly modulated where all the117 components are publicly available. Additionally, our framework is more flexible on adding new118 modalities, e.g., automatic speech recognition, without the need for complex redesigning.119\n3 Method120\nWe propose a hierarchical video representation framework which decomposes a video into three121 levels, i.e., visual token level, frame level and video level. The motivation is to separate the spatial122 and temporal dimension of a video in order to leverage image-language and language-only foundation123 models, such as CLIP [42] and GPT-3 [6]. All three levels use a unified textual representation which124 enables us to leverage the powerful few-shot ability from pretrained language models.125\n3.1 Frame Level: Image Captioning126\nFollowing [21] we first perform sparse sampling to obtain several video frames. Unless otherwise127 specified, we sample 4 frames for frame level and 8 frames for visual token level. We then feed each128 frame into a pre-trained image-language model to obtain frame level captions. An example can be129 found in the blue part of Figure 2. In our experiments, we use BLIP [26], a recent image-language130 framework containing both image-grounded encoder and decoder, for generating frame captions. We131 follow [26] to do both captioning and filtering on each frame. However, as mentioned in Section 1,132 videos contain rich semantics and temporal contents at multiple granularities. It is not enough to133 generate video-level target text such as video captions solely based on frame captions. Thus, we134 further perform visual tokenization for each frame to capture features at a finer granularity.135\n3.2 Visual Token Level: Structure-Aware Visual Tokenization136\nAt this level, we aim to extract the textual representations of salient visual token types, such as137 objects, events and attributes. We found that pre-defined classes for classification, such as those in138 ImageNet [10], are far from enough for covering the rich semantics in open-domain videos. Thus,139 instead of using classification-based methods for visual tokenization as in previous work [32, 61],140\nwe adopt a retrieval-based visual tokenization approach by leveraging pre-trained contrastive image-141 language models. Given a visual token vocabulary which contains all candidate object, event, and142 attribute text phrases, we compute the image embedding of a frame and the text embeddings of the143 candidate visual tokens using a contrastive multi-modal encoder, CLIP [42]. We then select top 5144 visual tokens based on the cosine similarity of the image and text embeddings. An example of the145 extracted object tokens can be found in the green part of Figure 2.146\nUnlike in images where objects and attributes already cover most visual features, events are more147 informative in videos. In order to discover events from video frames, we construct our own event148 vocabulary by extracting event structures from Visual Genome [19] synsets1 using Semantic Role149 Labeling. Specifically, we first select the phrases that contains at least one verb and one argument150 as events. Then we remove highly similar events based on their sentence similarity using Sentence-151 BERT [45] embeddings. For object vocabulary, we adopt OpenImage [20] full classes (\u223c20k), instead152 of using the visually groundable subset (\u223c600) as in concurrent work [67]. We found that using large153 but noisy vocabulary is more effective than using small but clean vocabulary in our retrieval-based154 setting with CLIP. For attribute vocabulary, we adopt visual genome attribute synset. In Section 4.6,155 we provide ablation study on the impact of different types of visual tokens. The statistics of visual156 token vocabulary can be found in Appendix.157\n3.3 Video Level: Temporal-Aware Few-shot Prompting158\nOnce we obtain the textual representation from frame level and visual token level, the final step is159 to put the pieces together to generate a video level target text. The goal is to build a model that160 can be quickly adapted to any video-to-text generation task with only a few examples. To this161 end, we propose to leverage large-scale pre-trained language models, such as GPT-3 [6], with a162\n1We use the keys in Visual Genome [19] object synsets which contains frequent <verb,object> pairs.\ntemporal-aware few-shot prompt. As shown in Figure 2, our framework can be readily applied to163 various video-to-text generation tasks, such as video captioning and video question answering, with a164 shared prompt template. The proposed prompting strategy enables a language model to attend to the165 lower level visual information as well as taking into account the temporal ordering.166\nHere, we use the video captioning task depicted in Figure 2 to illustrate the details. The few-shot167 prompt consists of three parts: instruction, few-shot context, and task query. The instruction is168 a concise description of the generation task, e.g., \"Generate a video caption based on the169 objects, events, attributes and frame captions. Example:\", which is proved to be170 effective in zero-shot and few-shot settings [6, 57]. The few-shot context contains the selected171 in-context examples as well as the test video instance. Each video instance is represented by the172 aggregated visual tokens2, e.g., \"Objects: First, bath toy. Then,...\", the frame cap-173 tions, e.g., \"Frame Captions: First, a toddler playing in a bathtub filled with174 toys. Then,...\", and the ASR inputs if available, e.g., \"Subtitle:<ASR Transcript>\".175 Finally, the task query is a task-specific suffix indicating the target text format, e.g. \"Video176 Caption:\". For in-context examples (omitted here for simplicity), the task query is followed by177 ground truth annotation, while for the test instance, the generation starts at the end of the task query.178\nFormally, we denote the instruction line as t, few-shot context as c, the task query as q, and the target179 text as y, where y = (y1, y2, ..., yL). The generation of the next target token yl can be modeled as:180\nyl = argmax y\np(y|s, c,q, y<l) (1)\nIn order to capture the temporal dynamics between frames and visual tokens, we further propose to181 inject temporal markers to the prompt. As shown in the few-shot context in Figure 2, each visual182 token and frame caption is prefixed with a natural language phrase indicating its temporal ordering,183 e.g., \"First,\",\"Then,\", and \"Finally,\". We found adding the temporal marker can make the184 language model condition on not only the literal but also the temporal information of the context. We185 show an example in Figure 3, where we compare our temporal-aware prompt with a static prompt on186 video captioning using InstructGPT. Again, the in-context examples are omitted here, which can be187 found in Appendix. In this example, the only difference between the two context is the ordering of188 the visual tokens and the frame captions. For the context on the left, where \"sun moving\" appears189 before \"night sky\", we are expected to see a story talking about sunset, while for the context on190 the right, we are expected to see sunrise. We can see the static prompt generates captions about191 sunset for both contexts, while the temporal-aware prompt can capture the temporal ordering correctly192 and generate sunrise for the context on the right.193\n4 Experiments194\n4.1 Experimental Setup195\nTo comprehensively evaluate our model, we show results on four video-language understanding196 tasks in few-shot settings: video captioning, video question answering (QA), video-language event197\n2To obtain video level visual tokens, the visual tokens extracted from each frame are further ranked and ordered based on frequency and frame index.\nprediction, and text-video retrieval. We compare our approach with state-of-the-art approaches on198 five benchmarks, i.e, MSR-VTT [60], MSVD [7], VaTeX [55], YouCook2 [72], and VLEP [23].199\nImplementation Details. We use CLIP-L/144200 as our default encoder for visual tokenization.201 We adopt BLIP captioning checkpoint5 fine-202 tuned on COCO [31] for frame captioning. We203 use InstructGPT [38] as our default language204 model for generating text conditioned on the205 few-shot prompt. To construct event vocabulary,206 we use the semantic role labeling model from207 AllenNLP6. The experiments are conducted on208 2 NVIDIA V100 (16GB) GPUs.209\nIn-context Example Selection. From our preliminary experiments, we found that the generation210 performance is sensitive to the quality of in-context examples. For example, for QA tasks such211 as MSVD-QA where the annotations are automatically generated, the <question, answer> pair in212 randomly selected in-context examples can be only weakly-correlated with the video context. Thus,213 instead of using a fixed prompt for each query, we dynamically filter out the irrelevant in-context214 examples. Specifically, given a randomly sampled M-shot support set from the training set, we select215 a subset of N-shots as in-context examples based on their SentenceBERT [45] similarities with text216 queries. For QA tasks, we choose the most relevant in-context examples by comparing with questions.217 While for captioning task, we compare with frame captions. If not otherwise specified, we use M=10218 and N=5, which we consider as 10-shot training.219\n4.2 Few-shot Video Captioning220\nWe report BLEU-4 [39], ROUGE-L [30], METEOR [5], and CIDEr [52] scores on three video caption-221 ing benchmarks covering both open-domain (MSR-VTT, VaTeX) and domain-specific (YouCook2)222 videos. We compare with both state-of-the-art video captioner (UniVL [34]) and image captioner223 (BLIP [26]). In order to implement the BLIP baseline for few-shot video captioning, we extend the224 approach used for text-video retrieval evaluation in [26] to video-language training. Specifically,225 we concatenate the visual features of sampled frames and then feed them into the image-grounded226 text-encoder to compute the language modeling loss. This is equivalent to stitching the sampled227 frames into a large image and then feeding it to BLIP for image captioning. We found that this simple228 approach results in very strong baselines.229\nAs shown in Table 2, existing methods have strong bias on certain datasets. For example, UniVL230 performs well on YouCook2 but fails on MSR-VTT and VaTeX, while BLIP performs the oppo-231 site. This is because UniVL is pretrained on HowTo100M which favors instructional videos, i.e.,232 YouCook2, while BLIP is pre-training on image-caption pairs which favors description-style captions,233 i.e., MSR-VTT and VaTeX. On the contrary, our model performs competitively on both open-domain234 and instructional videos, and significantly outperforms the baselines in the average CIDEr score235 across all three benchmarks. This indicates that by leveraging language models, we can maintain236 strong few-shot ability regardless of the video domain and or the target caption distribution.237\nAs discussed in Section 1, video captions describes the content in various semantic levels. The238 N-gram based metric may not fairly reflect the models\u2019 performance in capturing the video-caption239 alignment. We further verifies this hypothesis in Section 4.5. Thus, in addition to the automatic240 metrics, we include qualitative examples illustrated in Figure 4. More examples are in Appendix.241\nAdditionally, for most existing methods and also concurrent work, e.g., Flamingo [2], adding a new242 modality often requires a dedicated model redesign and or retraining. However, the nature of our243 framework, where we use a unified textual representation for each level, making it highly flexible244 for incorporating new modalities. As shown in row 5 and 6 in Table, our model can benefit from the245 added modality more effectively than the baselines.246\n4.3 Few-shot Video Question Answering247\nWe compare the test accuracy of our approach with few-shot pretrained BLIP, BLIPV QA [26], and248 concurrent work Flamingo [2] on two video question answering benchmarks, MSR-VTT_QA and249 MSVD_QA. BLIPV QA represents finetuned BLIP on VQA [4] dataset, which is the previous SOTA250 on zero/few-shot video question answering. In order to have fairer comparison with BLIPV QA, we251 reduce the shot number to 5 and report the average accuracy on three sets of randomly selected252 5-shot examples. As shown in Table 3, our method outperforms previous SOTA by a large margin.253 Comparing with concurrent work Flamingo, which is post-pretrained on a large number of video-text254 data, our model is training-free and did not observe any video data. However, with only image-255 language and language-only knowledge, our 5-shot model is able to beat 8-shot Flamingo-3B and256 achieve on-par performance with 4-shot Flamingo-80B.257\n4.4 Few-shot Video-Language Event Prediction258\nIn this section, we show that our model not only can answer questions about the video visual features259 but also answering \"What is more likely to happen next?\". Given a video with associated subtitle260 transcript as premise, the video-language event prediction (VLEP) task is to predict the most likely261 future event. The original VLEP [23] paper formulates the problem as a binary classification problem262 where the model will be chosen from two possible future event candidates. Instead, we formulate this263 problem as another video-to-text generation problem to fit into our framework. Figure 5 depicts an264\nexample with the same format as in Figure 2. Similar to the evaluation setting in QA, the generated265 free-form text will first be mapped to one of the two candidate answers using SentenceBert [45], and266 then calculate the accuracy. In Table 4, we report accuracy on the hidden test set of VLEP [23]. To267 our surprise, our 10-shot model beats state-of-the-art fully-supervised baseline, i.e., MERLOT [66],268 by a large margin (\u223c 4%). This is showing that our model has strong few-shot ability not only in269 video-language understanding but also in prediction. Since event prediction tasks rely heavily on270 temporal ordering, we show that with the proposed temporal-aware prompting, language models can271 be guided to capture temporal dynamics between historical and future events.272\n4.5 Semi-supervised Text-Video Retrieval273\nIn addition to video-to-text generation tasks, we show that a broader range of video-language tasks274 can benefit from our few-shot video captioner from a data perspective. Here, we consider a low-275 budget semi-supervised setting where we only have a few labeled video-captain pairs and a large276 amount of unlabeled videos. The idea is to leverage our video captioner to generate pseudo labels for277 training any given vision-language models. As a case study, we evaluate on two text-video retrieval278 benchmarks, i.e., MSR-VTT and VaTeX. We use greedy decoding to generate pseudo caption for279 each video in the training set. We then train an identical base model, i.e., BLIP, using different pseudo280 labeled data as well as ground truth annotations. We report Recall @ 1 and 5 for both video-to-text281\nand text-to-video retrieval. Table 5 shows that through training on our pseudo labels, we can achieve282 significant improvements compared with zero-shot BLIP. We also show that the performance gain283 is not simply a result of training on more data, since finetuning on the pseudo labels generated by284 other baselines (UniVL, BLIP) is less effective and can even hurt the performance. Furthermore, on285 MSR-VTT Recall @ 5 we can even achieve comparable performance against BLIP model finetuned286 on full ground truth annotations.287\nAnother interesting observation is that, compared with the video captioning results in Table 2, we288 found that the gain of our model over baselines on text-video retrieval is more visible than on289 captioning. A key factor in performing well on text-video retrieval tasks is to learn a good video-text290 multi-modal alignment. This result shows that our pseudo labels capture richer video-text alignment291 that can benefit the retrieval-style downstream task. The N-gram based generation metrics, e.g.,292 BLEU, may not be able to fully reflect the alignment information, due to the variety of semantic293 levels in video captions. Furthermore, from a data perspective, our video captioner can be viewed294 as a data augmentation tool which is capable of generating or augmenting any open-domain video-295 language pretraining datasets with minimal human effort. As a result, we can potentially improving296 video-language pretraining by constructing a cleaner and more diverse video-text corpus.297\nTable 6: Impact of visual tokens.\nVideo Representation Avg\u2191 Std\u2193 Frame 39.6 3.7 Frame+Object 40.3 2.9 Frame+Object+Event 39.9 2.8 Frame+Object+Event+Attribute 40.8 2.4\nTable 7: Impact of the shot selection.\n#shot w/o in-context selection w/ in-context selectionAvg\u2191 Std\u2193 Avg\u2191 Std\u2193 5-shot 38.4 2.1 40.4 1.2 10-shot 41.3 3.6 40.8 2.4 20-shot 42.6 3.3 42.2 2.0 30-shot 40.0 2.9 41.1 1.9\n4.6 Ablation Studies298\nWe perform comprehensive ablation studies on our few-shot prompt including the impact of different299 video representation, the number of shots and the in-context selection. All the ablation results are300 evaluated on MSVD_QA validation set, we report the mean and standard deviation of each setting301 on three sets of randomly sampled shots. For the cases with in-context example selection, we302 further select 5 examples as in-context examples from the sampled shots, while for the cases without303 in-context selection, all shots will be feed into the prompt. In Table 6, we show adding visual tokens304 consistently improves not only the model accuracy but also the model variance. A lower standard305 deviation indicates that the model is less sensitive to the few-shot sampling. In Table 7, we first show306 that, with the same context length, namely, 5 examples, in-context example selection significantly307 increases the performance as well as the robustness. At 10-shot, and 20-shot, directly fitting more308 shots into the prompt, i.e., w/o in-context selection, results in better performance than selecting from a309 larger support set while keeping the context length unchanged. Interestingly, at 30-shot, the situation310 reversed again. We observe that selecting 5 examples from a support set of size 30 results in better311 performance than adding all 30 shots as in-context example. This is showing that in-context selection312 can help our model utilize a larger number video examples. However, we still observe that the benefit313 of adding more shots saturated at around 20 to 30 shots, even if with in-context selection. we view314 this as a remaining challenging on how to make language models benefit from longer contexts.315\n5 Conclusions, Limitations and Future Work316\nThis paper proposes VidIL, a few-shot Video-language Learner via Image and Language models.317 It demonstrates the strong ability of large-scale language models on performing video-to-text tasks318 when frame features are provided as unified text representations using image-language models. We319 propose a temporal order aware prompt by decomposing videos into a hierarchical structure, which is320 able to plug in multiple levels of frame features, along with speech transcripts. Without pretraining321 on videos, our model outperforms vision-language models learned from large-scale video datasets322 on a variety of few-shot tasks, such as domain-specific captioning, question answering, and future323 event prediction. One limitation is that the model is sensitive to the quality of in-context examples, as324 discussed in ablation studies. Future work will focus on leveraging large-scale language models to325 assist in video pretraining, such as its script knowledge for long video understanding and storytelling.326 For broader impact on the society, please refer to the supplemental material.327\nReferences328 [1] Hassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong.329 Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text. Advances in330 Neural Information Processing Systems, 34, 2021. 2331\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,332 Arthur Mensch, Katie Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot333 learning. arXiv preprint arXiv:2204.14198, 2022. 3, 7, 8334\n[3] Jean-Baptiste Alayrac, Adria Recasens, Rosalia Schneider, Relja Arandjelovic\u0301, Jason Ramapuram, Jeffrey335 De Fauw, Lucas Smaira, Sander Dieleman, and Andrew Zisserman. Self-supervised multimodal versatile336 networks. Advances in Neural Information Processing Systems, 33:25\u201337, 2020. 2337\n[4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick,338 and Devi Parikh. VQA: Visual Question Answering. In International Conference on Computer Vision339 (ICCV), 2015. 7, 8340\n[5] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved341 correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation342 measures for machine translation and/or summarization, pages 65\u201372, 2005. 6343\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind344 Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.345 Advances in neural information processing systems, 33:1877\u20131901, 2020. 1, 2, 3, 4, 5346\n[7] David Chen and William Dolan. Collecting highly parallel data for paraphrase evaluation. In Proceedings of347 the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,348 pages 190\u2013200, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. 6349\n[8] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing350 Liu. Uniter: Universal image-text representation learning. In European conference on computer vision,351 pages 104\u2013120. Springer, 2020. 2352\n[9] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation.353 In International Conference on Machine Learning, pages 1931\u20131942. PMLR, 2021. 3354\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical355 image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255.356 Ieee, 2009. 3357\n[11] Han Fang, Pengfei Xiong, Luhui Xu, and Yu Chen. Clip2video: Mastering video-text retrieval via image358 clip. arXiv preprint arXiv:2106.11097, 2021. 3359\n[12] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video360 recognition. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6202\u2013361 6211, 2019. 3362\n[13] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners.363 arXiv preprint arXiv:2012.15723, 2020. 2364\n[14] Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson Liu, Matthew Peters,365 Michael Schmitz, and Luke Zettlemoyer. Allennlp: A deep semantic natural language processing platform.366 arXiv preprint arXiv:1803.07640, 2018. 2367\n[15] Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin,368 Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. Visual storytelling. In Proceedings of369 the 2016 Conference of the North American Chapter of the Association for Computational Linguistics:370 Human Language Technologies, pages 1233\u20131239, 2016. 2371\n[16] Zhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei Liu, Dongmei Fu, and Jianlong Fu. Seeing out of the372 box: End-to-end pre-training for vision-language representation learning. In Proceedings of the IEEE/CVF373 Conference on Computer Vision and Pattern Recognition, pages 12976\u201312985, 2021. 2374\n[17] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V Le, Yunhsuan Sung,375 Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text376 supervision. arXiv preprint arXiv:2102.05918, 2021. 2377\n[18] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or378 region supervision. In ICML, 2021. 2379\n[19] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,380 Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision381 using crowdsourced dense image annotations. International journal of computer vision, 123(1):32\u201373,382 2017. 4383\n[20] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Ka-384 mali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4. International385 Journal of Computer Vision, 128(7):1956\u20131981, 2020. 4386\n[21] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, and Jingjing Liu. Less is more:387 Clipbert for video-and-language learningvia sparse sampling. In CVPR, 2021. 1, 3388\n[22] Jie Lei, Licheng Yu, Mohit Bansal, and Tamara L Berg. Tvqa: Localized, compositional video question389 answering. In EMNLP, 2018. 2390\n[23] Jie Lei, Licheng Yu, Tamara L Berg, and Mohit Bansal. What is more likely to happen next? video-and-391 language future event prediction. In EMNLP, 2020. 1, 6, 7, 8392\n[24] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves393 Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language394 generation, translation, and comprehension. arXiv preprint arXiv:1910.13461, 2019. 1395\n[25] Dongxu Li, Junnan Li, Hongdong Li, Juan Carlos Niebles, and Steven C.H. Hoi. Align and prompt:396 Video-and-language pre-training with entity prompts. In arxiv, 2021. 1, 2, 3, 8397\n[26] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training398 for unified vision-language understanding and generation. arXiv preprint arXiv:2201.12086, 2022. 2, 3, 6,399 7, 8400\n[27] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong401 Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances402 in Neural Information Processing Systems, 34, 2021. 2403\n[28] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng Yu, and Jingjing Liu. Hero: Hierarchical encoder404 for video+ language omni-representation pre-training. In Proceedings of the 2020 Conference on Empirical405 Methods in Natural Language Processing (EMNLP), pages 2046\u20132065, 2020. 1, 2406\n[29] Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong407 Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In408 European Conference on Computer Vision, pages 121\u2013137. Springer, 2020. 2409\n[30] Chin-Yew Lin and Franz Josef Och. Automatic evaluation of machine translation quality using longest410 common subsequence and skip-bigram statistics. In Proceedings of the 42nd Annual Meeting of the411 Association for Computational Linguistics (ACL-04), pages 605\u2013612, 2004. 6412\n[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,413 and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on414 computer vision, pages 740\u2013755. Springer, 2014. 6415\n[32] Xudong Lin, Gedas Bertasius, Jue Wang, Shih-Fu Chang, Devi Parikh, and Lorenzo Torresani. Vx2text:416 End-to-end learning of video-based text generation from multimodal inputs. In 2021 IEEE/CVF Conference417 on Computer Vision and Pattern Recognition (CVPR), pages 7001\u20137011. IEEE, 2021. 3418\n[33] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic419 representations for vision-and-language tasks. arXiv preprint arXiv:1908.02265, 2019. 2420\n[34] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, and421 Ming Zhou. Univl: A unified video and language pre-training model for multimodal understanding and422 generation. arXiv preprint arXiv:2002.06353, 2020. 1, 2, 3, 6423\n[35] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. CLIP4Clip: An424 empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860, 2021. 3425\n[36] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan Laptev, Josef Sivic, and Andrew Zisserman.426 End-to-end learning of visual representations from uncurated instructional videos. In Proceedings of the427 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9879\u20139889, 2020. 2428\n[37] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic.429 Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In430 Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2630\u20132640, 2019. 1, 2431\n[38] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang,432 Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with433 human feedback. arXiv preprint arXiv:2203.02155, 2022. 2, 6434\n[39] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic evaluation435 of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational436 Linguistics, pages 311\u2013318, 2002. 6437\n[40] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander Hauptmann, Joao Henriques,438 and Andrea Vedaldi. Support-set bottlenecks for video-text representation learning. arXiv preprint439 arXiv:2010.02824, 2020. 2440\n[41] Baolin Peng, Chenguang Zhu, Chunyuan Li, Xiujun Li, Jinchao Li, Michael Zeng, and Jianfeng Gao.441 Few-shot natural language generation for task-oriented dialog. arXiv preprint arXiv:2002.12328, 2020. 1442\n[42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish443 Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from444 natural language supervision. arXiv preprint arXiv:2103.00020, 2021. 2, 3, 4445\n[43] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,446 Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.447 arXiv preprint arXiv:1910.10683, 2019. 1448\n[44] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,449 Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer.450 Journal of Machine Learning Research, 21:1\u201367, 2020. 3451\n[45] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In452 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th453 International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982\u20133992,454 2019. 4, 6, 8455\n[46] Timo Schick and Hinrich Sch\u00fctze. Exploiting cloze questions for few shot text classification and natural456 language inference. arXiv preprint arXiv:2001.07676, 2020. 2457\n[47] Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and Cordelia Schmid. End-to-end generative pretraining458 for multimodal video captioning. arXiv preprint arXiv:2201.08264, 2022. 1459\n[48] Yixuan Su, Tian Lan, Yahui Liu, Fangyu Liu, Dani Yogatama, Yan Wang, Lingpeng Kong, and Nigel Collier.460 Language models can see: Plugging visual controls in text generation. arXiv preprint arXiv:2205.02655,461 2022. 3462\n[49] Derek Tam, Rakesh R Menon, Mohit Bansal, Shashank Srivastava, and Colin Raffel. Improving and463 simplifying pattern exploiting training. arXiv preprint arXiv:2103.11955, 2021. 2464\n[50] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers.465 arXiv preprint arXiv:1908.07490, 2019. 2466\n[51] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Multimodal467 few-shot learning with frozen language models. Advances in Neural Information Processing Systems,468 34:200\u2013212, 2021. 3469\n[52] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description470 evaluation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages471 4566\u20134575, 2015. 6472\n[53] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren473 Zhou, and Hongxia Yang. Unifying architectures, tasks, and modalities through a simple sequence-to-474 sequence learning framework. arXiv preprint arXiv:2202.03052, 2022. 3475\n[54] Qiang Wang, Yanhao Zhang, Yun Zheng, Pan Pan, and Xian-Sheng Hua. Disentangled representation476 learning for text-video retrieval. arXiv preprint arXiv:2203.07111, 2022. 8477\n[55] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang, and William Yang Wang. Vatex: A478 large-scale, high-quality multilingual dataset for video-and-language research. In IEEE/CVF International479 Conference on Computer Vision (ICCV), pages 4580\u20134590. IEEE, 2019. 1, 6480\n[56] Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual481 language model pretraining with weak supervision. arXiv preprint arXiv:2108.10904, 2021. 2482\n[57] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M483 Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652,484 2021. 5485\n[58] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video486 question answering via gradually refined attention over appearance and motion. In ACM MM, 2017. 1487\n[59] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke488 Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text489 understanding. arXiv preprint arXiv:2109.14084, 2021. 1490\n[60] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video491 and language. In IEEE International Conference on Computer Vision and Pattern Recognition (CVPR),492 June 2016. 1, 6493\n[61] Zhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An494 empirical study of gpt-3 for few-shot knowledge-based vqa. arXiv preprint arXiv:2109.05014, 2021. 1, 3495\n[62] Ziyi Yang, Yuwei Fang, Chenguang Zhu, Reid Pryzant, Dongdong Chen, Yu Shi, Yichong Xu, Yao Qian,496 Mei Gao, Yi-Ling Chen, et al. i-code: An integrative and composable multimodal learning framework.497 arXiv preprint arXiv:2205.01818, 2022. 1498\n[63] Fei Yu, Jiji Tang, Weichong Yin, Yu Sun, Hao Tian, Hua Wu, and Haifeng Wang. Ernie-vil: Knowledge499 enhanced vision-language representations through scene graphs. In Proceedings of the AAAI Conference500 on Artificial Intelligence, volume 35, pages 3208\u20133216, 2021. 2501\n[64] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong502 Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv503 preprint arXiv:2111.11432, 2021. 2504\n[65] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza Salehi, Aditya505 Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi. Merlot reserve: Neural script knowledge through506 vision and language and sound. arXiv preprint arXiv:2201.02639, 2022. 1507\n[66] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin508 Choi. Merlot: Multimodal neural script knowledge models. Advances in Neural Information Processing509 Systems, 34, 2021. 1, 8510\n[67] Andy Zeng, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit,511 Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, et al. Socratic models: Composing512 zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598, 2022. 3, 4513\n[68] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and514 Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In Proceedings of the515 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5579\u20135588, 2021. 2516\n[69] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher517 Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models.518 arXiv preprint arXiv:2205.01068, 2022. 1519\n[70] Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improving520 few-shot performance of language models. In International Conference on Machine Learning, pages521 12697\u201312706. PMLR, 2021. 1522\n[71] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, and Jianfeng Gao. Unified vision-523 language pre-training for image captioning and vqa. In Proceedings of the AAAI Conference on Artificial524 Intelligence, volume 34, pages 13041\u201313049, 2020. 2525\n[72] Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards automatic learning of procedures from web526 instructional videos. In AAAI Conference on Artificial Intelligence, pages 7590\u20137598, 2018. 6527\n[73] Xizhou Zhu, Jinguo Zhu, Hao Li, Xiaoshi Wu, Xiaogang Wang, Hongsheng Li, Xiaohua Wang, and Jifeng528 Dai. Uni-perceiver: Pre-training unified architecture for generic perception for zero-shot and few-shot529 tasks. arXiv preprint arXiv:2112.01522, 2021. 3530\nChecklist531\n1. For all authors...532 (a) Do the main claims made in the abstract and introduction accurately reflect the paper\u2019s533 contributions and scope? [Yes]534 (b) Did you describe the limitations of your work? [Yes] See Section 4.6.535 (c) Did you discuss any potential negative societal impacts of your work? [Yes] See536 supplemental material.537 (d) Have you read the ethics review guidelines and ensured that your paper conforms to538 them? [Yes]539 2. If you are including theoretical results...540\n(a) Did you state the full set of assumptions of all theoretical results? [N/A]541 (b) Did you include complete proofs of all theoretical results? [N/A]542\n3. If you ran experiments...543 (a) Did you include the code, data, and instructions needed to reproduce the main experi-544 mental results (either in the supplemental material or as a URL)? [Yes] See supplemen-545 tal material.546\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they547 were chosen)? [Yes] See Section 4.1 and supplemental material.548\n(c) Did you report error bars (e.g., with respect to the random seed after running experi-549 ments multiple times)? [Yes] See Section 4.6.550\n(d) Did you include the total amount of compute and the type of resources used (e.g., type551 of GPUs, internal cluster, or cloud provider)? [Yes] See Section 4.1.552\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...553 (a) If your work uses existing assets, did you cite the creators? [Yes] see Section 4.6.554 (b) Did you mention the license of the assets? [Yes] see the citations in Section 4.6.555 (c) Did you include any new assets either in the supplemental material or as a URL? [N/A]556\n557\n(d) Did you discuss whether and how consent was obtained from people whose data you\u2019re558 using/curating? [Yes] see the citations in Section 4.6.559\n(e) Did you discuss whether the data you are using/curating contains personally identifiable560 information or offensive content? [Yes] see the citations in Section 4.6.561\n5. If you used crowdsourcing or conducted research with human subjects...562 (a) Did you include the full text of instructions given to participants and screenshots, if563 applicable? [N/A]564 (b) Did you describe any potential participant risks, with links to Institutional Review565 Board (IRB) approvals, if applicable? [N/A]566 (c) Did you include the estimated hourly wage paid to participants and the total amount567 spent on participant compensation? [N/A]568"
      }
    ],
    "year": 2022,
    "abstractText": "The goal of this work is to build flexible video-language models that can gener1 alize to various video-to-text tasks from few examples, such as domain-specific 2 captioning, question answering, and future event prediction. Existing few-shot 3 video-language learners focus exclusively on the encoder, resulting in the absence 4 of a video-to-text decoder to handle generative tasks. Video captioners have been 5 pretrained on large-scale video-language datasets, but they rely heavily on fine6 tuning and lack the ability to generate text for unseen tasks in a few-shot setting. 7 We propose VidIL, a few-shot Video-language Learner via Image and Language 8 models, which demonstrates strong performance on few-shot video-to-text tasks 9 without the necessity of pretraining or finetuning on any video datasets. We use the 10 image-language models to translate the video content into frame captions, object, 11 attribute, and event phrases, and compose them into a temporal structure template. 12 We then instruct a language model, with a prompt containing a few in-context 13 examples, to generate a target output from the composed content. The flexibility of 14 prompting allows the model to capture any form of text input, such as automatic 15 speech recognition (ASR) transcripts. Our experiments demonstrate the power 16 of language models in understanding videos on a wide variety of video-language 17 tasks, including video captioning, video question answering, video caption retrieval, 18 and video future event prediction. Especially, on video future event prediction, 19 our few-shot model significantly outperforms state-of-the-art supervised models 20 trained on large-scale video datasets. 21",
    "creator": "TeX"
  },
  "output": [
    [
      "1. A few clarifications are required around the generation of the prompt (see Question section).",
      "2. Conceptually, the approach is quite similar to the PICA paper [61]. It would be good to better clarify the difference with that approach? Is it mainly the video setting and the fact that one has to deal with temporal information? If yes, then would it be possible to show what happens if only a single frame from each video is used instead of multiple ones?",
      "3. Some ablations are missing (see Question section for specific suggestions).",
      "4. Some results seem a little bit inconsistent and raise some questions (see Question section for more details).",
      "5. Should Flamingo be also compared against in Table 2 for consistency (since they provide results for YC2 and VATEX as well?)",
      "6. It would be interesting to see how the performance evolves with the number of prompts for all benchmarks but mainly 5 shot results are given (see request in the Question section).",
      "7. This approach is brittle as it is bottlenecked by the extraction of visual entities and the captioning model.",
      "8. Such approach may not scale well to long-term video understanding (mentioned in the conclusion) since one would be quickly limited by the size of the prompt of Language Models. Also, temporal information is only given with words like \"First\", \"After\"... How would that scale to longer videos? Could the authors comment on that?"
    ],
    [
      "1. \"The proposed model is not end-to-end.\"",
      "2. \"It is based on using CLIP and InstructGPT3.\"",
      "3. \"The approach is simply based on extracting candidate objects and captions frame by frame using CLIP encoder, and then creating a prompt to feed to InstructGPT3 using in-context learning to achieve few-shot performance.\"",
      "4. \"Therefore, there is minimal novelty in the architecture.\""
    ],
    [
      "1. \"I believe the approach to be highly bottlenecked by the expressiveness of the vision foundational models. As a concrete example, I do not see how this approach can work well on traditional action recognition benchmarks (i.e. Kinetics or Something Something) when the task requires low-level temporal understanding of actions. Also the approach does not work for task where there is no useful signals to leverage from frame captions and visual attributes. For instance such task could be fine-grained spatial visual question answering. MSVD-QA and MSRVTT-QA are not really fine-grained VQA dataset, so there are no questions requiring spatial understanding such as 'What is on the left of the car?' or 'What is on bottom the table?'. Right now I do not see how the provided frame captions and attributes are enough to answer fine-grained question about the visual content.\"",
      "2. \"Why are the ablation studies only reported on MSVD-QA? The results would have been more convincing with several evaluation datasets. For example in Table 7, we do not see clear improvement when the number of shots increase. As the ablation study is only reported on one dataset, I'd assume the conclusion were not even better on the other datasets?\"",
      "3. \"Also I find that the number of in-context examples VS the true few-shot number in this Table 7 is not clear. It would have been clearer to have two columns, one for both. It also look like the in-context selection is not really beneficial for more than 5 shots.\"",
      "4. \"Table 3: Clarify the real number of shot used in that table. What is the number of in-context vs the number of shots the retrieval system has access to. Is it 5-shot in the sense of 5 in-context shot?\"",
      "5. \"Table 6: It looks like frame captions are enough to get most of the useful signal, as it only improves the MSVD-QA top-1 accuracy of 1.2% when adding the different visual attributes. This is disappointing knowing all the emphasise around the need for the visual attributes in the approach section.\"",
      "6. \"In the Related work section, the authors wrote L 110: \u2018We are the first work to leverage prompting a frozen language model for tackling few-shot video-language tasks with a unified textual representation\u2019.  I believe this is an overclaim as [1] have already done something in a similar spirit. It looks to me this paper, mostly extend the work of [1] with non-vqa tasks and video instead of images.\""
    ],
    [
      "1. The technical novelty of this paper is very limited. CLIP, BLIP, and InstructGPT are respectively deployed for visual tokenization, frame captioning, and conditional text generation with few-shot prompt. Excluding this, the contributions made by the authors are rather minor. I even have doubt that the performance of VidIL is mainly due to the deployment of CLIP, BLIP, and InstructGPT.",
      "2. The proposed VidIL can still be considered as prompt learning for large-scale cross-modal pre-training models. Therefore, other prompt learning methods should be included in the main experiments. Some examples are: [a] Align and Prompt: Video-and-Language Pre-training with Entity Prompts, arXiv:2112.09583, 2021. [b] CPT: Colorful Prompt Tuning for Pre-trained Vision-Language Models, arXiv:2109.11797, 2021.",
      "3. The method part should be reorganized in a more formal way. Although the source code is given, an outlined algorithm would help the readers easier to capture the proposed method."
    ]
  ],
  "review_num": 4,
  "item_num": [
    8,
    4,
    6,
    3
  ]
}